#Chapter 1 Notes

### Transformer Architecture

### Encoding
The input values are embedded with a 512 dimension embedding
Another 512 embedding is generated based on the position of the text in the sequence.
These two embeddings are added together to provide context and position in the embedding 

### SubLayer 1 Multiheaded Attention
The input is the encoded vector containing position and context for each word

