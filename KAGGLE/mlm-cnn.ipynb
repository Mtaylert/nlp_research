{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM, AutoConfig\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom time import time\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.optim import Adadelta\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-29T04:28:13.026130Z","iopub.execute_input":"2022-10-29T04:28:13.026523Z","iopub.status.idle":"2022-10-29T04:28:13.704944Z","shell.execute_reply.started":"2022-10-29T04:28:13.026440Z","shell.execute_reply":"2022-10-29T04:28:13.703855Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dberta-base-model/rust_model.ot\n/kaggle/input/dberta-base-model/config.json\n/kaggle/input/dberta-base-model/merges.txt\n/kaggle/input/dberta-base-model/vocab.json\n/kaggle/input/dberta-base-model/tf_model.h5\n/kaggle/input/dberta-base-model/tokenizer_config.json\n/kaggle/input/dberta-base-model/bpe_encoder.bin\n/kaggle/input/dberta-base-model/pytorch_model.bin\n/kaggle/input/cnn-sent-mod/CNN_SENT_MODEL.bin\n/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n/kaggle/input/big-embedder/1_epoch_mlm.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"class MLMCONFIG:\n    SOURCE = 'https://www.kaggle.com/code/chaitanyagiri/deberta-pre-training-using-mlm'\n    TRAIN = \"/kaggle/input/feedback-prize-english-language-learning/train.csv\"\n    TEST = \"/kaggle/input/feedback-prize-english-language-learning/test.csv\"\n    MASKING = 0.10\n    MAX_LEN = 356\n    CLS_TOKEN = 1\n    PAD_TOKEN = 0\n    SEP_TOKEN = 2\n    MASK_TOKEN = 50264\n    BATCH_SIZE = 8\n    TEST_BATCH_SIZE = 1\n    EPOCHS = 1\n    LR = 1e-5\n    MODEL_PATH = \"/kaggle/input/dberta-base-model/\"\n    SAVE_PATH = \"/kaggle/working/fine-tuned-mlm.bin\"\n    FINE_TUNED = \"/kaggle/input/big-embedder/1_epoch_mlm.bin\"\n    NUM_WORKERS = 4\n\n    \nclass CNN_CONFIG:\n    TRAIN = \"/kaggle/input/feedback-prize-english-language-learning/train.csv\"\n    TEST = \"/kaggle/input/feedback-prize-english-language-learning/test.csv\"\n    LR = 0.001\n    TRAIN_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 8\n    EPOCHS = 20\n    N_FOLDS = 4\n    TARGETS = ['cohesion', 'syntax', 'vocabulary',\n       'phraseology', 'grammar', 'conventions']\n    PADDING = False\n    NUM_WORKERS = 4\n    FINE_TUNED = \"/kaggle/input/cnn-sent-mod/CNN_SENT_MODEL.bin\"\n  ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.707356Z","iopub.execute_input":"2022-10-29T04:28:13.707949Z","iopub.status.idle":"2022-10-29T04:28:13.716029Z","shell.execute_reply.started":"2022-10-29T04:28:13.707892Z","shell.execute_reply":"2022-10-29T04:28:13.715004Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class TrainMLMDatasetup(Dataset):\n    def __init__(self, data):\n        self.data = data[\"full_text\"]\n        self.tokenizer = AutoTokenizer.from_pretrained(MLMCONFIG.MODEL_PATH)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        text = self.data[item]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            return_tensors='pt',\n            add_special_tokens=True,\n            max_length=MLMCONFIG.MAX_LEN,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        # copy input ids to create out labels tensor\n        inputs[\"labels\"] = inputs.input_ids.detach().clone()\n\n        # create a set of floats and mask anything below our masking %\n        # include logic to avoid masking the [CLS] [PAD], & [SEP] tokens\n        rand = torch.rand(inputs.input_ids.shape)\n        masked_arr = (\n            (rand < MLMCONFIG.MASKING)\n            * (inputs.labels != MLMCONFIG.CLS_TOKEN)\n            * (inputs.labels != MLMCONFIG.SEP_TOKEN)\n            * (inputs.labels != MLMCONFIG.PAD_TOKEN)\n        )\n\n        # get the non-zeros from the masked array\n        selection = []\n        for idx in range(masked_arr.shape[0]):\n            selection.append(torch.flatten(masked_arr[idx].nonzero()).tolist())\n\n        # use selection to mask the input_ids based upon the config.MASKING set\n        for idx in range(masked_arr.shape[0]):\n            inputs.input_ids[idx, selection[idx]] = MLMCONFIG.MASK_TOKEN\n\n        input_ids = torch.flatten(inputs.input_ids)\n        attention_mask = torch.flatten(inputs.attention_mask)\n        labels = torch.flatten(inputs.labels)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\n    \n    \n    \nclass TestMLMDatasetup(Dataset):\n    def __init__(self, data):\n        self.data = data[\"full_text\"]\n        self.tokenizer = AutoTokenizer.from_pretrained(MLMCONFIG.MODEL_PATH)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        text = self.data[item]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            return_tensors='pt',\n            add_special_tokens=True,\n            max_length=MLMCONFIG.MAX_LEN,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        input_ids = torch.flatten(inputs.input_ids)\n        attention_mask = torch.flatten(inputs.attention_mask)\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        }\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.718594Z","iopub.execute_input":"2022-10-29T04:28:13.719008Z","iopub.status.idle":"2022-10-29T04:28:13.734701Z","shell.execute_reply.started":"2022-10-29T04:28:13.718968Z","shell.execute_reply":"2022-10-29T04:28:13.733759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MLMFineTuner(nn.Module):\n    def __init__(self):\n        super(MLMFineTuner, self).__init__()\n        self.epochs = MLMCONFIG.EPOCHS\n        self.learning_rate = MLMCONFIG.LR\n        self.config = AutoConfig.from_pretrained(MLMCONFIG.MODEL_PATH, output_hidden_states=True)\n        self.model = AutoModelForMaskedLM.from_pretrained(MLMCONFIG.MODEL_PATH, config=self.config)\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n\n    def fit(self, data):\n\n        train_dataset = TrainMLMDatasetup(data)\n        train_dataloader = DataLoader(train_dataset, batch_size=MLMCONFIG.BATCH_SIZE, shuffle=True, num_workers=MLMCONFIG.NUM_WORKERS)\n        self.model.to(self.device)\n        self.model.train()\n        optim = AdamW(self.model.parameters(), lr=MLMCONFIG.LR)\n        for epoch in range(self.epochs):\n            for idx, batch in enumerate(train_dataloader):\n                self.model.zero_grad()\n                input_ids = batch['input_ids'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                output = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = output.loss\n                print(f\"epoch {epoch} --- step {idx} --- step size {input_ids.shape[0]} loss {loss.item()}\")\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                loss.backward()\n                optim.step()\n            torch.save(self.model.state_dict(), MLMCONFIG.SAVE_PATH)\n        \n    def get_embeddings(self, X):\n        test_dataset = TestMLMDatasetup(data=X)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=MLMCONFIG.TEST_BATCH_SIZE,\n            shuffle=False,\n            num_workers=MLMCONFIG.NUM_WORKERS,\n            pin_memory=True,\n            drop_last=False,\n        )\n\n        self.model.load_state_dict(\n            torch.load(MLMCONFIG.FINE_TUNED)\n        )\n        self.model.to(self.device)\n        self.model.eval()\n        token_vectors = []\n        for idx, batch in enumerate(tqdm(test_loader)):\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n            hidden_states = outputs.hidden_states\n            token_vectors.append(hidden_states[-2][0].cpu().detach())\n        return token_vectors\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.737579Z","iopub.execute_input":"2022-10-29T04:28:13.739975Z","iopub.status.idle":"2022-10-29T04:28:13.754787Z","shell.execute_reply.started":"2022-10-29T04:28:13.739922Z","shell.execute_reply":"2022-10-29T04:28:13.753842Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def targets_to_tensor(df, target_columns):\n    return torch.tensor(df[target_columns].values, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.757306Z","iopub.execute_input":"2022-10-29T04:28:13.757883Z","iopub.status.idle":"2022-10-29T04:28:13.769282Z","shell.execute_reply.started":"2022-10-29T04:28:13.757842Z","shell.execute_reply":"2022-10-29T04:28:13.768090Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"TRAIN = pd.read_csv(MLMCONFIG.TRAIN) \nTEST = pd.read_csv(MLMCONFIG.TEST) ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.772459Z","iopub.execute_input":"2022-10-29T04:28:13.772786Z","iopub.status.idle":"2022-10-29T04:28:13.867264Z","shell.execute_reply.started":"2022-10-29T04:28:13.772739Z","shell.execute_reply":"2022-10-29T04:28:13.866368Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"run_mlm = 0\nif run_mlm:\n    mlm = MLMFineTuner()\n    mlm.fit(data=TRAIN)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.868775Z","iopub.execute_input":"2022-10-29T04:28:13.869217Z","iopub.status.idle":"2022-10-29T04:28:13.874219Z","shell.execute_reply.started":"2022-10-29T04:28:13.869176Z","shell.execute_reply":"2022-10-29T04:28:13.873129Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TrainEmbeddingDataset(Dataset):\n    def __init__(self, data):\n        mlm = MLMFineTuner()\n        self.data = mlm.get_embeddings(X=data)\n        self.targets = targets_to_tensor(df=data,target_columns=CNN_CONFIG.TARGETS)\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        record = self.data[item]\n        target = self.targets[item]\n        \n        return {\n            \"input_ids\": record,\n            \"labels\": torch.tensor(target, dtype=torch.long),\n        }\n\n    \nclass TestEmbeddingDataset(Dataset):\n    def __init__(self, data):\n        mlm = MLMFineTuner()\n        self.data = mlm.get_embeddings(X=data)\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        record = self.data[item]\n        \n        return {\n            \"input_ids\": record,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.875735Z","iopub.execute_input":"2022-10-29T04:28:13.876395Z","iopub.status.idle":"2022-10-29T04:28:13.885926Z","shell.execute_reply.started":"2022-10-29T04:28:13.876359Z","shell.execute_reply":"2022-10-29T04:28:13.884951Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SentCNN(nn.Module):\n    def __init__(self):\n        super(SentCNN, self).__init__()\n        dropout = 0.5\n        static = True\n        V = MLMCONFIG.MAX_LEN\n        D = 768\n        C = 6\n        Co = 3\n        Ks = [3, 4, 5]\n        \n        self.static = static\n        self.embed = nn.Embedding(V, D)\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, D)) for K in Ks])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(len(Ks) * Co, 6)\n\n    def forward(self, x):\n        x = Variable(x)\n\n        x = x.unsqueeze(1)  # (N, Ci, W, D)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)  # (N, len(Ks)*Co)\n        logit = self.fc(x)  # (N, C)\n        return logit","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.889358Z","iopub.execute_input":"2022-10-29T04:28:13.889663Z","iopub.status.idle":"2022-10-29T04:28:13.901158Z","shell.execute_reply.started":"2022-10-29T04:28:13.889635Z","shell.execute_reply":"2022-10-29T04:28:13.900235Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SentenceCNNTrainer(nn.Module):\n    def __init__(self):\n        super(SentenceCNNTrainer, self).__init__()\n        self.model = SentCNN()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n    \n    def fit(self, data):\n        trainembedderset = TrainEmbeddingDataset(data=data)\n        train_dataloader = DataLoader(\n            trainembedderset,\n            batch_size=CNN_CONFIG.TRAIN_BATCH_SIZE,\n            shuffle=True,\n            num_workers=CNN_CONFIG.NUM_WORKERS,\n            pin_memory=True,\n            drop_last=True,\n        )\n        criterion = nn.SmoothL1Loss(reduction='mean')\n        self.model.to(self.device)\n        self.model.train()\n        optimizer = AdamW(params=self.model.parameters(), lr=CNN_CONFIG.LR)\n        for epoch in range(CNN_CONFIG.EPOCHS):\n            for step, batch in enumerate(train_dataloader):\n                self.model.zero_grad()\n                input_ids = batch[\"input_ids\"].to(self.device)\n                labels = batch[\"labels\"].to(self.device)\n                prediction_probas = self.model(input_ids)\n                loss = criterion(prediction_probas, labels)\n                loss.backward()\n                print(f\"epoch {epoch} --- step {step} --- step size {input_ids.shape[0]} loss {loss.item()}\")\n                # Update model parameters:\n                # fine tune BERT params and train additional dense layers\n                optimizer.step()\n                # update learning rate\n            torch.save(self.model.state_dict(), \"CNN_SENT_MODEL.bin\")\n    def predict(self, data):\n        testembedderset = TestEmbeddingDataset(data=data)\n        test_dataloader = DataLoader(\n            testembedderset,\n            batch_size=1,\n            shuffle=True,\n            num_workers=CNN_CONFIG.NUM_WORKERS,\n            pin_memory=True,\n            drop_last=True,\n        )\n        self.model.eval()\n        predictions = []\n        for step, batch in enumerate(tqdm(test_dataloader)):\n            input_ids = batch[\"input_ids\"].to(self.device)\n            with torch.no_grad():\n                pred_probas = self.model(input_ids)\n                \n            predictions.append(pred_probas.cpu().detach().numpy())\n        predictions = pd.DataFrame(np.concatenate(predictions))\n        predictions.columns = CNN_CONFIG.TARGETS\n        return predictions\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.904874Z","iopub.execute_input":"2022-10-29T04:28:13.905416Z","iopub.status.idle":"2022-10-29T04:28:13.918905Z","shell.execute_reply.started":"2022-10-29T04:28:13.905389Z","shell.execute_reply":"2022-10-29T04:28:13.917987Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"clf = SentenceCNNTrainer()\nclf.fit(data=TRAIN)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:28:13.920215Z","iopub.execute_input":"2022-10-29T04:28:13.921426Z","iopub.status.idle":"2022-10-29T04:32:28.701258Z","shell.execute_reply.started":"2022-10-29T04:28:13.921383Z","shell.execute_reply":"2022-10-29T04:32:28.700011Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at /kaggle/input/dberta-base-model/ were not used when initializing DebertaForMaskedLM: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n- This IS expected if you are initializing DebertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaForMaskedLM were not initialized from the model checkpoint at /kaggle/input/dberta-base-model/ and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n  0%|          | 0/3911 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 3911/3911 [02:11<00:00, 29.84it/s]\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 --- step 0 --- step size 32 loss 2.7028234004974365\nepoch 0 --- step 1 --- step size 32 loss 2.340407371520996\nepoch 0 --- step 2 --- step size 32 loss 2.3022775650024414\nepoch 0 --- step 3 --- step size 32 loss 2.3712892532348633\nepoch 0 --- step 4 --- step size 32 loss 2.304643154144287\nepoch 0 --- step 5 --- step size 32 loss 2.1172547340393066\nepoch 0 --- step 6 --- step size 32 loss 2.2287089824676514\nepoch 0 --- step 7 --- step size 32 loss 2.2328388690948486\nepoch 0 --- step 8 --- step size 32 loss 2.1692614555358887\nepoch 0 --- step 9 --- step size 32 loss 2.2546162605285645\nepoch 0 --- step 10 --- step size 32 loss 2.159183979034424\nepoch 0 --- step 11 --- step size 32 loss 2.151548385620117\nepoch 0 --- step 12 --- step size 32 loss 2.0133614540100098\nepoch 0 --- step 13 --- step size 32 loss 1.900999903678894\nepoch 0 --- step 14 --- step size 32 loss 2.2243897914886475\nepoch 0 --- step 15 --- step size 32 loss 2.0869388580322266\nepoch 0 --- step 16 --- step size 32 loss 2.0125012397766113\nepoch 0 --- step 17 --- step size 32 loss 2.1112093925476074\nepoch 0 --- step 18 --- step size 32 loss 2.067627429962158\nepoch 0 --- step 19 --- step size 32 loss 2.193413019180298\nepoch 0 --- step 20 --- step size 32 loss 2.143763780593872\nepoch 0 --- step 21 --- step size 32 loss 2.1471681594848633\nepoch 0 --- step 22 --- step size 32 loss 2.0858988761901855\nepoch 0 --- step 23 --- step size 32 loss 1.9329365491867065\nepoch 0 --- step 24 --- step size 32 loss 2.1919057369232178\nepoch 0 --- step 25 --- step size 32 loss 2.104382038116455\nepoch 0 --- step 26 --- step size 32 loss 1.9012064933776855\nepoch 0 --- step 27 --- step size 32 loss 2.0418429374694824\nepoch 0 --- step 28 --- step size 32 loss 2.080500364303589\nepoch 0 --- step 29 --- step size 32 loss 2.1458051204681396\nepoch 0 --- step 30 --- step size 32 loss 1.9318965673446655\nepoch 0 --- step 31 --- step size 32 loss 2.018582344055176\nepoch 0 --- step 32 --- step size 32 loss 2.160158634185791\nepoch 0 --- step 33 --- step size 32 loss 2.0238912105560303\nepoch 0 --- step 34 --- step size 32 loss 2.10996675491333\nepoch 0 --- step 35 --- step size 32 loss 2.118346929550171\nepoch 0 --- step 36 --- step size 32 loss 1.9803133010864258\nepoch 0 --- step 37 --- step size 32 loss 1.9503090381622314\nepoch 0 --- step 38 --- step size 32 loss 2.1131348609924316\nepoch 0 --- step 39 --- step size 32 loss 2.0707621574401855\nepoch 0 --- step 40 --- step size 32 loss 2.1907458305358887\nepoch 0 --- step 41 --- step size 32 loss 2.025794506072998\nepoch 0 --- step 42 --- step size 32 loss 1.998991847038269\nepoch 0 --- step 43 --- step size 32 loss 1.8011293411254883\nepoch 0 --- step 44 --- step size 32 loss 2.1401431560516357\nepoch 0 --- step 45 --- step size 32 loss 2.018747329711914\nepoch 0 --- step 46 --- step size 32 loss 1.9467462301254272\nepoch 0 --- step 47 --- step size 32 loss 1.8653367757797241\nepoch 0 --- step 48 --- step size 32 loss 2.062680959701538\nepoch 0 --- step 49 --- step size 32 loss 1.9971131086349487\nepoch 0 --- step 50 --- step size 32 loss 1.9692554473876953\nepoch 0 --- step 51 --- step size 32 loss 1.9345788955688477\nepoch 0 --- step 52 --- step size 32 loss 1.9699910879135132\nepoch 0 --- step 53 --- step size 32 loss 1.9430512189865112\nepoch 0 --- step 54 --- step size 32 loss 2.0469465255737305\nepoch 0 --- step 55 --- step size 32 loss 1.9356474876403809\nepoch 0 --- step 56 --- step size 32 loss 1.989288091659546\nepoch 0 --- step 57 --- step size 32 loss 1.9006856679916382\nepoch 0 --- step 58 --- step size 32 loss 2.0572867393493652\nepoch 0 --- step 59 --- step size 32 loss 2.0116775035858154\nepoch 0 --- step 60 --- step size 32 loss 1.8997124433517456\nepoch 0 --- step 61 --- step size 32 loss 1.9024009704589844\nepoch 0 --- step 62 --- step size 32 loss 1.8556960821151733\nepoch 0 --- step 63 --- step size 32 loss 1.831897258758545\nepoch 0 --- step 64 --- step size 32 loss 1.8495478630065918\nepoch 0 --- step 65 --- step size 32 loss 1.8864649534225464\nepoch 0 --- step 66 --- step size 32 loss 1.8788217306137085\nepoch 0 --- step 67 --- step size 32 loss 1.7684531211853027\nepoch 0 --- step 68 --- step size 32 loss 2.021143913269043\nepoch 0 --- step 69 --- step size 32 loss 1.8704593181610107\nepoch 0 --- step 70 --- step size 32 loss 1.803553581237793\nepoch 0 --- step 71 --- step size 32 loss 1.9391374588012695\nepoch 0 --- step 72 --- step size 32 loss 1.7007945775985718\nepoch 0 --- step 73 --- step size 32 loss 1.738659143447876\nepoch 0 --- step 74 --- step size 32 loss 2.0881943702697754\nepoch 0 --- step 75 --- step size 32 loss 1.7984471321105957\nepoch 0 --- step 76 --- step size 32 loss 2.0370078086853027\nepoch 0 --- step 77 --- step size 32 loss 1.611323356628418\nepoch 0 --- step 78 --- step size 32 loss 1.7770650386810303\nepoch 0 --- step 79 --- step size 32 loss 1.8616855144500732\nepoch 0 --- step 80 --- step size 32 loss 1.9110288619995117\nepoch 0 --- step 81 --- step size 32 loss 1.674268126487732\nepoch 0 --- step 82 --- step size 32 loss 1.7990977764129639\nepoch 0 --- step 83 --- step size 32 loss 1.7518314123153687\nepoch 0 --- step 84 --- step size 32 loss 1.694190263748169\nepoch 0 --- step 85 --- step size 32 loss 1.7405580282211304\nepoch 0 --- step 86 --- step size 32 loss 1.8497213125228882\nepoch 0 --- step 87 --- step size 32 loss 1.6824240684509277\nepoch 0 --- step 88 --- step size 32 loss 1.921314001083374\nepoch 0 --- step 89 --- step size 32 loss 1.5817630290985107\nepoch 0 --- step 90 --- step size 32 loss 1.8769443035125732\nepoch 0 --- step 91 --- step size 32 loss 1.6786043643951416\nepoch 0 --- step 92 --- step size 32 loss 1.658764362335205\nepoch 0 --- step 93 --- step size 32 loss 1.6900873184204102\nepoch 0 --- step 94 --- step size 32 loss 1.6236076354980469\nepoch 0 --- step 95 --- step size 32 loss 1.4359230995178223\nepoch 0 --- step 96 --- step size 32 loss 1.6996182203292847\nepoch 0 --- step 97 --- step size 32 loss 1.6729512214660645\nepoch 0 --- step 98 --- step size 32 loss 1.713674545288086\nepoch 0 --- step 99 --- step size 32 loss 1.6092174053192139\nepoch 0 --- step 100 --- step size 32 loss 1.6695268154144287\nepoch 0 --- step 101 --- step size 32 loss 1.7307586669921875\nepoch 0 --- step 102 --- step size 32 loss 1.5040063858032227\nepoch 0 --- step 103 --- step size 32 loss 1.5632785558700562\nepoch 0 --- step 104 --- step size 32 loss 1.4953358173370361\nepoch 0 --- step 105 --- step size 32 loss 1.4869627952575684\nepoch 0 --- step 106 --- step size 32 loss 1.7033641338348389\nepoch 0 --- step 107 --- step size 32 loss 1.6754822731018066\nepoch 0 --- step 108 --- step size 32 loss 1.4688165187835693\nepoch 0 --- step 109 --- step size 32 loss 1.8168883323669434\nepoch 0 --- step 110 --- step size 32 loss 1.7061734199523926\nepoch 0 --- step 111 --- step size 32 loss 1.6212310791015625\nepoch 0 --- step 112 --- step size 32 loss 1.5633955001831055\nepoch 0 --- step 113 --- step size 32 loss 1.605825424194336\nepoch 0 --- step 114 --- step size 32 loss 1.656557321548462\nepoch 0 --- step 115 --- step size 32 loss 1.5658997297286987\nepoch 0 --- step 116 --- step size 32 loss 1.6227563619613647\nepoch 0 --- step 117 --- step size 32 loss 1.6222712993621826\nepoch 0 --- step 118 --- step size 32 loss 1.6429338455200195\nepoch 0 --- step 119 --- step size 32 loss 1.7647981643676758\nepoch 0 --- step 120 --- step size 32 loss 1.5800931453704834\nepoch 0 --- step 121 --- step size 32 loss 1.503705382347107\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 1 --- step 0 --- step size 32 loss 1.4548826217651367\nepoch 1 --- step 1 --- step size 32 loss 1.4775099754333496\nepoch 1 --- step 2 --- step size 32 loss 1.5124086141586304\nepoch 1 --- step 3 --- step size 32 loss 1.6611522436141968\nepoch 1 --- step 4 --- step size 32 loss 1.5162461996078491\nepoch 1 --- step 5 --- step size 32 loss 1.444655418395996\nepoch 1 --- step 6 --- step size 32 loss 1.4352641105651855\nepoch 1 --- step 7 --- step size 32 loss 1.488265037536621\nepoch 1 --- step 8 --- step size 32 loss 1.5790613889694214\nepoch 1 --- step 9 --- step size 32 loss 1.3008599281311035\nepoch 1 --- step 10 --- step size 32 loss 1.5947318077087402\nepoch 1 --- step 11 --- step size 32 loss 1.4795112609863281\nepoch 1 --- step 12 --- step size 32 loss 1.3173869848251343\nepoch 1 --- step 13 --- step size 32 loss 1.3626420497894287\nepoch 1 --- step 14 --- step size 32 loss 1.5544264316558838\nepoch 1 --- step 15 --- step size 32 loss 1.3862273693084717\nepoch 1 --- step 16 --- step size 32 loss 1.1347006559371948\nepoch 1 --- step 17 --- step size 32 loss 1.445237159729004\nepoch 1 --- step 18 --- step size 32 loss 1.7111740112304688\nepoch 1 --- step 19 --- step size 32 loss 1.5142769813537598\nepoch 1 --- step 20 --- step size 32 loss 1.3967437744140625\nepoch 1 --- step 21 --- step size 32 loss 1.4957914352416992\nepoch 1 --- step 22 --- step size 32 loss 1.662134051322937\nepoch 1 --- step 23 --- step size 32 loss 1.4063646793365479\nepoch 1 --- step 24 --- step size 32 loss 1.594347357749939\nepoch 1 --- step 25 --- step size 32 loss 1.612370491027832\nepoch 1 --- step 26 --- step size 32 loss 1.4471147060394287\nepoch 1 --- step 27 --- step size 32 loss 1.5227488279342651\nepoch 1 --- step 28 --- step size 32 loss 1.4315917491912842\nepoch 1 --- step 29 --- step size 32 loss 1.2615830898284912\nepoch 1 --- step 30 --- step size 32 loss 1.4220794439315796\nepoch 1 --- step 31 --- step size 32 loss 1.5794132947921753\nepoch 1 --- step 32 --- step size 32 loss 1.3595068454742432\nepoch 1 --- step 33 --- step size 32 loss 1.4052022695541382\nepoch 1 --- step 34 --- step size 32 loss 1.2229785919189453\nepoch 1 --- step 35 --- step size 32 loss 1.5511277914047241\nepoch 1 --- step 36 --- step size 32 loss 1.3695564270019531\nepoch 1 --- step 37 --- step size 32 loss 1.1988723278045654\nepoch 1 --- step 38 --- step size 32 loss 1.330326795578003\nepoch 1 --- step 39 --- step size 32 loss 1.3119466304779053\nepoch 1 --- step 40 --- step size 32 loss 1.7071106433868408\nepoch 1 --- step 41 --- step size 32 loss 1.289794683456421\nepoch 1 --- step 42 --- step size 32 loss 1.6203250885009766\nepoch 1 --- step 43 --- step size 32 loss 1.4960830211639404\nepoch 1 --- step 44 --- step size 32 loss 1.4602246284484863\nepoch 1 --- step 45 --- step size 32 loss 1.5470497608184814\nepoch 1 --- step 46 --- step size 32 loss 1.20084810256958\nepoch 1 --- step 47 --- step size 32 loss 1.3643567562103271\nepoch 1 --- step 48 --- step size 32 loss 1.4381030797958374\nepoch 1 --- step 49 --- step size 32 loss 1.0575354099273682\nepoch 1 --- step 50 --- step size 32 loss 1.2131808996200562\nepoch 1 --- step 51 --- step size 32 loss 1.3424105644226074\nepoch 1 --- step 52 --- step size 32 loss 1.1177613735198975\nepoch 1 --- step 53 --- step size 32 loss 1.298658847808838\nepoch 1 --- step 54 --- step size 32 loss 1.6666063070297241\nepoch 1 --- step 55 --- step size 32 loss 1.4392646551132202\nepoch 1 --- step 56 --- step size 32 loss 1.328544020652771\nepoch 1 --- step 57 --- step size 32 loss 1.176464319229126\nepoch 1 --- step 58 --- step size 32 loss 1.3992360830307007\nepoch 1 --- step 59 --- step size 32 loss 1.507296085357666\nepoch 1 --- step 60 --- step size 32 loss 1.4218125343322754\nepoch 1 --- step 61 --- step size 32 loss 1.4074162244796753\nepoch 1 --- step 62 --- step size 32 loss 1.3470577001571655\nepoch 1 --- step 63 --- step size 32 loss 1.4288729429244995\nepoch 1 --- step 64 --- step size 32 loss 1.169403076171875\nepoch 1 --- step 65 --- step size 32 loss 1.4399850368499756\nepoch 1 --- step 66 --- step size 32 loss 1.1225600242614746\nepoch 1 --- step 67 --- step size 32 loss 1.2764122486114502\nepoch 1 --- step 68 --- step size 32 loss 1.1622637510299683\nepoch 1 --- step 69 --- step size 32 loss 0.9542430639266968\nepoch 1 --- step 70 --- step size 32 loss 0.9968101382255554\nepoch 1 --- step 71 --- step size 32 loss 1.0173447132110596\nepoch 1 --- step 72 --- step size 32 loss 1.3518388271331787\nepoch 1 --- step 73 --- step size 32 loss 1.0492998361587524\nepoch 1 --- step 74 --- step size 32 loss 1.273747205734253\nepoch 1 --- step 75 --- step size 32 loss 1.2265889644622803\nepoch 1 --- step 76 --- step size 32 loss 1.465457797050476\nepoch 1 --- step 77 --- step size 32 loss 1.368674635887146\nepoch 1 --- step 78 --- step size 32 loss 1.1990478038787842\nepoch 1 --- step 79 --- step size 32 loss 1.3601365089416504\nepoch 1 --- step 80 --- step size 32 loss 1.382616400718689\nepoch 1 --- step 81 --- step size 32 loss 1.2761249542236328\nepoch 1 --- step 82 --- step size 32 loss 1.3873796463012695\nepoch 1 --- step 83 --- step size 32 loss 1.256821632385254\nepoch 1 --- step 84 --- step size 32 loss 1.2920234203338623\nepoch 1 --- step 85 --- step size 32 loss 1.2049453258514404\nepoch 1 --- step 86 --- step size 32 loss 0.803378701210022\nepoch 1 --- step 87 --- step size 32 loss 1.2572575807571411\nepoch 1 --- step 88 --- step size 32 loss 1.0220328569412231\nepoch 1 --- step 89 --- step size 32 loss 0.7686473727226257\nepoch 1 --- step 90 --- step size 32 loss 1.2162882089614868\nepoch 1 --- step 91 --- step size 32 loss 1.2886009216308594\nepoch 1 --- step 92 --- step size 32 loss 1.261729121208191\nepoch 1 --- step 93 --- step size 32 loss 1.014343023300171\nepoch 1 --- step 94 --- step size 32 loss 1.395362138748169\nepoch 1 --- step 95 --- step size 32 loss 1.3661108016967773\nepoch 1 --- step 96 --- step size 32 loss 0.9047746062278748\nepoch 1 --- step 97 --- step size 32 loss 1.0309351682662964\nepoch 1 --- step 98 --- step size 32 loss 0.9653759002685547\nepoch 1 --- step 99 --- step size 32 loss 1.2110097408294678\nepoch 1 --- step 100 --- step size 32 loss 1.0542845726013184\nepoch 1 --- step 101 --- step size 32 loss 1.1679456233978271\nepoch 1 --- step 102 --- step size 32 loss 1.1840476989746094\nepoch 1 --- step 103 --- step size 32 loss 1.307321310043335\nepoch 1 --- step 104 --- step size 32 loss 1.1055554151535034\nepoch 1 --- step 105 --- step size 32 loss 0.8727774024009705\nepoch 1 --- step 106 --- step size 32 loss 1.0092921257019043\nepoch 1 --- step 107 --- step size 32 loss 1.280410647392273\nepoch 1 --- step 108 --- step size 32 loss 1.1873981952667236\nepoch 1 --- step 109 --- step size 32 loss 1.512561559677124\nepoch 1 --- step 110 --- step size 32 loss 0.9835647344589233\nepoch 1 --- step 111 --- step size 32 loss 1.384315013885498\nepoch 1 --- step 112 --- step size 32 loss 1.3059402704238892\nepoch 1 --- step 113 --- step size 32 loss 1.0556071996688843\nepoch 1 --- step 114 --- step size 32 loss 1.1847423315048218\nepoch 1 --- step 115 --- step size 32 loss 1.0783330202102661\nepoch 1 --- step 116 --- step size 32 loss 0.9760069847106934\nepoch 1 --- step 117 --- step size 32 loss 0.9082788825035095\nepoch 1 --- step 118 --- step size 32 loss 1.1435210704803467\nepoch 1 --- step 119 --- step size 32 loss 1.0773167610168457\nepoch 1 --- step 120 --- step size 32 loss 0.9162008166313171\nepoch 1 --- step 121 --- step size 32 loss 1.1716771125793457\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 2 --- step 0 --- step size 32 loss 0.7930294275283813\nepoch 2 --- step 1 --- step size 32 loss 1.2406840324401855\nepoch 2 --- step 2 --- step size 32 loss 0.8518131971359253\nepoch 2 --- step 3 --- step size 32 loss 1.0576876401901245\nepoch 2 --- step 4 --- step size 32 loss 1.1208155155181885\nepoch 2 --- step 5 --- step size 32 loss 0.8274275064468384\nepoch 2 --- step 6 --- step size 32 loss 1.3087491989135742\nepoch 2 --- step 7 --- step size 32 loss 1.1989595890045166\nepoch 2 --- step 8 --- step size 32 loss 1.0116784572601318\nepoch 2 --- step 9 --- step size 32 loss 0.8142489790916443\nepoch 2 --- step 10 --- step size 32 loss 1.0984776020050049\nepoch 2 --- step 11 --- step size 32 loss 1.1637020111083984\nepoch 2 --- step 12 --- step size 32 loss 0.9215742945671082\nepoch 2 --- step 13 --- step size 32 loss 1.0822912454605103\nepoch 2 --- step 14 --- step size 32 loss 1.0401074886322021\nepoch 2 --- step 15 --- step size 32 loss 0.9928900599479675\nepoch 2 --- step 16 --- step size 32 loss 0.922339141368866\nepoch 2 --- step 17 --- step size 32 loss 1.2151283025741577\nepoch 2 --- step 18 --- step size 32 loss 1.2160286903381348\nepoch 2 --- step 19 --- step size 32 loss 0.9920822978019714\nepoch 2 --- step 20 --- step size 32 loss 1.2470135688781738\nepoch 2 --- step 21 --- step size 32 loss 1.0920978784561157\nepoch 2 --- step 22 --- step size 32 loss 1.2857651710510254\nepoch 2 --- step 23 --- step size 32 loss 0.8410598635673523\nepoch 2 --- step 24 --- step size 32 loss 0.9842338562011719\nepoch 2 --- step 25 --- step size 32 loss 1.1238125562667847\nepoch 2 --- step 26 --- step size 32 loss 1.293242335319519\nepoch 2 --- step 27 --- step size 32 loss 1.0980956554412842\nepoch 2 --- step 28 --- step size 32 loss 1.1781952381134033\nepoch 2 --- step 29 --- step size 32 loss 1.127037525177002\nepoch 2 --- step 30 --- step size 32 loss 1.2520480155944824\nepoch 2 --- step 31 --- step size 32 loss 1.3385413885116577\nepoch 2 --- step 32 --- step size 32 loss 1.1065032482147217\nepoch 2 --- step 33 --- step size 32 loss 1.2323557138442993\nepoch 2 --- step 34 --- step size 32 loss 0.9757665991783142\nepoch 2 --- step 35 --- step size 32 loss 1.2049217224121094\nepoch 2 --- step 36 --- step size 32 loss 1.3503100872039795\nepoch 2 --- step 37 --- step size 32 loss 0.7619805335998535\nepoch 2 --- step 38 --- step size 32 loss 0.9841760396957397\nepoch 2 --- step 39 --- step size 32 loss 0.8480522632598877\nepoch 2 --- step 40 --- step size 32 loss 1.0491057634353638\nepoch 2 --- step 41 --- step size 32 loss 1.2834045886993408\nepoch 2 --- step 42 --- step size 32 loss 1.3581013679504395\nepoch 2 --- step 43 --- step size 32 loss 1.5091454982757568\nepoch 2 --- step 44 --- step size 32 loss 1.2732144594192505\nepoch 2 --- step 45 --- step size 32 loss 1.019594430923462\nepoch 2 --- step 46 --- step size 32 loss 1.2203876972198486\nepoch 2 --- step 47 --- step size 32 loss 1.3055469989776611\nepoch 2 --- step 48 --- step size 32 loss 1.3170490264892578\nepoch 2 --- step 49 --- step size 32 loss 1.0002868175506592\nepoch 2 --- step 50 --- step size 32 loss 0.984493613243103\nepoch 2 --- step 51 --- step size 32 loss 0.947158694267273\nepoch 2 --- step 52 --- step size 32 loss 0.7815214991569519\nepoch 2 --- step 53 --- step size 32 loss 1.24102783203125\nepoch 2 --- step 54 --- step size 32 loss 0.896517276763916\nepoch 2 --- step 55 --- step size 32 loss 1.1991071701049805\nepoch 2 --- step 56 --- step size 32 loss 1.1817493438720703\nepoch 2 --- step 57 --- step size 32 loss 1.034304141998291\nepoch 2 --- step 58 --- step size 32 loss 0.9344520568847656\nepoch 2 --- step 59 --- step size 32 loss 1.263708472251892\nepoch 2 --- step 60 --- step size 32 loss 1.1651487350463867\nepoch 2 --- step 61 --- step size 32 loss 1.19053316116333\nepoch 2 --- step 62 --- step size 32 loss 1.1562339067459106\nepoch 2 --- step 63 --- step size 32 loss 1.1767950057983398\nepoch 2 --- step 64 --- step size 32 loss 0.9658445715904236\nepoch 2 --- step 65 --- step size 32 loss 1.2712346315383911\nepoch 2 --- step 66 --- step size 32 loss 1.1026809215545654\nepoch 2 --- step 67 --- step size 32 loss 0.9235178828239441\nepoch 2 --- step 68 --- step size 32 loss 0.9329113364219666\nepoch 2 --- step 69 --- step size 32 loss 1.0974568128585815\nepoch 2 --- step 70 --- step size 32 loss 1.0744428634643555\nepoch 2 --- step 71 --- step size 32 loss 1.179978609085083\nepoch 2 --- step 72 --- step size 32 loss 1.1540842056274414\nepoch 2 --- step 73 --- step size 32 loss 1.2207896709442139\nepoch 2 --- step 74 --- step size 32 loss 1.201951265335083\nepoch 2 --- step 75 --- step size 32 loss 1.1221781969070435\nepoch 2 --- step 76 --- step size 32 loss 1.0621793270111084\nepoch 2 --- step 77 --- step size 32 loss 1.0910618305206299\nepoch 2 --- step 78 --- step size 32 loss 0.9765816926956177\nepoch 2 --- step 79 --- step size 32 loss 1.1391377449035645\nepoch 2 --- step 80 --- step size 32 loss 0.9732513427734375\nepoch 2 --- step 81 --- step size 32 loss 1.0574864149093628\nepoch 2 --- step 82 --- step size 32 loss 1.0003325939178467\nepoch 2 --- step 83 --- step size 32 loss 0.9004899263381958\nepoch 2 --- step 84 --- step size 32 loss 1.108846664428711\nepoch 2 --- step 85 --- step size 32 loss 1.2522306442260742\nepoch 2 --- step 86 --- step size 32 loss 1.0897929668426514\nepoch 2 --- step 87 --- step size 32 loss 1.0869883298873901\nepoch 2 --- step 88 --- step size 32 loss 1.0785627365112305\nepoch 2 --- step 89 --- step size 32 loss 1.036167025566101\nepoch 2 --- step 90 --- step size 32 loss 0.8155602216720581\nepoch 2 --- step 91 --- step size 32 loss 1.0280321836471558\nepoch 2 --- step 92 --- step size 32 loss 0.9569403529167175\nepoch 2 --- step 93 --- step size 32 loss 0.999817967414856\nepoch 2 --- step 94 --- step size 32 loss 1.0748181343078613\nepoch 2 --- step 95 --- step size 32 loss 1.2374848127365112\nepoch 2 --- step 96 --- step size 32 loss 0.9232850074768066\nepoch 2 --- step 97 --- step size 32 loss 1.1813551187515259\nepoch 2 --- step 98 --- step size 32 loss 0.8402327299118042\nepoch 2 --- step 99 --- step size 32 loss 0.8908588290214539\nepoch 2 --- step 100 --- step size 32 loss 1.0328171253204346\nepoch 2 --- step 101 --- step size 32 loss 0.9423367977142334\nepoch 2 --- step 102 --- step size 32 loss 1.1611614227294922\nepoch 2 --- step 103 --- step size 32 loss 1.1578987836837769\nepoch 2 --- step 104 --- step size 32 loss 1.0192335844039917\nepoch 2 --- step 105 --- step size 32 loss 1.2134345769882202\nepoch 2 --- step 106 --- step size 32 loss 1.253609299659729\nepoch 2 --- step 107 --- step size 32 loss 1.0505850315093994\nepoch 2 --- step 108 --- step size 32 loss 0.9998799562454224\nepoch 2 --- step 109 --- step size 32 loss 0.790847659111023\nepoch 2 --- step 110 --- step size 32 loss 1.175048589706421\nepoch 2 --- step 111 --- step size 32 loss 1.2436702251434326\nepoch 2 --- step 112 --- step size 32 loss 0.8646315336227417\nepoch 2 --- step 113 --- step size 32 loss 1.0245144367218018\nepoch 2 --- step 114 --- step size 32 loss 1.0304756164550781\nepoch 2 --- step 115 --- step size 32 loss 1.0602169036865234\nepoch 2 --- step 116 --- step size 32 loss 1.0168126821517944\nepoch 2 --- step 117 --- step size 32 loss 1.2030160427093506\nepoch 2 --- step 118 --- step size 32 loss 1.3831733465194702\nepoch 2 --- step 119 --- step size 32 loss 1.2136728763580322\nepoch 2 --- step 120 --- step size 32 loss 0.6867081522941589\nepoch 2 --- step 121 --- step size 32 loss 1.1572120189666748\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 3 --- step 0 --- step size 32 loss 0.9000314474105835\nepoch 3 --- step 1 --- step size 32 loss 1.1312274932861328\nepoch 3 --- step 2 --- step size 32 loss 0.9971312880516052\nepoch 3 --- step 3 --- step size 32 loss 0.8865262866020203\nepoch 3 --- step 4 --- step size 32 loss 0.8181437253952026\nepoch 3 --- step 5 --- step size 32 loss 1.179244041442871\nepoch 3 --- step 6 --- step size 32 loss 0.938035249710083\nepoch 3 --- step 7 --- step size 32 loss 0.9198497533798218\nepoch 3 --- step 8 --- step size 32 loss 1.0521676540374756\nepoch 3 --- step 9 --- step size 32 loss 0.9147354364395142\nepoch 3 --- step 10 --- step size 32 loss 0.9763995409011841\nepoch 3 --- step 11 --- step size 32 loss 0.9963921904563904\nepoch 3 --- step 12 --- step size 32 loss 0.9816960692405701\nepoch 3 --- step 13 --- step size 32 loss 0.9184975028038025\nepoch 3 --- step 14 --- step size 32 loss 0.9016556739807129\nepoch 3 --- step 15 --- step size 32 loss 1.0667763948440552\nepoch 3 --- step 16 --- step size 32 loss 1.2031711339950562\nepoch 3 --- step 17 --- step size 32 loss 1.403381109237671\nepoch 3 --- step 18 --- step size 32 loss 1.1307339668273926\nepoch 3 --- step 19 --- step size 32 loss 1.2108221054077148\nepoch 3 --- step 20 --- step size 32 loss 1.0811420679092407\nepoch 3 --- step 21 --- step size 32 loss 0.9401528835296631\nepoch 3 --- step 22 --- step size 32 loss 1.1785492897033691\nepoch 3 --- step 23 --- step size 32 loss 0.7203971147537231\nepoch 3 --- step 24 --- step size 32 loss 1.0672636032104492\nepoch 3 --- step 25 --- step size 32 loss 1.0074557065963745\nepoch 3 --- step 26 --- step size 32 loss 1.2641775608062744\nepoch 3 --- step 27 --- step size 32 loss 1.0269837379455566\nepoch 3 --- step 28 --- step size 32 loss 1.1685384511947632\nepoch 3 --- step 29 --- step size 32 loss 1.1016281843185425\nepoch 3 --- step 30 --- step size 32 loss 0.8987478017807007\nepoch 3 --- step 31 --- step size 32 loss 0.9904516935348511\nepoch 3 --- step 32 --- step size 32 loss 1.051121473312378\nepoch 3 --- step 33 --- step size 32 loss 0.8577544093132019\nepoch 3 --- step 34 --- step size 32 loss 0.9644769430160522\nepoch 3 --- step 35 --- step size 32 loss 0.9583396911621094\nepoch 3 --- step 36 --- step size 32 loss 1.0238590240478516\nepoch 3 --- step 37 --- step size 32 loss 0.7726891040802002\nepoch 3 --- step 38 --- step size 32 loss 1.1265146732330322\nepoch 3 --- step 39 --- step size 32 loss 1.0984139442443848\nepoch 3 --- step 40 --- step size 32 loss 1.191381812095642\nepoch 3 --- step 41 --- step size 32 loss 0.7487626075744629\nepoch 3 --- step 42 --- step size 32 loss 1.046829104423523\nepoch 3 --- step 43 --- step size 32 loss 1.111149787902832\nepoch 3 --- step 44 --- step size 32 loss 0.8882162570953369\nepoch 3 --- step 45 --- step size 32 loss 1.3243963718414307\nepoch 3 --- step 46 --- step size 32 loss 0.9154809713363647\nepoch 3 --- step 47 --- step size 32 loss 1.3134918212890625\nepoch 3 --- step 48 --- step size 32 loss 1.0790237188339233\nepoch 3 --- step 49 --- step size 32 loss 1.0842063426971436\nepoch 3 --- step 50 --- step size 32 loss 0.8634819984436035\nepoch 3 --- step 51 --- step size 32 loss 0.7887955904006958\nepoch 3 --- step 52 --- step size 32 loss 1.1491928100585938\nepoch 3 --- step 53 --- step size 32 loss 1.3279836177825928\nepoch 3 --- step 54 --- step size 32 loss 0.7474679946899414\nepoch 3 --- step 55 --- step size 32 loss 1.1400084495544434\nepoch 3 --- step 56 --- step size 32 loss 1.092780351638794\nepoch 3 --- step 57 --- step size 32 loss 1.082940936088562\nepoch 3 --- step 58 --- step size 32 loss 1.058972716331482\nepoch 3 --- step 59 --- step size 32 loss 0.8711804747581482\nepoch 3 --- step 60 --- step size 32 loss 1.0829799175262451\nepoch 3 --- step 61 --- step size 32 loss 1.1236400604248047\nepoch 3 --- step 62 --- step size 32 loss 0.8700364828109741\nepoch 3 --- step 63 --- step size 32 loss 0.8614338636398315\nepoch 3 --- step 64 --- step size 32 loss 0.6486207246780396\nepoch 3 --- step 65 --- step size 32 loss 0.9233803749084473\nepoch 3 --- step 66 --- step size 32 loss 1.2198798656463623\nepoch 3 --- step 67 --- step size 32 loss 1.2558271884918213\nepoch 3 --- step 68 --- step size 32 loss 0.8777544498443604\nepoch 3 --- step 69 --- step size 32 loss 0.9167606830596924\nepoch 3 --- step 70 --- step size 32 loss 1.0971062183380127\nepoch 3 --- step 71 --- step size 32 loss 0.6537435054779053\nepoch 3 --- step 72 --- step size 32 loss 1.0218642950057983\nepoch 3 --- step 73 --- step size 32 loss 0.9126678705215454\nepoch 3 --- step 74 --- step size 32 loss 0.895072877407074\nepoch 3 --- step 75 --- step size 32 loss 1.0391240119934082\nepoch 3 --- step 76 --- step size 32 loss 1.3403056859970093\nepoch 3 --- step 77 --- step size 32 loss 0.8874050378799438\nepoch 3 --- step 78 --- step size 32 loss 1.2387363910675049\nepoch 3 --- step 79 --- step size 32 loss 1.1069787740707397\nepoch 3 --- step 80 --- step size 32 loss 0.9409162402153015\nepoch 3 --- step 81 --- step size 32 loss 0.898651123046875\nepoch 3 --- step 82 --- step size 32 loss 0.5757051706314087\nepoch 3 --- step 83 --- step size 32 loss 1.0113524198532104\nepoch 3 --- step 84 --- step size 32 loss 1.1315126419067383\nepoch 3 --- step 85 --- step size 32 loss 1.0367577075958252\nepoch 3 --- step 86 --- step size 32 loss 1.0333070755004883\nepoch 3 --- step 87 --- step size 32 loss 0.9938615560531616\nepoch 3 --- step 88 --- step size 32 loss 1.0992978811264038\nepoch 3 --- step 89 --- step size 32 loss 1.0247007608413696\nepoch 3 --- step 90 --- step size 32 loss 0.9461908340454102\nepoch 3 --- step 91 --- step size 32 loss 1.075395941734314\nepoch 3 --- step 92 --- step size 32 loss 1.1270896196365356\nepoch 3 --- step 93 --- step size 32 loss 0.8335617780685425\nepoch 3 --- step 94 --- step size 32 loss 0.7968897223472595\nepoch 3 --- step 95 --- step size 32 loss 1.235309362411499\nepoch 3 --- step 96 --- step size 32 loss 0.8805407881736755\nepoch 3 --- step 97 --- step size 32 loss 1.1299855709075928\nepoch 3 --- step 98 --- step size 32 loss 0.9662875533103943\nepoch 3 --- step 99 --- step size 32 loss 1.3190113306045532\nepoch 3 --- step 100 --- step size 32 loss 1.147702932357788\nepoch 3 --- step 101 --- step size 32 loss 0.989530086517334\nepoch 3 --- step 102 --- step size 32 loss 1.047966718673706\nepoch 3 --- step 103 --- step size 32 loss 1.0160465240478516\nepoch 3 --- step 104 --- step size 32 loss 1.0701674222946167\nepoch 3 --- step 105 --- step size 32 loss 0.9932045936584473\nepoch 3 --- step 106 --- step size 32 loss 1.0757770538330078\nepoch 3 --- step 107 --- step size 32 loss 0.8512085676193237\nepoch 3 --- step 108 --- step size 32 loss 1.2778571844100952\nepoch 3 --- step 109 --- step size 32 loss 0.8201373815536499\nepoch 3 --- step 110 --- step size 32 loss 0.8143331408500671\nepoch 3 --- step 111 --- step size 32 loss 0.8525751829147339\nepoch 3 --- step 112 --- step size 32 loss 1.443938136100769\nepoch 3 --- step 113 --- step size 32 loss 0.9111742973327637\nepoch 3 --- step 114 --- step size 32 loss 0.883786678314209\nepoch 3 --- step 115 --- step size 32 loss 1.0285131931304932\nepoch 3 --- step 116 --- step size 32 loss 0.8293659090995789\nepoch 3 --- step 117 --- step size 32 loss 1.0499670505523682\nepoch 3 --- step 118 --- step size 32 loss 1.0342882871627808\nepoch 3 --- step 119 --- step size 32 loss 0.981837272644043\nepoch 3 --- step 120 --- step size 32 loss 1.3159763813018799\nepoch 3 --- step 121 --- step size 32 loss 0.9370361566543579\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 4 --- step 0 --- step size 32 loss 1.0021775960922241\nepoch 4 --- step 1 --- step size 32 loss 0.8883436918258667\nepoch 4 --- step 2 --- step size 32 loss 1.04630446434021\nepoch 4 --- step 3 --- step size 32 loss 1.193956732749939\nepoch 4 --- step 4 --- step size 32 loss 0.9205971956253052\nepoch 4 --- step 5 --- step size 32 loss 0.9954632520675659\nepoch 4 --- step 6 --- step size 32 loss 0.8895813822746277\nepoch 4 --- step 7 --- step size 32 loss 1.0067473649978638\nepoch 4 --- step 8 --- step size 32 loss 1.0647318363189697\nepoch 4 --- step 9 --- step size 32 loss 1.2216904163360596\nepoch 4 --- step 10 --- step size 32 loss 1.0764868259429932\nepoch 4 --- step 11 --- step size 32 loss 0.7904078364372253\nepoch 4 --- step 12 --- step size 32 loss 1.229978084564209\nepoch 4 --- step 13 --- step size 32 loss 0.7029874324798584\nepoch 4 --- step 14 --- step size 32 loss 1.095947027206421\nepoch 4 --- step 15 --- step size 32 loss 0.7513853907585144\nepoch 4 --- step 16 --- step size 32 loss 0.9103147387504578\nepoch 4 --- step 17 --- step size 32 loss 0.7963443994522095\nepoch 4 --- step 18 --- step size 32 loss 0.7925500869750977\nepoch 4 --- step 19 --- step size 32 loss 0.7414613962173462\nepoch 4 --- step 20 --- step size 32 loss 0.8234607577323914\nepoch 4 --- step 21 --- step size 32 loss 0.827459990978241\nepoch 4 --- step 22 --- step size 32 loss 0.8041497468948364\nepoch 4 --- step 23 --- step size 32 loss 0.6899206638336182\nepoch 4 --- step 24 --- step size 32 loss 1.0238068103790283\nepoch 4 --- step 25 --- step size 32 loss 0.673048734664917\nepoch 4 --- step 26 --- step size 32 loss 1.1333520412445068\nepoch 4 --- step 27 --- step size 32 loss 0.9332100749015808\nepoch 4 --- step 28 --- step size 32 loss 1.149733066558838\nepoch 4 --- step 29 --- step size 32 loss 0.9734334945678711\nepoch 4 --- step 30 --- step size 32 loss 0.8707423210144043\nepoch 4 --- step 31 --- step size 32 loss 1.031530737876892\nepoch 4 --- step 32 --- step size 32 loss 1.0598784685134888\nepoch 4 --- step 33 --- step size 32 loss 0.801337480545044\nepoch 4 --- step 34 --- step size 32 loss 0.8825691938400269\nepoch 4 --- step 35 --- step size 32 loss 0.9078032374382019\nepoch 4 --- step 36 --- step size 32 loss 0.9571489095687866\nepoch 4 --- step 37 --- step size 32 loss 0.9625824689865112\nepoch 4 --- step 38 --- step size 32 loss 0.8283064365386963\nepoch 4 --- step 39 --- step size 32 loss 1.0945085287094116\nepoch 4 --- step 40 --- step size 32 loss 1.18305242061615\nepoch 4 --- step 41 --- step size 32 loss 0.9905540347099304\nepoch 4 --- step 42 --- step size 32 loss 1.0414832830429077\nepoch 4 --- step 43 --- step size 32 loss 1.003753900527954\nepoch 4 --- step 44 --- step size 32 loss 0.862050473690033\nepoch 4 --- step 45 --- step size 32 loss 1.0720767974853516\nepoch 4 --- step 46 --- step size 32 loss 1.381540060043335\nepoch 4 --- step 47 --- step size 32 loss 1.0552551746368408\nepoch 4 --- step 48 --- step size 32 loss 1.0246422290802002\nepoch 4 --- step 49 --- step size 32 loss 1.1373038291931152\nepoch 4 --- step 50 --- step size 32 loss 0.9671100974082947\nepoch 4 --- step 51 --- step size 32 loss 1.0114669799804688\nepoch 4 --- step 52 --- step size 32 loss 1.0224761962890625\nepoch 4 --- step 53 --- step size 32 loss 0.7764077186584473\nepoch 4 --- step 54 --- step size 32 loss 0.9749578237533569\nepoch 4 --- step 55 --- step size 32 loss 0.8540407419204712\nepoch 4 --- step 56 --- step size 32 loss 0.9047693610191345\nepoch 4 --- step 57 --- step size 32 loss 1.1339627504348755\nepoch 4 --- step 58 --- step size 32 loss 1.0658020973205566\nepoch 4 --- step 59 --- step size 32 loss 0.9505565166473389\nepoch 4 --- step 60 --- step size 32 loss 0.986638069152832\nepoch 4 --- step 61 --- step size 32 loss 0.8788816332817078\nepoch 4 --- step 62 --- step size 32 loss 1.080460548400879\nepoch 4 --- step 63 --- step size 32 loss 0.9145631790161133\nepoch 4 --- step 64 --- step size 32 loss 1.063920021057129\nepoch 4 --- step 65 --- step size 32 loss 0.9158262014389038\nepoch 4 --- step 66 --- step size 32 loss 0.7182843089103699\nepoch 4 --- step 67 --- step size 32 loss 0.797866940498352\nepoch 4 --- step 68 --- step size 32 loss 1.2381423711776733\nepoch 4 --- step 69 --- step size 32 loss 0.991654634475708\nepoch 4 --- step 70 --- step size 32 loss 0.7475131750106812\nepoch 4 --- step 71 --- step size 32 loss 0.6396881937980652\nepoch 4 --- step 72 --- step size 32 loss 0.7862977981567383\nepoch 4 --- step 73 --- step size 32 loss 0.7177984118461609\nepoch 4 --- step 74 --- step size 32 loss 0.906354546546936\nepoch 4 --- step 75 --- step size 32 loss 0.6788088083267212\nepoch 4 --- step 76 --- step size 32 loss 0.8704888224601746\nepoch 4 --- step 77 --- step size 32 loss 0.6919242143630981\nepoch 4 --- step 78 --- step size 32 loss 1.0654810667037964\nepoch 4 --- step 79 --- step size 32 loss 1.1879104375839233\nepoch 4 --- step 80 --- step size 32 loss 1.1240336894989014\nepoch 4 --- step 81 --- step size 32 loss 0.9566278457641602\nepoch 4 --- step 82 --- step size 32 loss 0.9775174856185913\nepoch 4 --- step 83 --- step size 32 loss 0.8365663290023804\nepoch 4 --- step 84 --- step size 32 loss 0.7658690214157104\nepoch 4 --- step 85 --- step size 32 loss 0.8409559726715088\nepoch 4 --- step 86 --- step size 32 loss 0.906507670879364\nepoch 4 --- step 87 --- step size 32 loss 0.7916111946105957\nepoch 4 --- step 88 --- step size 32 loss 0.9473599195480347\nepoch 4 --- step 89 --- step size 32 loss 1.042059302330017\nepoch 4 --- step 90 --- step size 32 loss 1.1103723049163818\nepoch 4 --- step 91 --- step size 32 loss 0.8390550017356873\nepoch 4 --- step 92 --- step size 32 loss 0.8511853218078613\nepoch 4 --- step 93 --- step size 32 loss 0.8046852350234985\nepoch 4 --- step 94 --- step size 32 loss 0.7773938179016113\nepoch 4 --- step 95 --- step size 32 loss 0.8237115144729614\nepoch 4 --- step 96 --- step size 32 loss 0.9819828271865845\nepoch 4 --- step 97 --- step size 32 loss 0.7895247340202332\nepoch 4 --- step 98 --- step size 32 loss 0.8634452223777771\nepoch 4 --- step 99 --- step size 32 loss 0.8850773572921753\nepoch 4 --- step 100 --- step size 32 loss 0.9742349982261658\nepoch 4 --- step 101 --- step size 32 loss 1.0439752340316772\nepoch 4 --- step 102 --- step size 32 loss 1.0426796674728394\nepoch 4 --- step 103 --- step size 32 loss 0.893086850643158\nepoch 4 --- step 104 --- step size 32 loss 1.2586047649383545\nepoch 4 --- step 105 --- step size 32 loss 0.7069126963615417\nepoch 4 --- step 106 --- step size 32 loss 0.6586289405822754\nepoch 4 --- step 107 --- step size 32 loss 1.1289271116256714\nepoch 4 --- step 108 --- step size 32 loss 0.8967283964157104\nepoch 4 --- step 109 --- step size 32 loss 0.8213967084884644\nepoch 4 --- step 110 --- step size 32 loss 0.8777626752853394\nepoch 4 --- step 111 --- step size 32 loss 0.9936391115188599\nepoch 4 --- step 112 --- step size 32 loss 0.8747279644012451\nepoch 4 --- step 113 --- step size 32 loss 1.0539145469665527\nepoch 4 --- step 114 --- step size 32 loss 0.7544028162956238\nepoch 4 --- step 115 --- step size 32 loss 0.9802811145782471\nepoch 4 --- step 116 --- step size 32 loss 0.6809988021850586\nepoch 4 --- step 117 --- step size 32 loss 0.9859471321105957\nepoch 4 --- step 118 --- step size 32 loss 1.1939992904663086\nepoch 4 --- step 119 --- step size 32 loss 0.7443575263023376\nepoch 4 --- step 120 --- step size 32 loss 0.8777784109115601\nepoch 4 --- step 121 --- step size 32 loss 0.8587223887443542\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 5 --- step 0 --- step size 32 loss 0.6874631643295288\nepoch 5 --- step 1 --- step size 32 loss 0.9219799041748047\nepoch 5 --- step 2 --- step size 32 loss 0.9414147734642029\nepoch 5 --- step 3 --- step size 32 loss 1.0608611106872559\nepoch 5 --- step 4 --- step size 32 loss 1.0212328433990479\nepoch 5 --- step 5 --- step size 32 loss 0.7373861074447632\nepoch 5 --- step 6 --- step size 32 loss 0.7290773391723633\nepoch 5 --- step 7 --- step size 32 loss 0.8841294050216675\nepoch 5 --- step 8 --- step size 32 loss 0.773552656173706\nepoch 5 --- step 9 --- step size 32 loss 0.8199908137321472\nepoch 5 --- step 10 --- step size 32 loss 0.8379899859428406\nepoch 5 --- step 11 --- step size 32 loss 1.057822585105896\nepoch 5 --- step 12 --- step size 32 loss 0.8003783226013184\nepoch 5 --- step 13 --- step size 32 loss 0.8412742614746094\nepoch 5 --- step 14 --- step size 32 loss 0.7406493425369263\nepoch 5 --- step 15 --- step size 32 loss 0.7789849638938904\nepoch 5 --- step 16 --- step size 32 loss 1.0724257230758667\nepoch 5 --- step 17 --- step size 32 loss 1.1303889751434326\nepoch 5 --- step 18 --- step size 32 loss 0.9561740159988403\nepoch 5 --- step 19 --- step size 32 loss 0.6215692758560181\nepoch 5 --- step 20 --- step size 32 loss 0.8039878606796265\nepoch 5 --- step 21 --- step size 32 loss 0.8693272471427917\nepoch 5 --- step 22 --- step size 32 loss 0.7669751048088074\nepoch 5 --- step 23 --- step size 32 loss 0.8308323621749878\nepoch 5 --- step 24 --- step size 32 loss 1.1746399402618408\nepoch 5 --- step 25 --- step size 32 loss 0.6013197898864746\nepoch 5 --- step 26 --- step size 32 loss 0.7267407774925232\nepoch 5 --- step 27 --- step size 32 loss 0.9344052076339722\nepoch 5 --- step 28 --- step size 32 loss 1.0072658061981201\nepoch 5 --- step 29 --- step size 32 loss 0.8067106604576111\nepoch 5 --- step 30 --- step size 32 loss 0.8105347156524658\nepoch 5 --- step 31 --- step size 32 loss 0.7735936045646667\nepoch 5 --- step 32 --- step size 32 loss 0.7469417452812195\nepoch 5 --- step 33 --- step size 32 loss 0.7541979551315308\nepoch 5 --- step 34 --- step size 32 loss 0.744141697883606\nepoch 5 --- step 35 --- step size 32 loss 1.0366594791412354\nepoch 5 --- step 36 --- step size 32 loss 0.996042013168335\nepoch 5 --- step 37 --- step size 32 loss 0.9893883466720581\nepoch 5 --- step 38 --- step size 32 loss 0.768624484539032\nepoch 5 --- step 39 --- step size 32 loss 0.8292467594146729\nepoch 5 --- step 40 --- step size 32 loss 0.9707192182540894\nepoch 5 --- step 41 --- step size 32 loss 0.7910399436950684\nepoch 5 --- step 42 --- step size 32 loss 1.3409759998321533\nepoch 5 --- step 43 --- step size 32 loss 1.0251011848449707\nepoch 5 --- step 44 --- step size 32 loss 0.7707734107971191\nepoch 5 --- step 45 --- step size 32 loss 1.049028754234314\nepoch 5 --- step 46 --- step size 32 loss 0.797919511795044\nepoch 5 --- step 47 --- step size 32 loss 0.8237185478210449\nepoch 5 --- step 48 --- step size 32 loss 0.819890022277832\nepoch 5 --- step 49 --- step size 32 loss 0.8620675802230835\nepoch 5 --- step 50 --- step size 32 loss 0.9492143392562866\nepoch 5 --- step 51 --- step size 32 loss 0.6352200508117676\nepoch 5 --- step 52 --- step size 32 loss 0.7869691252708435\nepoch 5 --- step 53 --- step size 32 loss 0.7578288912773132\nepoch 5 --- step 54 --- step size 32 loss 0.9058554172515869\nepoch 5 --- step 55 --- step size 32 loss 0.7655824422836304\nepoch 5 --- step 56 --- step size 32 loss 0.6361054182052612\nepoch 5 --- step 57 --- step size 32 loss 0.8839631080627441\nepoch 5 --- step 58 --- step size 32 loss 0.8810696601867676\nepoch 5 --- step 59 --- step size 32 loss 1.0346862077713013\nepoch 5 --- step 60 --- step size 32 loss 0.803355872631073\nepoch 5 --- step 61 --- step size 32 loss 0.783667802810669\nepoch 5 --- step 62 --- step size 32 loss 1.0880746841430664\nepoch 5 --- step 63 --- step size 32 loss 1.1205393075942993\nepoch 5 --- step 64 --- step size 32 loss 0.7694061994552612\nepoch 5 --- step 65 --- step size 32 loss 0.8776745796203613\nepoch 5 --- step 66 --- step size 32 loss 0.7938259840011597\nepoch 5 --- step 67 --- step size 32 loss 0.9179777503013611\nepoch 5 --- step 68 --- step size 32 loss 0.6478453278541565\nepoch 5 --- step 69 --- step size 32 loss 0.6406234502792358\nepoch 5 --- step 70 --- step size 32 loss 0.6729652285575867\nepoch 5 --- step 71 --- step size 32 loss 0.8363341689109802\nepoch 5 --- step 72 --- step size 32 loss 0.790818452835083\nepoch 5 --- step 73 --- step size 32 loss 0.6713515520095825\nepoch 5 --- step 74 --- step size 32 loss 0.8594667315483093\nepoch 5 --- step 75 --- step size 32 loss 0.9296843409538269\nepoch 5 --- step 76 --- step size 32 loss 0.8612127304077148\nepoch 5 --- step 77 --- step size 32 loss 1.0435922145843506\nepoch 5 --- step 78 --- step size 32 loss 0.6196691989898682\nepoch 5 --- step 79 --- step size 32 loss 0.7022466659545898\nepoch 5 --- step 80 --- step size 32 loss 0.793653666973114\nepoch 5 --- step 81 --- step size 32 loss 1.1269688606262207\nepoch 5 --- step 82 --- step size 32 loss 0.6966278553009033\nepoch 5 --- step 83 --- step size 32 loss 0.7508764266967773\nepoch 5 --- step 84 --- step size 32 loss 0.9694058895111084\nepoch 5 --- step 85 --- step size 32 loss 0.874364972114563\nepoch 5 --- step 86 --- step size 32 loss 0.723544716835022\nepoch 5 --- step 87 --- step size 32 loss 0.9584721326828003\nepoch 5 --- step 88 --- step size 32 loss 0.8464148044586182\nepoch 5 --- step 89 --- step size 32 loss 1.0099533796310425\nepoch 5 --- step 90 --- step size 32 loss 0.9229960441589355\nepoch 5 --- step 91 --- step size 32 loss 0.7051109075546265\nepoch 5 --- step 92 --- step size 32 loss 0.7898397445678711\nepoch 5 --- step 93 --- step size 32 loss 0.8636832237243652\nepoch 5 --- step 94 --- step size 32 loss 0.8983151912689209\nepoch 5 --- step 95 --- step size 32 loss 0.9452888369560242\nepoch 5 --- step 96 --- step size 32 loss 0.9623017311096191\nepoch 5 --- step 97 --- step size 32 loss 0.8722518682479858\nepoch 5 --- step 98 --- step size 32 loss 1.0268796682357788\nepoch 5 --- step 99 --- step size 32 loss 0.9340934753417969\nepoch 5 --- step 100 --- step size 32 loss 0.9314826726913452\nepoch 5 --- step 101 --- step size 32 loss 0.9112197756767273\nepoch 5 --- step 102 --- step size 32 loss 1.0534377098083496\nepoch 5 --- step 103 --- step size 32 loss 0.8260587453842163\nepoch 5 --- step 104 --- step size 32 loss 0.8119165301322937\nepoch 5 --- step 105 --- step size 32 loss 0.7233173251152039\nepoch 5 --- step 106 --- step size 32 loss 0.8769118189811707\nepoch 5 --- step 107 --- step size 32 loss 0.6941943168640137\nepoch 5 --- step 108 --- step size 32 loss 0.8603501319885254\nepoch 5 --- step 109 --- step size 32 loss 0.8624684810638428\nepoch 5 --- step 110 --- step size 32 loss 0.6891508102416992\nepoch 5 --- step 111 --- step size 32 loss 0.7327584624290466\nepoch 5 --- step 112 --- step size 32 loss 0.9484896063804626\nepoch 5 --- step 113 --- step size 32 loss 0.6689063310623169\nepoch 5 --- step 114 --- step size 32 loss 0.576359212398529\nepoch 5 --- step 115 --- step size 32 loss 0.7945876121520996\nepoch 5 --- step 116 --- step size 32 loss 0.666003942489624\nepoch 5 --- step 117 --- step size 32 loss 0.8863816261291504\nepoch 5 --- step 118 --- step size 32 loss 0.8727275729179382\nepoch 5 --- step 119 --- step size 32 loss 0.9201846122741699\nepoch 5 --- step 120 --- step size 32 loss 1.0414979457855225\nepoch 5 --- step 121 --- step size 32 loss 0.8556629419326782\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 6 --- step 0 --- step size 32 loss 1.035840392112732\nepoch 6 --- step 1 --- step size 32 loss 0.8038513660430908\nepoch 6 --- step 2 --- step size 32 loss 0.8911392092704773\nepoch 6 --- step 3 --- step size 32 loss 0.8971449732780457\nepoch 6 --- step 4 --- step size 32 loss 0.599969744682312\nepoch 6 --- step 5 --- step size 32 loss 0.9239029884338379\nepoch 6 --- step 6 --- step size 32 loss 0.8480814695358276\nepoch 6 --- step 7 --- step size 32 loss 0.9310060739517212\nepoch 6 --- step 8 --- step size 32 loss 1.0460331439971924\nepoch 6 --- step 9 --- step size 32 loss 0.8564097285270691\nepoch 6 --- step 10 --- step size 32 loss 0.961037814617157\nepoch 6 --- step 11 --- step size 32 loss 0.7574424743652344\nepoch 6 --- step 12 --- step size 32 loss 0.9451439380645752\nepoch 6 --- step 13 --- step size 32 loss 0.9186372756958008\nepoch 6 --- step 14 --- step size 32 loss 0.8685213327407837\nepoch 6 --- step 15 --- step size 32 loss 0.8400440216064453\nepoch 6 --- step 16 --- step size 32 loss 0.8572689294815063\nepoch 6 --- step 17 --- step size 32 loss 0.9925708770751953\nepoch 6 --- step 18 --- step size 32 loss 0.9398309588432312\nepoch 6 --- step 19 --- step size 32 loss 0.6229780316352844\nepoch 6 --- step 20 --- step size 32 loss 0.7810086011886597\nepoch 6 --- step 21 --- step size 32 loss 0.8906404376029968\nepoch 6 --- step 22 --- step size 32 loss 1.0834918022155762\nepoch 6 --- step 23 --- step size 32 loss 0.7696399688720703\nepoch 6 --- step 24 --- step size 32 loss 0.8744260668754578\nepoch 6 --- step 25 --- step size 32 loss 0.7080652117729187\nepoch 6 --- step 26 --- step size 32 loss 1.0308384895324707\nepoch 6 --- step 27 --- step size 32 loss 0.6194273829460144\nepoch 6 --- step 28 --- step size 32 loss 0.7304431796073914\nepoch 6 --- step 29 --- step size 32 loss 0.7913371920585632\nepoch 6 --- step 30 --- step size 32 loss 0.8181609511375427\nepoch 6 --- step 31 --- step size 32 loss 0.7832940816879272\nepoch 6 --- step 32 --- step size 32 loss 0.7264209985733032\nepoch 6 --- step 33 --- step size 32 loss 0.6695727705955505\nepoch 6 --- step 34 --- step size 32 loss 0.8474886417388916\nepoch 6 --- step 35 --- step size 32 loss 0.8158396482467651\nepoch 6 --- step 36 --- step size 32 loss 0.9373459815979004\nepoch 6 --- step 37 --- step size 32 loss 0.7506012320518494\nepoch 6 --- step 38 --- step size 32 loss 0.7332970499992371\nepoch 6 --- step 39 --- step size 32 loss 0.8660917282104492\nepoch 6 --- step 40 --- step size 32 loss 0.8704468607902527\nepoch 6 --- step 41 --- step size 32 loss 1.1115643978118896\nepoch 6 --- step 42 --- step size 32 loss 0.7237727046012878\nepoch 6 --- step 43 --- step size 32 loss 0.5547246932983398\nepoch 6 --- step 44 --- step size 32 loss 0.8463083505630493\nepoch 6 --- step 45 --- step size 32 loss 0.7781239748001099\nepoch 6 --- step 46 --- step size 32 loss 0.8871865272521973\nepoch 6 --- step 47 --- step size 32 loss 0.7330853939056396\nepoch 6 --- step 48 --- step size 32 loss 0.6776501536369324\nepoch 6 --- step 49 --- step size 32 loss 0.7100874185562134\nepoch 6 --- step 50 --- step size 32 loss 0.9481686353683472\nepoch 6 --- step 51 --- step size 32 loss 0.8116892576217651\nepoch 6 --- step 52 --- step size 32 loss 0.976182222366333\nepoch 6 --- step 53 --- step size 32 loss 0.6448919177055359\nepoch 6 --- step 54 --- step size 32 loss 0.6799361109733582\nepoch 6 --- step 55 --- step size 32 loss 0.9825630187988281\nepoch 6 --- step 56 --- step size 32 loss 0.8592643737792969\nepoch 6 --- step 57 --- step size 32 loss 0.8813371062278748\nepoch 6 --- step 58 --- step size 32 loss 0.5504636168479919\nepoch 6 --- step 59 --- step size 32 loss 0.7777714729309082\nepoch 6 --- step 60 --- step size 32 loss 0.6362795829772949\nepoch 6 --- step 61 --- step size 32 loss 0.8735886812210083\nepoch 6 --- step 62 --- step size 32 loss 0.6500719785690308\nepoch 6 --- step 63 --- step size 32 loss 0.7055265307426453\nepoch 6 --- step 64 --- step size 32 loss 0.8420225977897644\nepoch 6 --- step 65 --- step size 32 loss 0.9834269881248474\nepoch 6 --- step 66 --- step size 32 loss 0.9231225848197937\nepoch 6 --- step 67 --- step size 32 loss 0.6174915432929993\nepoch 6 --- step 68 --- step size 32 loss 0.7587792277336121\nepoch 6 --- step 69 --- step size 32 loss 0.95341956615448\nepoch 6 --- step 70 --- step size 32 loss 0.7623394727706909\nepoch 6 --- step 71 --- step size 32 loss 0.7084792852401733\nepoch 6 --- step 72 --- step size 32 loss 0.6597010493278503\nepoch 6 --- step 73 --- step size 32 loss 0.6863750219345093\nepoch 6 --- step 74 --- step size 32 loss 0.9672624468803406\nepoch 6 --- step 75 --- step size 32 loss 0.812365710735321\nepoch 6 --- step 76 --- step size 32 loss 0.5772824287414551\nepoch 6 --- step 77 --- step size 32 loss 0.6136438250541687\nepoch 6 --- step 78 --- step size 32 loss 0.7120475769042969\nepoch 6 --- step 79 --- step size 32 loss 0.7310975790023804\nepoch 6 --- step 80 --- step size 32 loss 0.9106673002243042\nepoch 6 --- step 81 --- step size 32 loss 0.7068738341331482\nepoch 6 --- step 82 --- step size 32 loss 0.7928637266159058\nepoch 6 --- step 83 --- step size 32 loss 0.7406178712844849\nepoch 6 --- step 84 --- step size 32 loss 0.7465639114379883\nepoch 6 --- step 85 --- step size 32 loss 0.7961735725402832\nepoch 6 --- step 86 --- step size 32 loss 0.6484476327896118\nepoch 6 --- step 87 --- step size 32 loss 0.8171054720878601\nepoch 6 --- step 88 --- step size 32 loss 0.932039737701416\nepoch 6 --- step 89 --- step size 32 loss 0.908868670463562\nepoch 6 --- step 90 --- step size 32 loss 0.7961443662643433\nepoch 6 --- step 91 --- step size 32 loss 0.8343745470046997\nepoch 6 --- step 92 --- step size 32 loss 0.7193635702133179\nepoch 6 --- step 93 --- step size 32 loss 0.6980222463607788\nepoch 6 --- step 94 --- step size 32 loss 0.8552872538566589\nepoch 6 --- step 95 --- step size 32 loss 0.6541916728019714\nepoch 6 --- step 96 --- step size 32 loss 0.8384355306625366\nepoch 6 --- step 97 --- step size 32 loss 0.737440288066864\nepoch 6 --- step 98 --- step size 32 loss 0.5859065055847168\nepoch 6 --- step 99 --- step size 32 loss 0.8741515874862671\nepoch 6 --- step 100 --- step size 32 loss 0.7128291130065918\nepoch 6 --- step 101 --- step size 32 loss 0.6832236051559448\nepoch 6 --- step 102 --- step size 32 loss 0.6233376860618591\nepoch 6 --- step 103 --- step size 32 loss 1.004346489906311\nepoch 6 --- step 104 --- step size 32 loss 0.6911711692810059\nepoch 6 --- step 105 --- step size 32 loss 0.6834589838981628\nepoch 6 --- step 106 --- step size 32 loss 0.8863493800163269\nepoch 6 --- step 107 --- step size 32 loss 0.556818962097168\nepoch 6 --- step 108 --- step size 32 loss 0.8584175109863281\nepoch 6 --- step 109 --- step size 32 loss 1.0403131246566772\nepoch 6 --- step 110 --- step size 32 loss 0.6044148206710815\nepoch 6 --- step 111 --- step size 32 loss 0.7743441462516785\nepoch 6 --- step 112 --- step size 32 loss 0.7353847026824951\nepoch 6 --- step 113 --- step size 32 loss 0.8104249835014343\nepoch 6 --- step 114 --- step size 32 loss 0.8851560354232788\nepoch 6 --- step 115 --- step size 32 loss 0.6818803548812866\nepoch 6 --- step 116 --- step size 32 loss 0.9287111759185791\nepoch 6 --- step 117 --- step size 32 loss 0.7288250923156738\nepoch 6 --- step 118 --- step size 32 loss 0.7445415258407593\nepoch 6 --- step 119 --- step size 32 loss 0.6141272783279419\nepoch 6 --- step 120 --- step size 32 loss 0.6269563436508179\nepoch 6 --- step 121 --- step size 32 loss 0.8169256448745728\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 7 --- step 0 --- step size 32 loss 0.6725301742553711\nepoch 7 --- step 1 --- step size 32 loss 0.9778841733932495\nepoch 7 --- step 2 --- step size 32 loss 0.9795126914978027\nepoch 7 --- step 3 --- step size 32 loss 0.574864387512207\nepoch 7 --- step 4 --- step size 32 loss 0.5677630305290222\nepoch 7 --- step 5 --- step size 32 loss 0.9376203417778015\nepoch 7 --- step 6 --- step size 32 loss 0.7206381559371948\nepoch 7 --- step 7 --- step size 32 loss 0.8179852962493896\nepoch 7 --- step 8 --- step size 32 loss 0.8266403675079346\nepoch 7 --- step 9 --- step size 32 loss 0.8451043963432312\nepoch 7 --- step 10 --- step size 32 loss 0.8853945732116699\nepoch 7 --- step 11 --- step size 32 loss 1.0046075582504272\nepoch 7 --- step 12 --- step size 32 loss 0.978888213634491\nepoch 7 --- step 13 --- step size 32 loss 0.6538918614387512\nepoch 7 --- step 14 --- step size 32 loss 0.8434879183769226\nepoch 7 --- step 15 --- step size 32 loss 0.7714749574661255\nepoch 7 --- step 16 --- step size 32 loss 0.6990326642990112\nepoch 7 --- step 17 --- step size 32 loss 0.7010993361473083\nepoch 7 --- step 18 --- step size 32 loss 0.8586010336875916\nepoch 7 --- step 19 --- step size 32 loss 1.02394700050354\nepoch 7 --- step 20 --- step size 32 loss 0.6911198496818542\nepoch 7 --- step 21 --- step size 32 loss 0.6897263526916504\nepoch 7 --- step 22 --- step size 32 loss 0.7399811744689941\nepoch 7 --- step 23 --- step size 32 loss 1.0077507495880127\nepoch 7 --- step 24 --- step size 32 loss 0.607144832611084\nepoch 7 --- step 25 --- step size 32 loss 0.6539193391799927\nepoch 7 --- step 26 --- step size 32 loss 0.8886398673057556\nepoch 7 --- step 27 --- step size 32 loss 0.8852044939994812\nepoch 7 --- step 28 --- step size 32 loss 0.9405181407928467\nepoch 7 --- step 29 --- step size 32 loss 0.9489597678184509\nepoch 7 --- step 30 --- step size 32 loss 0.7590862512588501\nepoch 7 --- step 31 --- step size 32 loss 0.7102987766265869\nepoch 7 --- step 32 --- step size 32 loss 0.6644458174705505\nepoch 7 --- step 33 --- step size 32 loss 1.026658058166504\nepoch 7 --- step 34 --- step size 32 loss 0.8440909385681152\nepoch 7 --- step 35 --- step size 32 loss 0.6993252038955688\nepoch 7 --- step 36 --- step size 32 loss 0.8898168802261353\nepoch 7 --- step 37 --- step size 32 loss 0.7462743520736694\nepoch 7 --- step 38 --- step size 32 loss 0.8716558814048767\nepoch 7 --- step 39 --- step size 32 loss 0.8367635607719421\nepoch 7 --- step 40 --- step size 32 loss 0.6699790358543396\nepoch 7 --- step 41 --- step size 32 loss 0.7809602618217468\nepoch 7 --- step 42 --- step size 32 loss 0.7849849462509155\nepoch 7 --- step 43 --- step size 32 loss 0.8091416358947754\nepoch 7 --- step 44 --- step size 32 loss 0.80817049741745\nepoch 7 --- step 45 --- step size 32 loss 1.0445295572280884\nepoch 7 --- step 46 --- step size 32 loss 0.99202561378479\nepoch 7 --- step 47 --- step size 32 loss 0.8663644790649414\nepoch 7 --- step 48 --- step size 32 loss 0.7549834251403809\nepoch 7 --- step 49 --- step size 32 loss 0.7336248755455017\nepoch 7 --- step 50 --- step size 32 loss 0.7839126586914062\nepoch 7 --- step 51 --- step size 32 loss 0.6837059855461121\nepoch 7 --- step 52 --- step size 32 loss 0.6907678842544556\nepoch 7 --- step 53 --- step size 32 loss 0.7582130432128906\nepoch 7 --- step 54 --- step size 32 loss 0.5498504638671875\nepoch 7 --- step 55 --- step size 32 loss 0.698919951915741\nepoch 7 --- step 56 --- step size 32 loss 0.7182034254074097\nepoch 7 --- step 57 --- step size 32 loss 1.0275423526763916\nepoch 7 --- step 58 --- step size 32 loss 0.8932293653488159\nepoch 7 --- step 59 --- step size 32 loss 0.6688307523727417\nepoch 7 --- step 60 --- step size 32 loss 0.7774592638015747\nepoch 7 --- step 61 --- step size 32 loss 0.7719482183456421\nepoch 7 --- step 62 --- step size 32 loss 0.8689632415771484\nepoch 7 --- step 63 --- step size 32 loss 0.6668844223022461\nepoch 7 --- step 64 --- step size 32 loss 0.6991679072380066\nepoch 7 --- step 65 --- step size 32 loss 0.6960725784301758\nepoch 7 --- step 66 --- step size 32 loss 0.4959859549999237\nepoch 7 --- step 67 --- step size 32 loss 0.6790840029716492\nepoch 7 --- step 68 --- step size 32 loss 0.8841444849967957\nepoch 7 --- step 69 --- step size 32 loss 0.7097250819206238\nepoch 7 --- step 70 --- step size 32 loss 0.6239365935325623\nepoch 7 --- step 71 --- step size 32 loss 0.8041536808013916\nepoch 7 --- step 72 --- step size 32 loss 0.874832034111023\nepoch 7 --- step 73 --- step size 32 loss 0.7172646522521973\nepoch 7 --- step 74 --- step size 32 loss 0.693869948387146\nepoch 7 --- step 75 --- step size 32 loss 0.8818821907043457\nepoch 7 --- step 76 --- step size 32 loss 0.6806190013885498\nepoch 7 --- step 77 --- step size 32 loss 0.7154948711395264\nepoch 7 --- step 78 --- step size 32 loss 0.8297814130783081\nepoch 7 --- step 79 --- step size 32 loss 0.5811282396316528\nepoch 7 --- step 80 --- step size 32 loss 0.8690580129623413\nepoch 7 --- step 81 --- step size 32 loss 0.5337936282157898\nepoch 7 --- step 82 --- step size 32 loss 0.8735616207122803\nepoch 7 --- step 83 --- step size 32 loss 0.5937720537185669\nepoch 7 --- step 84 --- step size 32 loss 0.6252349615097046\nepoch 7 --- step 85 --- step size 32 loss 0.6973719000816345\nepoch 7 --- step 86 --- step size 32 loss 0.48396629095077515\nepoch 7 --- step 87 --- step size 32 loss 0.7218878269195557\nepoch 7 --- step 88 --- step size 32 loss 0.6889955401420593\nepoch 7 --- step 89 --- step size 32 loss 0.8054572939872742\nepoch 7 --- step 90 --- step size 32 loss 0.5708106756210327\nepoch 7 --- step 91 --- step size 32 loss 0.5547695159912109\nepoch 7 --- step 92 --- step size 32 loss 0.6682428121566772\nepoch 7 --- step 93 --- step size 32 loss 0.7479230761528015\nepoch 7 --- step 94 --- step size 32 loss 0.6982671618461609\nepoch 7 --- step 95 --- step size 32 loss 0.7131829261779785\nepoch 7 --- step 96 --- step size 32 loss 0.6634909510612488\nepoch 7 --- step 97 --- step size 32 loss 0.781295657157898\nepoch 7 --- step 98 --- step size 32 loss 0.5518898963928223\nepoch 7 --- step 99 --- step size 32 loss 0.697741687297821\nepoch 7 --- step 100 --- step size 32 loss 0.8660477995872498\nepoch 7 --- step 101 --- step size 32 loss 0.7897933721542358\nepoch 7 --- step 102 --- step size 32 loss 0.7573288679122925\nepoch 7 --- step 103 --- step size 32 loss 0.6237436532974243\nepoch 7 --- step 104 --- step size 32 loss 0.9028921127319336\nepoch 7 --- step 105 --- step size 32 loss 0.62758469581604\nepoch 7 --- step 106 --- step size 32 loss 1.0480797290802002\nepoch 7 --- step 107 --- step size 32 loss 0.6787488460540771\nepoch 7 --- step 108 --- step size 32 loss 0.546022355556488\nepoch 7 --- step 109 --- step size 32 loss 0.637528657913208\nepoch 7 --- step 110 --- step size 32 loss 0.6311962604522705\nepoch 7 --- step 111 --- step size 32 loss 0.7348036766052246\nepoch 7 --- step 112 --- step size 32 loss 0.7785094976425171\nepoch 7 --- step 113 --- step size 32 loss 0.8717247247695923\nepoch 7 --- step 114 --- step size 32 loss 0.8905636668205261\nepoch 7 --- step 115 --- step size 32 loss 0.8565086722373962\nepoch 7 --- step 116 --- step size 32 loss 0.828568160533905\nepoch 7 --- step 117 --- step size 32 loss 0.734101414680481\nepoch 7 --- step 118 --- step size 32 loss 0.6290501356124878\nepoch 7 --- step 119 --- step size 32 loss 0.5692673325538635\nepoch 7 --- step 120 --- step size 32 loss 0.5682045221328735\nepoch 7 --- step 121 --- step size 32 loss 0.8414770364761353\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 8 --- step 0 --- step size 32 loss 0.7080212831497192\nepoch 8 --- step 1 --- step size 32 loss 0.7145084142684937\nepoch 8 --- step 2 --- step size 32 loss 0.8021794557571411\nepoch 8 --- step 3 --- step size 32 loss 0.7408468127250671\nepoch 8 --- step 4 --- step size 32 loss 0.8299428820610046\nepoch 8 --- step 5 --- step size 32 loss 0.850614070892334\nepoch 8 --- step 6 --- step size 32 loss 0.5214703679084778\nepoch 8 --- step 7 --- step size 32 loss 0.704089343547821\nepoch 8 --- step 8 --- step size 32 loss 0.6579766273498535\nepoch 8 --- step 9 --- step size 32 loss 0.6837639212608337\nepoch 8 --- step 10 --- step size 32 loss 0.8841185569763184\nepoch 8 --- step 11 --- step size 32 loss 0.5580471754074097\nepoch 8 --- step 12 --- step size 32 loss 0.611538290977478\nepoch 8 --- step 13 --- step size 32 loss 0.5653799772262573\nepoch 8 --- step 14 --- step size 32 loss 0.6477466225624084\nepoch 8 --- step 15 --- step size 32 loss 0.697677493095398\nepoch 8 --- step 16 --- step size 32 loss 0.6973952054977417\nepoch 8 --- step 17 --- step size 32 loss 0.7851144075393677\nepoch 8 --- step 18 --- step size 32 loss 0.8876672983169556\nepoch 8 --- step 19 --- step size 32 loss 0.5334131717681885\nepoch 8 --- step 20 --- step size 32 loss 0.7261544466018677\nepoch 8 --- step 21 --- step size 32 loss 0.854387640953064\nepoch 8 --- step 22 --- step size 32 loss 1.0707993507385254\nepoch 8 --- step 23 --- step size 32 loss 0.6648526191711426\nepoch 8 --- step 24 --- step size 32 loss 0.8142721056938171\nepoch 8 --- step 25 --- step size 32 loss 0.6627936959266663\nepoch 8 --- step 26 --- step size 32 loss 0.8985337018966675\nepoch 8 --- step 27 --- step size 32 loss 0.7510096430778503\nepoch 8 --- step 28 --- step size 32 loss 0.8647907972335815\nepoch 8 --- step 29 --- step size 32 loss 0.8969526290893555\nepoch 8 --- step 30 --- step size 32 loss 0.6905118823051453\nepoch 8 --- step 31 --- step size 32 loss 0.7359710931777954\nepoch 8 --- step 32 --- step size 32 loss 0.7459893226623535\nepoch 8 --- step 33 --- step size 32 loss 0.6682969927787781\nepoch 8 --- step 34 --- step size 32 loss 0.8994559049606323\nepoch 8 --- step 35 --- step size 32 loss 0.79496830701828\nepoch 8 --- step 36 --- step size 32 loss 0.7219187617301941\nepoch 8 --- step 37 --- step size 32 loss 0.8445107340812683\nepoch 8 --- step 38 --- step size 32 loss 0.6908624172210693\nepoch 8 --- step 39 --- step size 32 loss 0.5852892398834229\nepoch 8 --- step 40 --- step size 32 loss 0.6323897838592529\nepoch 8 --- step 41 --- step size 32 loss 0.8463938236236572\nepoch 8 --- step 42 --- step size 32 loss 0.8743619918823242\nepoch 8 --- step 43 --- step size 32 loss 0.655179500579834\nepoch 8 --- step 44 --- step size 32 loss 0.6670588850975037\nepoch 8 --- step 45 --- step size 32 loss 0.7954429984092712\nepoch 8 --- step 46 --- step size 32 loss 0.7294365763664246\nepoch 8 --- step 47 --- step size 32 loss 0.7906879186630249\nepoch 8 --- step 48 --- step size 32 loss 0.6689936518669128\nepoch 8 --- step 49 --- step size 32 loss 0.6250312328338623\nepoch 8 --- step 50 --- step size 32 loss 0.633671224117279\nepoch 8 --- step 51 --- step size 32 loss 0.6691111326217651\nepoch 8 --- step 52 --- step size 32 loss 0.8192795515060425\nepoch 8 --- step 53 --- step size 32 loss 0.5218772888183594\nepoch 8 --- step 54 --- step size 32 loss 0.8221947550773621\nepoch 8 --- step 55 --- step size 32 loss 0.8357611894607544\nepoch 8 --- step 56 --- step size 32 loss 0.6991832256317139\nepoch 8 --- step 57 --- step size 32 loss 0.6875888705253601\nepoch 8 --- step 58 --- step size 32 loss 0.7770216464996338\nepoch 8 --- step 59 --- step size 32 loss 0.8365227580070496\nepoch 8 --- step 60 --- step size 32 loss 0.6378623247146606\nepoch 8 --- step 61 --- step size 32 loss 0.7172026038169861\nepoch 8 --- step 62 --- step size 32 loss 0.5826396942138672\nepoch 8 --- step 63 --- step size 32 loss 0.7417065501213074\nepoch 8 --- step 64 --- step size 32 loss 0.7365147471427917\nepoch 8 --- step 65 --- step size 32 loss 0.7428925037384033\nepoch 8 --- step 66 --- step size 32 loss 0.4737624526023865\nepoch 8 --- step 67 --- step size 32 loss 0.7466373443603516\nepoch 8 --- step 68 --- step size 32 loss 0.6497732400894165\nepoch 8 --- step 69 --- step size 32 loss 0.7515648603439331\nepoch 8 --- step 70 --- step size 32 loss 0.7419763803482056\nepoch 8 --- step 71 --- step size 32 loss 0.7903701066970825\nepoch 8 --- step 72 --- step size 32 loss 0.771155834197998\nepoch 8 --- step 73 --- step size 32 loss 0.7961721420288086\nepoch 8 --- step 74 --- step size 32 loss 0.6751260161399841\nepoch 8 --- step 75 --- step size 32 loss 0.8228306770324707\nepoch 8 --- step 76 --- step size 32 loss 0.7800204157829285\nepoch 8 --- step 77 --- step size 32 loss 0.8169535398483276\nepoch 8 --- step 78 --- step size 32 loss 0.7903642654418945\nepoch 8 --- step 79 --- step size 32 loss 0.9572276473045349\nepoch 8 --- step 80 --- step size 32 loss 0.6284327507019043\nepoch 8 --- step 81 --- step size 32 loss 0.6810870170593262\nepoch 8 --- step 82 --- step size 32 loss 0.684726893901825\nepoch 8 --- step 83 --- step size 32 loss 0.7216247320175171\nepoch 8 --- step 84 --- step size 32 loss 0.566727876663208\nepoch 8 --- step 85 --- step size 32 loss 0.6777667999267578\nepoch 8 --- step 86 --- step size 32 loss 0.8118219375610352\nepoch 8 --- step 87 --- step size 32 loss 0.8617323637008667\nepoch 8 --- step 88 --- step size 32 loss 0.7201423645019531\nepoch 8 --- step 89 --- step size 32 loss 0.7447699308395386\nepoch 8 --- step 90 --- step size 32 loss 0.6283438205718994\nepoch 8 --- step 91 --- step size 32 loss 0.7908589839935303\nepoch 8 --- step 92 --- step size 32 loss 0.7331835031509399\nepoch 8 --- step 93 --- step size 32 loss 0.9462940096855164\nepoch 8 --- step 94 --- step size 32 loss 0.6685253977775574\nepoch 8 --- step 95 --- step size 32 loss 0.6803926825523376\nepoch 8 --- step 96 --- step size 32 loss 0.6850986480712891\nepoch 8 --- step 97 --- step size 32 loss 0.8568159937858582\nepoch 8 --- step 98 --- step size 32 loss 0.8380352854728699\nepoch 8 --- step 99 --- step size 32 loss 0.7320636510848999\nepoch 8 --- step 100 --- step size 32 loss 0.38999074697494507\nepoch 8 --- step 101 --- step size 32 loss 0.5901495218276978\nepoch 8 --- step 102 --- step size 32 loss 0.5982946753501892\nepoch 8 --- step 103 --- step size 32 loss 0.7025885581970215\nepoch 8 --- step 104 --- step size 32 loss 0.5757145285606384\nepoch 8 --- step 105 --- step size 32 loss 0.6893196105957031\nepoch 8 --- step 106 --- step size 32 loss 0.7234690189361572\nepoch 8 --- step 107 --- step size 32 loss 0.7438840866088867\nepoch 8 --- step 108 --- step size 32 loss 0.5390295386314392\nepoch 8 --- step 109 --- step size 32 loss 0.5519277453422546\nepoch 8 --- step 110 --- step size 32 loss 0.6933428049087524\nepoch 8 --- step 111 --- step size 32 loss 0.8005852699279785\nepoch 8 --- step 112 --- step size 32 loss 0.6596196889877319\nepoch 8 --- step 113 --- step size 32 loss 0.6626887917518616\nepoch 8 --- step 114 --- step size 32 loss 0.6777065992355347\nepoch 8 --- step 115 --- step size 32 loss 0.7174067497253418\nepoch 8 --- step 116 --- step size 32 loss 0.8174394369125366\nepoch 8 --- step 117 --- step size 32 loss 0.851510226726532\nepoch 8 --- step 118 --- step size 32 loss 0.8799428939819336\nepoch 8 --- step 119 --- step size 32 loss 0.6201084852218628\nepoch 8 --- step 120 --- step size 32 loss 0.5331369638442993\nepoch 8 --- step 121 --- step size 32 loss 0.8654381632804871\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 9 --- step 0 --- step size 32 loss 0.6381730437278748\nepoch 9 --- step 1 --- step size 32 loss 0.7776914834976196\nepoch 9 --- step 2 --- step size 32 loss 0.8415529727935791\nepoch 9 --- step 3 --- step size 32 loss 0.895645797252655\nepoch 9 --- step 4 --- step size 32 loss 0.8523887395858765\nepoch 9 --- step 5 --- step size 32 loss 0.6597689986228943\nepoch 9 --- step 6 --- step size 32 loss 0.4611775577068329\nepoch 9 --- step 7 --- step size 32 loss 0.5555219054222107\nepoch 9 --- step 8 --- step size 32 loss 0.49104997515678406\nepoch 9 --- step 9 --- step size 32 loss 0.41784751415252686\nepoch 9 --- step 10 --- step size 32 loss 0.9559122920036316\nepoch 9 --- step 11 --- step size 32 loss 0.5581264495849609\nepoch 9 --- step 12 --- step size 32 loss 0.9784551858901978\nepoch 9 --- step 13 --- step size 32 loss 0.5030515789985657\nepoch 9 --- step 14 --- step size 32 loss 0.6695941686630249\nepoch 9 --- step 15 --- step size 32 loss 0.5940180420875549\nepoch 9 --- step 16 --- step size 32 loss 0.5867689847946167\nepoch 9 --- step 17 --- step size 32 loss 0.49080944061279297\nepoch 9 --- step 18 --- step size 32 loss 0.6336358785629272\nepoch 9 --- step 19 --- step size 32 loss 0.8357831835746765\nepoch 9 --- step 20 --- step size 32 loss 0.4891678988933563\nepoch 9 --- step 21 --- step size 32 loss 0.6110591292381287\nepoch 9 --- step 22 --- step size 32 loss 0.6997731924057007\nepoch 9 --- step 23 --- step size 32 loss 0.5576478242874146\nepoch 9 --- step 24 --- step size 32 loss 0.6441686749458313\nepoch 9 --- step 25 --- step size 32 loss 0.7608077526092529\nepoch 9 --- step 26 --- step size 32 loss 0.902871310710907\nepoch 9 --- step 27 --- step size 32 loss 0.593348503112793\nepoch 9 --- step 28 --- step size 32 loss 1.006861686706543\nepoch 9 --- step 29 --- step size 32 loss 0.7619096636772156\nepoch 9 --- step 30 --- step size 32 loss 0.661126971244812\nepoch 9 --- step 31 --- step size 32 loss 0.639883816242218\nepoch 9 --- step 32 --- step size 32 loss 0.6728755831718445\nepoch 9 --- step 33 --- step size 32 loss 0.6836992502212524\nepoch 9 --- step 34 --- step size 32 loss 0.4218418002128601\nepoch 9 --- step 35 --- step size 32 loss 0.6952956914901733\nepoch 9 --- step 36 --- step size 32 loss 0.8258727788925171\nepoch 9 --- step 37 --- step size 32 loss 0.6858240365982056\nepoch 9 --- step 38 --- step size 32 loss 0.6088542938232422\nepoch 9 --- step 39 --- step size 32 loss 0.6391527652740479\nepoch 9 --- step 40 --- step size 32 loss 0.7027561664581299\nepoch 9 --- step 41 --- step size 32 loss 0.5821690559387207\nepoch 9 --- step 42 --- step size 32 loss 0.5996045470237732\nepoch 9 --- step 43 --- step size 32 loss 0.5812755823135376\nepoch 9 --- step 44 --- step size 32 loss 0.8112718462944031\nepoch 9 --- step 45 --- step size 32 loss 0.6118528842926025\nepoch 9 --- step 46 --- step size 32 loss 0.40236467123031616\nepoch 9 --- step 47 --- step size 32 loss 0.6339491009712219\nepoch 9 --- step 48 --- step size 32 loss 0.5383390188217163\nepoch 9 --- step 49 --- step size 32 loss 0.8486663699150085\nepoch 9 --- step 50 --- step size 32 loss 0.5842928886413574\nepoch 9 --- step 51 --- step size 32 loss 0.5918138027191162\nepoch 9 --- step 52 --- step size 32 loss 0.7218254804611206\nepoch 9 --- step 53 --- step size 32 loss 0.6379808783531189\nepoch 9 --- step 54 --- step size 32 loss 0.8466606140136719\nepoch 9 --- step 55 --- step size 32 loss 0.569137454032898\nepoch 9 --- step 56 --- step size 32 loss 0.9450869560241699\nepoch 9 --- step 57 --- step size 32 loss 0.8102458715438843\nepoch 9 --- step 58 --- step size 32 loss 0.8687781691551208\nepoch 9 --- step 59 --- step size 32 loss 0.6832250952720642\nepoch 9 --- step 60 --- step size 32 loss 0.6832411289215088\nepoch 9 --- step 61 --- step size 32 loss 0.7506841421127319\nepoch 9 --- step 62 --- step size 32 loss 0.5736818313598633\nepoch 9 --- step 63 --- step size 32 loss 0.6655998229980469\nepoch 9 --- step 64 --- step size 32 loss 0.5857105255126953\nepoch 9 --- step 65 --- step size 32 loss 0.6965650320053101\nepoch 9 --- step 66 --- step size 32 loss 0.4715958535671234\nepoch 9 --- step 67 --- step size 32 loss 0.7464677691459656\nepoch 9 --- step 68 --- step size 32 loss 0.7016651630401611\nepoch 9 --- step 69 --- step size 32 loss 0.8870560526847839\nepoch 9 --- step 70 --- step size 32 loss 0.5413774847984314\nepoch 9 --- step 71 --- step size 32 loss 0.6757091283798218\nepoch 9 --- step 72 --- step size 32 loss 0.965241551399231\nepoch 9 --- step 73 --- step size 32 loss 0.7914585471153259\nepoch 9 --- step 74 --- step size 32 loss 0.6176246404647827\nepoch 9 --- step 75 --- step size 32 loss 0.5975508689880371\nepoch 9 --- step 76 --- step size 32 loss 0.550041913986206\nepoch 9 --- step 77 --- step size 32 loss 0.7380990982055664\nepoch 9 --- step 78 --- step size 32 loss 0.5752336382865906\nepoch 9 --- step 79 --- step size 32 loss 0.6791769862174988\nepoch 9 --- step 80 --- step size 32 loss 0.7067450284957886\nepoch 9 --- step 81 --- step size 32 loss 0.5538222193717957\nepoch 9 --- step 82 --- step size 32 loss 0.739586353302002\nepoch 9 --- step 83 --- step size 32 loss 0.7338220477104187\nepoch 9 --- step 84 --- step size 32 loss 0.768984317779541\nepoch 9 --- step 85 --- step size 32 loss 0.5864400863647461\nepoch 9 --- step 86 --- step size 32 loss 0.7497760653495789\nepoch 9 --- step 87 --- step size 32 loss 0.6437113285064697\nepoch 9 --- step 88 --- step size 32 loss 0.5629743337631226\nepoch 9 --- step 89 --- step size 32 loss 0.8673598766326904\nepoch 9 --- step 90 --- step size 32 loss 0.7648152112960815\nepoch 9 --- step 91 --- step size 32 loss 0.4927666187286377\nepoch 9 --- step 92 --- step size 32 loss 0.7408238649368286\nepoch 9 --- step 93 --- step size 32 loss 0.5639721751213074\nepoch 9 --- step 94 --- step size 32 loss 0.7457094192504883\nepoch 9 --- step 95 --- step size 32 loss 0.5651102662086487\nepoch 9 --- step 96 --- step size 32 loss 0.6362069845199585\nepoch 9 --- step 97 --- step size 32 loss 0.7658417224884033\nepoch 9 --- step 98 --- step size 32 loss 0.9413549304008484\nepoch 9 --- step 99 --- step size 32 loss 0.7203814387321472\nepoch 9 --- step 100 --- step size 32 loss 0.7023009061813354\nepoch 9 --- step 101 --- step size 32 loss 0.6163594722747803\nepoch 9 --- step 102 --- step size 32 loss 0.5619106292724609\nepoch 9 --- step 103 --- step size 32 loss 0.5295431613922119\nepoch 9 --- step 104 --- step size 32 loss 0.655605673789978\nepoch 9 --- step 105 --- step size 32 loss 0.7535889148712158\nepoch 9 --- step 106 --- step size 32 loss 0.6073497533798218\nepoch 9 --- step 107 --- step size 32 loss 0.5010404586791992\nepoch 9 --- step 108 --- step size 32 loss 0.794619083404541\nepoch 9 --- step 109 --- step size 32 loss 0.6810598373413086\nepoch 9 --- step 110 --- step size 32 loss 0.5782247185707092\nepoch 9 --- step 111 --- step size 32 loss 0.7614433765411377\nepoch 9 --- step 112 --- step size 32 loss 0.5993269681930542\nepoch 9 --- step 113 --- step size 32 loss 0.5731521844863892\nepoch 9 --- step 114 --- step size 32 loss 0.6720005869865417\nepoch 9 --- step 115 --- step size 32 loss 0.9810839891433716\nepoch 9 --- step 116 --- step size 32 loss 0.7749155759811401\nepoch 9 --- step 117 --- step size 32 loss 0.8725872039794922\nepoch 9 --- step 118 --- step size 32 loss 0.8181352019309998\nepoch 9 --- step 119 --- step size 32 loss 0.4838089942932129\nepoch 9 --- step 120 --- step size 32 loss 0.7227325439453125\nepoch 9 --- step 121 --- step size 32 loss 0.5541104674339294\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 10 --- step 0 --- step size 32 loss 0.6029194593429565\nepoch 10 --- step 1 --- step size 32 loss 0.6589966416358948\nepoch 10 --- step 2 --- step size 32 loss 0.6348452568054199\nepoch 10 --- step 3 --- step size 32 loss 0.7099733352661133\nepoch 10 --- step 4 --- step size 32 loss 0.5001696944236755\nepoch 10 --- step 5 --- step size 32 loss 0.6206339597702026\nepoch 10 --- step 6 --- step size 32 loss 0.5583438873291016\nepoch 10 --- step 7 --- step size 32 loss 0.6093217730522156\nepoch 10 --- step 8 --- step size 32 loss 0.8634888529777527\nepoch 10 --- step 9 --- step size 32 loss 0.6426889300346375\nepoch 10 --- step 10 --- step size 32 loss 0.8504513502120972\nepoch 10 --- step 11 --- step size 32 loss 0.7041780948638916\nepoch 10 --- step 12 --- step size 32 loss 0.5892360210418701\nepoch 10 --- step 13 --- step size 32 loss 0.30662959814071655\nepoch 10 --- step 14 --- step size 32 loss 0.7097091674804688\nepoch 10 --- step 15 --- step size 32 loss 0.6030022501945496\nepoch 10 --- step 16 --- step size 32 loss 0.6309221386909485\nepoch 10 --- step 17 --- step size 32 loss 0.5901469588279724\nepoch 10 --- step 18 --- step size 32 loss 0.7894489169120789\nepoch 10 --- step 19 --- step size 32 loss 0.6322532892227173\nepoch 10 --- step 20 --- step size 32 loss 0.4134750962257385\nepoch 10 --- step 21 --- step size 32 loss 0.6446393728256226\nepoch 10 --- step 22 --- step size 32 loss 0.5500327348709106\nepoch 10 --- step 23 --- step size 32 loss 0.4758070111274719\nepoch 10 --- step 24 --- step size 32 loss 0.4482198655605316\nepoch 10 --- step 25 --- step size 32 loss 0.6301776766777039\nepoch 10 --- step 26 --- step size 32 loss 0.8292924761772156\nepoch 10 --- step 27 --- step size 32 loss 0.6623416543006897\nepoch 10 --- step 28 --- step size 32 loss 0.7463550567626953\nepoch 10 --- step 29 --- step size 32 loss 0.7332519888877869\nepoch 10 --- step 30 --- step size 32 loss 0.8080195188522339\nepoch 10 --- step 31 --- step size 32 loss 0.7856252193450928\nepoch 10 --- step 32 --- step size 32 loss 0.5920455455780029\nepoch 10 --- step 33 --- step size 32 loss 0.729651927947998\nepoch 10 --- step 34 --- step size 32 loss 0.5134942531585693\nepoch 10 --- step 35 --- step size 32 loss 0.7761482000350952\nepoch 10 --- step 36 --- step size 32 loss 0.5543459057807922\nepoch 10 --- step 37 --- step size 32 loss 0.7628442645072937\nepoch 10 --- step 38 --- step size 32 loss 0.7786478996276855\nepoch 10 --- step 39 --- step size 32 loss 0.6672307848930359\nepoch 10 --- step 40 --- step size 32 loss 0.6074800491333008\nepoch 10 --- step 41 --- step size 32 loss 0.5976272225379944\nepoch 10 --- step 42 --- step size 32 loss 0.6980834007263184\nepoch 10 --- step 43 --- step size 32 loss 0.643975019454956\nepoch 10 --- step 44 --- step size 32 loss 0.5071714520454407\nepoch 10 --- step 45 --- step size 32 loss 0.7483086585998535\nepoch 10 --- step 46 --- step size 32 loss 0.6090562343597412\nepoch 10 --- step 47 --- step size 32 loss 0.7120291590690613\nepoch 10 --- step 48 --- step size 32 loss 0.8640873432159424\nepoch 10 --- step 49 --- step size 32 loss 0.5198336839675903\nepoch 10 --- step 50 --- step size 32 loss 0.7033421397209167\nepoch 10 --- step 51 --- step size 32 loss 0.7163047790527344\nepoch 10 --- step 52 --- step size 32 loss 0.6043283939361572\nepoch 10 --- step 53 --- step size 32 loss 0.63926100730896\nepoch 10 --- step 54 --- step size 32 loss 0.7449169158935547\nepoch 10 --- step 55 --- step size 32 loss 0.5204300880432129\nepoch 10 --- step 56 --- step size 32 loss 0.4613909423351288\nepoch 10 --- step 57 --- step size 32 loss 0.5320889949798584\nepoch 10 --- step 58 --- step size 32 loss 0.7148523330688477\nepoch 10 --- step 59 --- step size 32 loss 0.5909634232521057\nepoch 10 --- step 60 --- step size 32 loss 0.5914720892906189\nepoch 10 --- step 61 --- step size 32 loss 0.7236670851707458\nepoch 10 --- step 62 --- step size 32 loss 0.6337149143218994\nepoch 10 --- step 63 --- step size 32 loss 0.5734450221061707\nepoch 10 --- step 64 --- step size 32 loss 0.7410597801208496\nepoch 10 --- step 65 --- step size 32 loss 0.5092461705207825\nepoch 10 --- step 66 --- step size 32 loss 0.539496898651123\nepoch 10 --- step 67 --- step size 32 loss 0.6977661848068237\nepoch 10 --- step 68 --- step size 32 loss 0.497964471578598\nepoch 10 --- step 69 --- step size 32 loss 0.8177845478057861\nepoch 10 --- step 70 --- step size 32 loss 0.5794405937194824\nepoch 10 --- step 71 --- step size 32 loss 0.759325385093689\nepoch 10 --- step 72 --- step size 32 loss 0.655244767665863\nepoch 10 --- step 73 --- step size 32 loss 0.7917054891586304\nepoch 10 --- step 74 --- step size 32 loss 0.5686838030815125\nepoch 10 --- step 75 --- step size 32 loss 0.7219633460044861\nepoch 10 --- step 76 --- step size 32 loss 0.7980181574821472\nepoch 10 --- step 77 --- step size 32 loss 0.6276573538780212\nepoch 10 --- step 78 --- step size 32 loss 0.6023867726325989\nepoch 10 --- step 79 --- step size 32 loss 0.8016471862792969\nepoch 10 --- step 80 --- step size 32 loss 0.4716974198818207\nepoch 10 --- step 81 --- step size 32 loss 0.6849519610404968\nepoch 10 --- step 82 --- step size 32 loss 0.654262900352478\nepoch 10 --- step 83 --- step size 32 loss 0.5953115820884705\nepoch 10 --- step 84 --- step size 32 loss 0.523404061794281\nepoch 10 --- step 85 --- step size 32 loss 0.6347081065177917\nepoch 10 --- step 86 --- step size 32 loss 0.4838830828666687\nepoch 10 --- step 87 --- step size 32 loss 0.6156508326530457\nepoch 10 --- step 88 --- step size 32 loss 0.533454418182373\nepoch 10 --- step 89 --- step size 32 loss 0.730519711971283\nepoch 10 --- step 90 --- step size 32 loss 0.649956464767456\nepoch 10 --- step 91 --- step size 32 loss 0.8929646015167236\nepoch 10 --- step 92 --- step size 32 loss 0.5149533748626709\nepoch 10 --- step 93 --- step size 32 loss 0.6397094130516052\nepoch 10 --- step 94 --- step size 32 loss 0.6739501953125\nepoch 10 --- step 95 --- step size 32 loss 0.5532345771789551\nepoch 10 --- step 96 --- step size 32 loss 0.6181018352508545\nepoch 10 --- step 97 --- step size 32 loss 0.7445634603500366\nepoch 10 --- step 98 --- step size 32 loss 0.915460467338562\nepoch 10 --- step 99 --- step size 32 loss 0.4883922338485718\nepoch 10 --- step 100 --- step size 32 loss 0.67839515209198\nepoch 10 --- step 101 --- step size 32 loss 0.6408069133758545\nepoch 10 --- step 102 --- step size 32 loss 0.9072102308273315\nepoch 10 --- step 103 --- step size 32 loss 0.5495863556861877\nepoch 10 --- step 104 --- step size 32 loss 0.694143533706665\nepoch 10 --- step 105 --- step size 32 loss 0.632591724395752\nepoch 10 --- step 106 --- step size 32 loss 0.5075010061264038\nepoch 10 --- step 107 --- step size 32 loss 0.5439586639404297\nepoch 10 --- step 108 --- step size 32 loss 0.5400713682174683\nepoch 10 --- step 109 --- step size 32 loss 0.6654360890388489\nepoch 10 --- step 110 --- step size 32 loss 0.5978046655654907\nepoch 10 --- step 111 --- step size 32 loss 0.5254462957382202\nepoch 10 --- step 112 --- step size 32 loss 0.5170295238494873\nepoch 10 --- step 113 --- step size 32 loss 0.6642424464225769\nepoch 10 --- step 114 --- step size 32 loss 0.6051540374755859\nepoch 10 --- step 115 --- step size 32 loss 0.6922459602355957\nepoch 10 --- step 116 --- step size 32 loss 0.8180261850357056\nepoch 10 --- step 117 --- step size 32 loss 0.5004377365112305\nepoch 10 --- step 118 --- step size 32 loss 0.6229459643363953\nepoch 10 --- step 119 --- step size 32 loss 0.9683398604393005\nepoch 10 --- step 120 --- step size 32 loss 0.5654301047325134\nepoch 10 --- step 121 --- step size 32 loss 0.5710858106613159\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 11 --- step 0 --- step size 32 loss 0.6036339998245239\nepoch 11 --- step 1 --- step size 32 loss 0.6178892254829407\nepoch 11 --- step 2 --- step size 32 loss 0.5949679613113403\nepoch 11 --- step 3 --- step size 32 loss 0.6775168180465698\nepoch 11 --- step 4 --- step size 32 loss 0.7103623151779175\nepoch 11 --- step 5 --- step size 32 loss 0.5993355512619019\nepoch 11 --- step 6 --- step size 32 loss 0.6822556257247925\nepoch 11 --- step 7 --- step size 32 loss 0.6403923034667969\nepoch 11 --- step 8 --- step size 32 loss 0.55438232421875\nepoch 11 --- step 9 --- step size 32 loss 0.5267961025238037\nepoch 11 --- step 10 --- step size 32 loss 0.5262545347213745\nepoch 11 --- step 11 --- step size 32 loss 0.6154951453208923\nepoch 11 --- step 12 --- step size 32 loss 0.7278600931167603\nepoch 11 --- step 13 --- step size 32 loss 0.7436599731445312\nepoch 11 --- step 14 --- step size 32 loss 0.3885658383369446\nepoch 11 --- step 15 --- step size 32 loss 0.4282308518886566\nepoch 11 --- step 16 --- step size 32 loss 0.5843292474746704\nepoch 11 --- step 17 --- step size 32 loss 0.7467674612998962\nepoch 11 --- step 18 --- step size 32 loss 0.5039839148521423\nepoch 11 --- step 19 --- step size 32 loss 0.7064902782440186\nepoch 11 --- step 20 --- step size 32 loss 0.6649637818336487\nepoch 11 --- step 21 --- step size 32 loss 0.7013242244720459\nepoch 11 --- step 22 --- step size 32 loss 0.5701800584793091\nepoch 11 --- step 23 --- step size 32 loss 0.7795966863632202\nepoch 11 --- step 24 --- step size 32 loss 0.6515458822250366\nepoch 11 --- step 25 --- step size 32 loss 0.41093772649765015\nepoch 11 --- step 26 --- step size 32 loss 0.6130890250205994\nepoch 11 --- step 27 --- step size 32 loss 0.6497051119804382\nepoch 11 --- step 28 --- step size 32 loss 0.6501438021659851\nepoch 11 --- step 29 --- step size 32 loss 0.7254883050918579\nepoch 11 --- step 30 --- step size 32 loss 0.66350919008255\nepoch 11 --- step 31 --- step size 32 loss 0.7089681029319763\nepoch 11 --- step 32 --- step size 32 loss 0.6945599317550659\nepoch 11 --- step 33 --- step size 32 loss 0.6466156840324402\nepoch 11 --- step 34 --- step size 32 loss 0.6046486496925354\nepoch 11 --- step 35 --- step size 32 loss 0.6755615472793579\nepoch 11 --- step 36 --- step size 32 loss 0.7083136439323425\nepoch 11 --- step 37 --- step size 32 loss 0.6353208422660828\nepoch 11 --- step 38 --- step size 32 loss 0.5590155124664307\nepoch 11 --- step 39 --- step size 32 loss 0.6435529589653015\nepoch 11 --- step 40 --- step size 32 loss 0.7317088842391968\nepoch 11 --- step 41 --- step size 32 loss 0.8551197052001953\nepoch 11 --- step 42 --- step size 32 loss 0.5722621083259583\nepoch 11 --- step 43 --- step size 32 loss 0.6710875034332275\nepoch 11 --- step 44 --- step size 32 loss 0.6219692230224609\nepoch 11 --- step 45 --- step size 32 loss 0.45220810174942017\nepoch 11 --- step 46 --- step size 32 loss 0.6689660549163818\nepoch 11 --- step 47 --- step size 32 loss 0.386466383934021\nepoch 11 --- step 48 --- step size 32 loss 0.5968837738037109\nepoch 11 --- step 49 --- step size 32 loss 0.5332281589508057\nepoch 11 --- step 50 --- step size 32 loss 0.5291919708251953\nepoch 11 --- step 51 --- step size 32 loss 0.7153065204620361\nepoch 11 --- step 52 --- step size 32 loss 0.5848729014396667\nepoch 11 --- step 53 --- step size 32 loss 0.5750219821929932\nepoch 11 --- step 54 --- step size 32 loss 0.45573967695236206\nepoch 11 --- step 55 --- step size 32 loss 0.5333365797996521\nepoch 11 --- step 56 --- step size 32 loss 0.907191276550293\nepoch 11 --- step 57 --- step size 32 loss 0.489282488822937\nepoch 11 --- step 58 --- step size 32 loss 0.7149072885513306\nepoch 11 --- step 59 --- step size 32 loss 0.46671056747436523\nepoch 11 --- step 60 --- step size 32 loss 0.48185548186302185\nepoch 11 --- step 61 --- step size 32 loss 0.5124077796936035\nepoch 11 --- step 62 --- step size 32 loss 0.713106632232666\nepoch 11 --- step 63 --- step size 32 loss 0.623631477355957\nepoch 11 --- step 64 --- step size 32 loss 0.9030322432518005\nepoch 11 --- step 65 --- step size 32 loss 0.709467351436615\nepoch 11 --- step 66 --- step size 32 loss 0.5699572563171387\nepoch 11 --- step 67 --- step size 32 loss 0.6833722591400146\nepoch 11 --- step 68 --- step size 32 loss 0.4949589967727661\nepoch 11 --- step 69 --- step size 32 loss 0.4162690341472626\nepoch 11 --- step 70 --- step size 32 loss 0.3480205535888672\nepoch 11 --- step 71 --- step size 32 loss 0.7602207064628601\nepoch 11 --- step 72 --- step size 32 loss 0.6143918037414551\nepoch 11 --- step 73 --- step size 32 loss 0.51693195104599\nepoch 11 --- step 74 --- step size 32 loss 0.45732995867729187\nepoch 11 --- step 75 --- step size 32 loss 0.6230518221855164\nepoch 11 --- step 76 --- step size 32 loss 0.45628854632377625\nepoch 11 --- step 77 --- step size 32 loss 0.586829423904419\nepoch 11 --- step 78 --- step size 32 loss 0.6601309180259705\nepoch 11 --- step 79 --- step size 32 loss 0.5966396331787109\nepoch 11 --- step 80 --- step size 32 loss 0.545862078666687\nepoch 11 --- step 81 --- step size 32 loss 0.5256363153457642\nepoch 11 --- step 82 --- step size 32 loss 0.5770944952964783\nepoch 11 --- step 83 --- step size 32 loss 0.6937915086746216\nepoch 11 --- step 84 --- step size 32 loss 0.7758049964904785\nepoch 11 --- step 85 --- step size 32 loss 0.5026208758354187\nepoch 11 --- step 86 --- step size 32 loss 0.5129518508911133\nepoch 11 --- step 87 --- step size 32 loss 0.672696590423584\nepoch 11 --- step 88 --- step size 32 loss 0.3217817544937134\nepoch 11 --- step 89 --- step size 32 loss 0.6904318332672119\nepoch 11 --- step 90 --- step size 32 loss 0.7084092497825623\nepoch 11 --- step 91 --- step size 32 loss 0.3972591757774353\nepoch 11 --- step 92 --- step size 32 loss 0.6359730958938599\nepoch 11 --- step 93 --- step size 32 loss 0.5077328681945801\nepoch 11 --- step 94 --- step size 32 loss 0.6362782716751099\nepoch 11 --- step 95 --- step size 32 loss 0.6627326607704163\nepoch 11 --- step 96 --- step size 32 loss 0.4391769766807556\nepoch 11 --- step 97 --- step size 32 loss 0.5238667726516724\nepoch 11 --- step 98 --- step size 32 loss 0.705272376537323\nepoch 11 --- step 99 --- step size 32 loss 0.6000970602035522\nepoch 11 --- step 100 --- step size 32 loss 0.5439153909683228\nepoch 11 --- step 101 --- step size 32 loss 0.5857713222503662\nepoch 11 --- step 102 --- step size 32 loss 0.6019165515899658\nepoch 11 --- step 103 --- step size 32 loss 0.5756403207778931\nepoch 11 --- step 104 --- step size 32 loss 0.7189891338348389\nepoch 11 --- step 105 --- step size 32 loss 0.7419379949569702\nepoch 11 --- step 106 --- step size 32 loss 0.5856201648712158\nepoch 11 --- step 107 --- step size 32 loss 0.6281181573867798\nepoch 11 --- step 108 --- step size 32 loss 0.6057648062705994\nepoch 11 --- step 109 --- step size 32 loss 0.5153563022613525\nepoch 11 --- step 110 --- step size 32 loss 0.5419389605522156\nepoch 11 --- step 111 --- step size 32 loss 0.5999603271484375\nepoch 11 --- step 112 --- step size 32 loss 0.5389913320541382\nepoch 11 --- step 113 --- step size 32 loss 0.5112491846084595\nepoch 11 --- step 114 --- step size 32 loss 0.5240408182144165\nepoch 11 --- step 115 --- step size 32 loss 0.46527203917503357\nepoch 11 --- step 116 --- step size 32 loss 0.7064098119735718\nepoch 11 --- step 117 --- step size 32 loss 0.7823287844657898\nepoch 11 --- step 118 --- step size 32 loss 0.6560587286949158\nepoch 11 --- step 119 --- step size 32 loss 0.653313159942627\nepoch 11 --- step 120 --- step size 32 loss 0.6064649820327759\nepoch 11 --- step 121 --- step size 32 loss 0.509627103805542\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 12 --- step 0 --- step size 32 loss 0.5220321416854858\nepoch 12 --- step 1 --- step size 32 loss 0.6830662488937378\nepoch 12 --- step 2 --- step size 32 loss 0.5366396903991699\nepoch 12 --- step 3 --- step size 32 loss 0.7276802062988281\nepoch 12 --- step 4 --- step size 32 loss 0.45973441004753113\nepoch 12 --- step 5 --- step size 32 loss 0.786472499370575\nepoch 12 --- step 6 --- step size 32 loss 0.6102802753448486\nepoch 12 --- step 7 --- step size 32 loss 0.5491071939468384\nepoch 12 --- step 8 --- step size 32 loss 0.6715575456619263\nepoch 12 --- step 9 --- step size 32 loss 0.6796800494194031\nepoch 12 --- step 10 --- step size 32 loss 0.4728120267391205\nepoch 12 --- step 11 --- step size 32 loss 0.531541109085083\nepoch 12 --- step 12 --- step size 32 loss 0.5566167831420898\nepoch 12 --- step 13 --- step size 32 loss 0.5695840716362\nepoch 12 --- step 14 --- step size 32 loss 0.7373594045639038\nepoch 12 --- step 15 --- step size 32 loss 0.5826961994171143\nepoch 12 --- step 16 --- step size 32 loss 0.5944101214408875\nepoch 12 --- step 17 --- step size 32 loss 0.5141071081161499\nepoch 12 --- step 18 --- step size 32 loss 0.5846363306045532\nepoch 12 --- step 19 --- step size 32 loss 0.572428286075592\nepoch 12 --- step 20 --- step size 32 loss 0.5911391973495483\nepoch 12 --- step 21 --- step size 32 loss 0.5601376295089722\nepoch 12 --- step 22 --- step size 32 loss 0.549336850643158\nepoch 12 --- step 23 --- step size 32 loss 0.6881709098815918\nepoch 12 --- step 24 --- step size 32 loss 0.5775551795959473\nepoch 12 --- step 25 --- step size 32 loss 0.5119509696960449\nepoch 12 --- step 26 --- step size 32 loss 0.4633473753929138\nepoch 12 --- step 27 --- step size 32 loss 0.48391374945640564\nepoch 12 --- step 28 --- step size 32 loss 0.45387452840805054\nepoch 12 --- step 29 --- step size 32 loss 0.7013969421386719\nepoch 12 --- step 30 --- step size 32 loss 0.7656269073486328\nepoch 12 --- step 31 --- step size 32 loss 0.6729564070701599\nepoch 12 --- step 32 --- step size 32 loss 0.5701724886894226\nepoch 12 --- step 33 --- step size 32 loss 0.6865751147270203\nepoch 12 --- step 34 --- step size 32 loss 0.506118893623352\nepoch 12 --- step 35 --- step size 32 loss 0.680578887462616\nepoch 12 --- step 36 --- step size 32 loss 0.5583719611167908\nepoch 12 --- step 37 --- step size 32 loss 0.6835195422172546\nepoch 12 --- step 38 --- step size 32 loss 0.5325571298599243\nepoch 12 --- step 39 --- step size 32 loss 0.5443685054779053\nepoch 12 --- step 40 --- step size 32 loss 0.47173017263412476\nepoch 12 --- step 41 --- step size 32 loss 0.5808044672012329\nepoch 12 --- step 42 --- step size 32 loss 0.5606634616851807\nepoch 12 --- step 43 --- step size 32 loss 0.4848615527153015\nepoch 12 --- step 44 --- step size 32 loss 0.6580339670181274\nepoch 12 --- step 45 --- step size 32 loss 0.5663459300994873\nepoch 12 --- step 46 --- step size 32 loss 0.49308130145072937\nepoch 12 --- step 47 --- step size 32 loss 0.6434757113456726\nepoch 12 --- step 48 --- step size 32 loss 0.5323902368545532\nepoch 12 --- step 49 --- step size 32 loss 0.5513075590133667\nepoch 12 --- step 50 --- step size 32 loss 0.45671069622039795\nepoch 12 --- step 51 --- step size 32 loss 0.5993390083312988\nepoch 12 --- step 52 --- step size 32 loss 0.6145415306091309\nepoch 12 --- step 53 --- step size 32 loss 0.4782530665397644\nepoch 12 --- step 54 --- step size 32 loss 0.48633453249931335\nepoch 12 --- step 55 --- step size 32 loss 0.44769877195358276\nepoch 12 --- step 56 --- step size 32 loss 0.5839211344718933\nepoch 12 --- step 57 --- step size 32 loss 0.6184115409851074\nepoch 12 --- step 58 --- step size 32 loss 0.6429244875907898\nepoch 12 --- step 59 --- step size 32 loss 0.477541983127594\nepoch 12 --- step 60 --- step size 32 loss 0.564170777797699\nepoch 12 --- step 61 --- step size 32 loss 0.5675734281539917\nepoch 12 --- step 62 --- step size 32 loss 0.4336230158805847\nepoch 12 --- step 63 --- step size 32 loss 0.37903302907943726\nepoch 12 --- step 64 --- step size 32 loss 0.30346864461898804\nepoch 12 --- step 65 --- step size 32 loss 0.5927647352218628\nepoch 12 --- step 66 --- step size 32 loss 0.623406708240509\nepoch 12 --- step 67 --- step size 32 loss 0.5104072093963623\nepoch 12 --- step 68 --- step size 32 loss 0.4439605176448822\nepoch 12 --- step 69 --- step size 32 loss 0.6975070834159851\nepoch 12 --- step 70 --- step size 32 loss 0.6120659708976746\nepoch 12 --- step 71 --- step size 32 loss 0.488539457321167\nepoch 12 --- step 72 --- step size 32 loss 0.8527004718780518\nepoch 12 --- step 73 --- step size 32 loss 0.718484103679657\nepoch 12 --- step 74 --- step size 32 loss 0.6747629046440125\nepoch 12 --- step 75 --- step size 32 loss 0.4572024345397949\nepoch 12 --- step 76 --- step size 32 loss 0.6566855311393738\nepoch 12 --- step 77 --- step size 32 loss 0.4354768991470337\nepoch 12 --- step 78 --- step size 32 loss 0.4452614188194275\nepoch 12 --- step 79 --- step size 32 loss 0.6166424751281738\nepoch 12 --- step 80 --- step size 32 loss 0.8122746348381042\nepoch 12 --- step 81 --- step size 32 loss 0.5214416980743408\nepoch 12 --- step 82 --- step size 32 loss 0.5575892329216003\nepoch 12 --- step 83 --- step size 32 loss 0.4975883960723877\nepoch 12 --- step 84 --- step size 32 loss 0.3857516646385193\nepoch 12 --- step 85 --- step size 32 loss 0.6545944213867188\nepoch 12 --- step 86 --- step size 32 loss 0.5839076042175293\nepoch 12 --- step 87 --- step size 32 loss 0.4605045020580292\nepoch 12 --- step 88 --- step size 32 loss 0.4991459846496582\nepoch 12 --- step 89 --- step size 32 loss 0.6780805587768555\nepoch 12 --- step 90 --- step size 32 loss 0.5898653268814087\nepoch 12 --- step 91 --- step size 32 loss 0.4981539249420166\nepoch 12 --- step 92 --- step size 32 loss 0.6338467001914978\nepoch 12 --- step 93 --- step size 32 loss 0.6803880929946899\nepoch 12 --- step 94 --- step size 32 loss 0.5568250417709351\nepoch 12 --- step 95 --- step size 32 loss 0.511755645275116\nepoch 12 --- step 96 --- step size 32 loss 0.5655105710029602\nepoch 12 --- step 97 --- step size 32 loss 0.3860311508178711\nepoch 12 --- step 98 --- step size 32 loss 0.5177354216575623\nepoch 12 --- step 99 --- step size 32 loss 0.6406117677688599\nepoch 12 --- step 100 --- step size 32 loss 0.4666289687156677\nepoch 12 --- step 101 --- step size 32 loss 0.5147194862365723\nepoch 12 --- step 102 --- step size 32 loss 0.5843319892883301\nepoch 12 --- step 103 --- step size 32 loss 0.5956038236618042\nepoch 12 --- step 104 --- step size 32 loss 0.49701446294784546\nepoch 12 --- step 105 --- step size 32 loss 0.5230454802513123\nepoch 12 --- step 106 --- step size 32 loss 0.5176605582237244\nepoch 12 --- step 107 --- step size 32 loss 0.5195300579071045\nepoch 12 --- step 108 --- step size 32 loss 0.48968467116355896\nepoch 12 --- step 109 --- step size 32 loss 0.5795373916625977\nepoch 12 --- step 110 --- step size 32 loss 0.54786217212677\nepoch 12 --- step 111 --- step size 32 loss 0.5373610258102417\nepoch 12 --- step 112 --- step size 32 loss 0.35791993141174316\nepoch 12 --- step 113 --- step size 32 loss 0.5019427537918091\nepoch 12 --- step 114 --- step size 32 loss 0.7193381786346436\nepoch 12 --- step 115 --- step size 32 loss 0.7078137397766113\nepoch 12 --- step 116 --- step size 32 loss 0.5810959339141846\nepoch 12 --- step 117 --- step size 32 loss 0.30800071358680725\nepoch 12 --- step 118 --- step size 32 loss 0.6119288206100464\nepoch 12 --- step 119 --- step size 32 loss 0.5719364285469055\nepoch 12 --- step 120 --- step size 32 loss 0.7575528025627136\nepoch 12 --- step 121 --- step size 32 loss 0.5091598033905029\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 13 --- step 0 --- step size 32 loss 0.6508440971374512\nepoch 13 --- step 1 --- step size 32 loss 0.5336178541183472\nepoch 13 --- step 2 --- step size 32 loss 0.64972984790802\nepoch 13 --- step 3 --- step size 32 loss 0.45799654722213745\nepoch 13 --- step 4 --- step size 32 loss 0.4390489459037781\nepoch 13 --- step 5 --- step size 32 loss 0.5378383994102478\nepoch 13 --- step 6 --- step size 32 loss 0.5054198503494263\nepoch 13 --- step 7 --- step size 32 loss 0.5147808790206909\nepoch 13 --- step 8 --- step size 32 loss 0.5729013085365295\nepoch 13 --- step 9 --- step size 32 loss 0.572144091129303\nepoch 13 --- step 10 --- step size 32 loss 0.392514705657959\nepoch 13 --- step 11 --- step size 32 loss 0.6190312504768372\nepoch 13 --- step 12 --- step size 32 loss 0.756989598274231\nepoch 13 --- step 13 --- step size 32 loss 0.6214737296104431\nepoch 13 --- step 14 --- step size 32 loss 0.5788519382476807\nepoch 13 --- step 15 --- step size 32 loss 0.5516217947006226\nepoch 13 --- step 16 --- step size 32 loss 0.47329992055892944\nepoch 13 --- step 17 --- step size 32 loss 0.5201044082641602\nepoch 13 --- step 18 --- step size 32 loss 0.5405486822128296\nepoch 13 --- step 19 --- step size 32 loss 0.5854513645172119\nepoch 13 --- step 20 --- step size 32 loss 0.6329411268234253\nepoch 13 --- step 21 --- step size 32 loss 0.5418495535850525\nepoch 13 --- step 22 --- step size 32 loss 0.6863660216331482\nepoch 13 --- step 23 --- step size 32 loss 0.4376401901245117\nepoch 13 --- step 24 --- step size 32 loss 0.5375595092773438\nepoch 13 --- step 25 --- step size 32 loss 0.42558178305625916\nepoch 13 --- step 26 --- step size 32 loss 0.5230063199996948\nepoch 13 --- step 27 --- step size 32 loss 0.5845851302146912\nepoch 13 --- step 28 --- step size 32 loss 0.6088922023773193\nepoch 13 --- step 29 --- step size 32 loss 0.6830316781997681\nepoch 13 --- step 30 --- step size 32 loss 0.6267468333244324\nepoch 13 --- step 31 --- step size 32 loss 0.5458868741989136\nepoch 13 --- step 32 --- step size 32 loss 0.7772977352142334\nepoch 13 --- step 33 --- step size 32 loss 0.6109485030174255\nepoch 13 --- step 34 --- step size 32 loss 0.4453910291194916\nepoch 13 --- step 35 --- step size 32 loss 0.462363064289093\nepoch 13 --- step 36 --- step size 32 loss 0.5950846672058105\nepoch 13 --- step 37 --- step size 32 loss 0.6045130491256714\nepoch 13 --- step 38 --- step size 32 loss 0.49526095390319824\nepoch 13 --- step 39 --- step size 32 loss 0.6199266314506531\nepoch 13 --- step 40 --- step size 32 loss 0.4486556649208069\nepoch 13 --- step 41 --- step size 32 loss 0.6492143869400024\nepoch 13 --- step 42 --- step size 32 loss 0.6654123067855835\nepoch 13 --- step 43 --- step size 32 loss 0.4419703185558319\nepoch 13 --- step 44 --- step size 32 loss 0.37567704916000366\nepoch 13 --- step 45 --- step size 32 loss 0.4038521945476532\nepoch 13 --- step 46 --- step size 32 loss 0.6057842969894409\nepoch 13 --- step 47 --- step size 32 loss 0.49166959524154663\nepoch 13 --- step 48 --- step size 32 loss 0.7979451417922974\nepoch 13 --- step 49 --- step size 32 loss 0.42465925216674805\nepoch 13 --- step 50 --- step size 32 loss 0.3902531862258911\nepoch 13 --- step 51 --- step size 32 loss 0.5891236662864685\nepoch 13 --- step 52 --- step size 32 loss 0.7317189574241638\nepoch 13 --- step 53 --- step size 32 loss 0.6781173944473267\nepoch 13 --- step 54 --- step size 32 loss 0.706794261932373\nepoch 13 --- step 55 --- step size 32 loss 0.5377507209777832\nepoch 13 --- step 56 --- step size 32 loss 0.5040208697319031\nepoch 13 --- step 57 --- step size 32 loss 0.47323164343833923\nepoch 13 --- step 58 --- step size 32 loss 0.590126097202301\nepoch 13 --- step 59 --- step size 32 loss 0.4458625316619873\nepoch 13 --- step 60 --- step size 32 loss 0.5464309453964233\nepoch 13 --- step 61 --- step size 32 loss 0.5627226233482361\nepoch 13 --- step 62 --- step size 32 loss 0.46495193243026733\nepoch 13 --- step 63 --- step size 32 loss 0.5027084350585938\nepoch 13 --- step 64 --- step size 32 loss 0.6230556964874268\nepoch 13 --- step 65 --- step size 32 loss 0.566135048866272\nepoch 13 --- step 66 --- step size 32 loss 0.6906048059463501\nepoch 13 --- step 67 --- step size 32 loss 0.49180883169174194\nepoch 13 --- step 68 --- step size 32 loss 0.5926045179367065\nepoch 13 --- step 69 --- step size 32 loss 0.5899750590324402\nepoch 13 --- step 70 --- step size 32 loss 0.6200587153434753\nepoch 13 --- step 71 --- step size 32 loss 0.5360386371612549\nepoch 13 --- step 72 --- step size 32 loss 0.6083215475082397\nepoch 13 --- step 73 --- step size 32 loss 0.5696505308151245\nepoch 13 --- step 74 --- step size 32 loss 0.6925370097160339\nepoch 13 --- step 75 --- step size 32 loss 0.570504367351532\nepoch 13 --- step 76 --- step size 32 loss 0.6839138865470886\nepoch 13 --- step 77 --- step size 32 loss 0.652333676815033\nepoch 13 --- step 78 --- step size 32 loss 0.4094688296318054\nepoch 13 --- step 79 --- step size 32 loss 0.49810275435447693\nepoch 13 --- step 80 --- step size 32 loss 0.7354693412780762\nepoch 13 --- step 81 --- step size 32 loss 0.4628814458847046\nepoch 13 --- step 82 --- step size 32 loss 0.39842185378074646\nepoch 13 --- step 83 --- step size 32 loss 0.6981334686279297\nepoch 13 --- step 84 --- step size 32 loss 0.5761675834655762\nepoch 13 --- step 85 --- step size 32 loss 0.4399643838405609\nepoch 13 --- step 86 --- step size 32 loss 0.42614123225212097\nepoch 13 --- step 87 --- step size 32 loss 0.5964018702507019\nepoch 13 --- step 88 --- step size 32 loss 0.423521488904953\nepoch 13 --- step 89 --- step size 32 loss 0.5625568628311157\nepoch 13 --- step 90 --- step size 32 loss 0.6136062145233154\nepoch 13 --- step 91 --- step size 32 loss 0.5161484479904175\nepoch 13 --- step 92 --- step size 32 loss 0.706192135810852\nepoch 13 --- step 93 --- step size 32 loss 0.46261221170425415\nepoch 13 --- step 94 --- step size 32 loss 0.5980638265609741\nepoch 13 --- step 95 --- step size 32 loss 0.47907543182373047\nepoch 13 --- step 96 --- step size 32 loss 0.4740827679634094\nepoch 13 --- step 97 --- step size 32 loss 0.5555545091629028\nepoch 13 --- step 98 --- step size 32 loss 0.42208895087242126\nepoch 13 --- step 99 --- step size 32 loss 0.4606506824493408\nepoch 13 --- step 100 --- step size 32 loss 0.3671104311943054\nepoch 13 --- step 101 --- step size 32 loss 0.3511929512023926\nepoch 13 --- step 102 --- step size 32 loss 0.3173825144767761\nepoch 13 --- step 103 --- step size 32 loss 0.6529137492179871\nepoch 13 --- step 104 --- step size 32 loss 0.3826213777065277\nepoch 13 --- step 105 --- step size 32 loss 0.3604400157928467\nepoch 13 --- step 106 --- step size 32 loss 0.6072854995727539\nepoch 13 --- step 107 --- step size 32 loss 0.49548178911209106\nepoch 13 --- step 108 --- step size 32 loss 0.4387088716030121\nepoch 13 --- step 109 --- step size 32 loss 0.4799308776855469\nepoch 13 --- step 110 --- step size 32 loss 0.37105584144592285\nepoch 13 --- step 111 --- step size 32 loss 0.5824269652366638\nepoch 13 --- step 112 --- step size 32 loss 0.7136418223381042\nepoch 13 --- step 113 --- step size 32 loss 0.5082045793533325\nepoch 13 --- step 114 --- step size 32 loss 0.5447808504104614\nepoch 13 --- step 115 --- step size 32 loss 0.38957685232162476\nepoch 13 --- step 116 --- step size 32 loss 0.4923066794872284\nepoch 13 --- step 117 --- step size 32 loss 0.5231527090072632\nepoch 13 --- step 118 --- step size 32 loss 0.4450395703315735\nepoch 13 --- step 119 --- step size 32 loss 0.6195182800292969\nepoch 13 --- step 120 --- step size 32 loss 0.535918116569519\nepoch 13 --- step 121 --- step size 32 loss 0.681128978729248\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 14 --- step 0 --- step size 32 loss 0.6093695163726807\nepoch 14 --- step 1 --- step size 32 loss 0.595442533493042\nepoch 14 --- step 2 --- step size 32 loss 0.4080328941345215\nepoch 14 --- step 3 --- step size 32 loss 0.5854384303092957\nepoch 14 --- step 4 --- step size 32 loss 0.5035566091537476\nepoch 14 --- step 5 --- step size 32 loss 0.4625512957572937\nepoch 14 --- step 6 --- step size 32 loss 0.5905678272247314\nepoch 14 --- step 7 --- step size 32 loss 0.6599698066711426\nepoch 14 --- step 8 --- step size 32 loss 0.5707839727401733\nepoch 14 --- step 9 --- step size 32 loss 0.405874103307724\nepoch 14 --- step 10 --- step size 32 loss 0.6535835266113281\nepoch 14 --- step 11 --- step size 32 loss 0.5114632844924927\nepoch 14 --- step 12 --- step size 32 loss 0.6329573392868042\nepoch 14 --- step 13 --- step size 32 loss 0.5938897132873535\nepoch 14 --- step 14 --- step size 32 loss 0.5535603165626526\nepoch 14 --- step 15 --- step size 32 loss 0.4901411533355713\nepoch 14 --- step 16 --- step size 32 loss 0.5257160067558289\nepoch 14 --- step 17 --- step size 32 loss 0.48215195536613464\nepoch 14 --- step 18 --- step size 32 loss 0.5509465932846069\nepoch 14 --- step 19 --- step size 32 loss 0.512347936630249\nepoch 14 --- step 20 --- step size 32 loss 0.41729360818862915\nepoch 14 --- step 21 --- step size 32 loss 0.5179758071899414\nepoch 14 --- step 22 --- step size 32 loss 0.705049991607666\nepoch 14 --- step 23 --- step size 32 loss 0.5384280681610107\nepoch 14 --- step 24 --- step size 32 loss 0.5439512729644775\nepoch 14 --- step 25 --- step size 32 loss 0.5337976813316345\nepoch 14 --- step 26 --- step size 32 loss 0.6085432767868042\nepoch 14 --- step 27 --- step size 32 loss 0.4934433400630951\nepoch 14 --- step 28 --- step size 32 loss 0.596739649772644\nepoch 14 --- step 29 --- step size 32 loss 0.6217714548110962\nepoch 14 --- step 30 --- step size 32 loss 0.4965211749076843\nepoch 14 --- step 31 --- step size 32 loss 0.5909988880157471\nepoch 14 --- step 32 --- step size 32 loss 0.5636793375015259\nepoch 14 --- step 33 --- step size 32 loss 0.4136143624782562\nepoch 14 --- step 34 --- step size 32 loss 0.4713324010372162\nepoch 14 --- step 35 --- step size 32 loss 0.46034151315689087\nepoch 14 --- step 36 --- step size 32 loss 0.47238773107528687\nepoch 14 --- step 37 --- step size 32 loss 0.522579550743103\nepoch 14 --- step 38 --- step size 32 loss 0.44172412157058716\nepoch 14 --- step 39 --- step size 32 loss 0.40005362033843994\nepoch 14 --- step 40 --- step size 32 loss 0.6134153604507446\nepoch 14 --- step 41 --- step size 32 loss 0.5883796215057373\nepoch 14 --- step 42 --- step size 32 loss 0.3888739049434662\nepoch 14 --- step 43 --- step size 32 loss 0.513808012008667\nepoch 14 --- step 44 --- step size 32 loss 0.5889984965324402\nepoch 14 --- step 45 --- step size 32 loss 0.46387743949890137\nepoch 14 --- step 46 --- step size 32 loss 0.47786378860473633\nepoch 14 --- step 47 --- step size 32 loss 0.4187157154083252\nepoch 14 --- step 48 --- step size 32 loss 0.5725347399711609\nepoch 14 --- step 49 --- step size 32 loss 0.5741260051727295\nepoch 14 --- step 50 --- step size 32 loss 0.48700404167175293\nepoch 14 --- step 51 --- step size 32 loss 0.4939073324203491\nepoch 14 --- step 52 --- step size 32 loss 0.5860496759414673\nepoch 14 --- step 53 --- step size 32 loss 0.531985878944397\nepoch 14 --- step 54 --- step size 32 loss 0.5529863238334656\nepoch 14 --- step 55 --- step size 32 loss 0.5401597023010254\nepoch 14 --- step 56 --- step size 32 loss 0.5251110792160034\nepoch 14 --- step 57 --- step size 32 loss 0.41640233993530273\nepoch 14 --- step 58 --- step size 32 loss 0.529116690158844\nepoch 14 --- step 59 --- step size 32 loss 0.42041200399398804\nepoch 14 --- step 60 --- step size 32 loss 0.40814322233200073\nepoch 14 --- step 61 --- step size 32 loss 0.48989710211753845\nepoch 14 --- step 62 --- step size 32 loss 0.5132033228874207\nepoch 14 --- step 63 --- step size 32 loss 0.5387553572654724\nepoch 14 --- step 64 --- step size 32 loss 0.5348713397979736\nepoch 14 --- step 65 --- step size 32 loss 0.523017168045044\nepoch 14 --- step 66 --- step size 32 loss 0.6324217319488525\nepoch 14 --- step 67 --- step size 32 loss 0.4981420934200287\nepoch 14 --- step 68 --- step size 32 loss 0.5870893597602844\nepoch 14 --- step 69 --- step size 32 loss 0.45250415802001953\nepoch 14 --- step 70 --- step size 32 loss 0.4073666036128998\nepoch 14 --- step 71 --- step size 32 loss 0.3843901753425598\nepoch 14 --- step 72 --- step size 32 loss 0.43989166617393494\nepoch 14 --- step 73 --- step size 32 loss 0.3851328492164612\nepoch 14 --- step 74 --- step size 32 loss 0.5568628311157227\nepoch 14 --- step 75 --- step size 32 loss 0.5718538165092468\nepoch 14 --- step 76 --- step size 32 loss 0.4696583151817322\nepoch 14 --- step 77 --- step size 32 loss 0.6728388667106628\nepoch 14 --- step 78 --- step size 32 loss 0.5705419778823853\nepoch 14 --- step 79 --- step size 32 loss 0.48611196875572205\nepoch 14 --- step 80 --- step size 32 loss 0.30230456590652466\nepoch 14 --- step 81 --- step size 32 loss 0.4616217613220215\nepoch 14 --- step 82 --- step size 32 loss 0.4373700022697449\nepoch 14 --- step 83 --- step size 32 loss 0.5676872730255127\nepoch 14 --- step 84 --- step size 32 loss 0.3831465244293213\nepoch 14 --- step 85 --- step size 32 loss 0.5235780477523804\nepoch 14 --- step 86 --- step size 32 loss 0.3517744541168213\nepoch 14 --- step 87 --- step size 32 loss 0.4372940957546234\nepoch 14 --- step 88 --- step size 32 loss 0.46491384506225586\nepoch 14 --- step 89 --- step size 32 loss 0.4280984401702881\nepoch 14 --- step 90 --- step size 32 loss 0.4254758059978485\nepoch 14 --- step 91 --- step size 32 loss 0.5026466846466064\nepoch 14 --- step 92 --- step size 32 loss 0.5794219374656677\nepoch 14 --- step 93 --- step size 32 loss 0.4079000949859619\nepoch 14 --- step 94 --- step size 32 loss 0.4617866277694702\nepoch 14 --- step 95 --- step size 32 loss 0.4645387530326843\nepoch 14 --- step 96 --- step size 32 loss 0.4969627261161804\nepoch 14 --- step 97 --- step size 32 loss 0.602607011795044\nepoch 14 --- step 98 --- step size 32 loss 0.5089858770370483\nepoch 14 --- step 99 --- step size 32 loss 0.5469615459442139\nepoch 14 --- step 100 --- step size 32 loss 0.4963993728160858\nepoch 14 --- step 101 --- step size 32 loss 0.4409794807434082\nepoch 14 --- step 102 --- step size 32 loss 0.5762958526611328\nepoch 14 --- step 103 --- step size 32 loss 0.5240865349769592\nepoch 14 --- step 104 --- step size 32 loss 0.4469449520111084\nepoch 14 --- step 105 --- step size 32 loss 0.38394150137901306\nepoch 14 --- step 106 --- step size 32 loss 0.5400122404098511\nepoch 14 --- step 107 --- step size 32 loss 0.3683238625526428\nepoch 14 --- step 108 --- step size 32 loss 0.44017452001571655\nepoch 14 --- step 109 --- step size 32 loss 0.5061834454536438\nepoch 14 --- step 110 --- step size 32 loss 0.4611388146877289\nepoch 14 --- step 111 --- step size 32 loss 0.5691308379173279\nepoch 14 --- step 112 --- step size 32 loss 0.5700380206108093\nepoch 14 --- step 113 --- step size 32 loss 0.45483964681625366\nepoch 14 --- step 114 --- step size 32 loss 0.5007688999176025\nepoch 14 --- step 115 --- step size 32 loss 0.44850534200668335\nepoch 14 --- step 116 --- step size 32 loss 0.5143897533416748\nepoch 14 --- step 117 --- step size 32 loss 0.33544808626174927\nepoch 14 --- step 118 --- step size 32 loss 0.44498786330223083\nepoch 14 --- step 119 --- step size 32 loss 0.47649988532066345\nepoch 14 --- step 120 --- step size 32 loss 0.649156928062439\nepoch 14 --- step 121 --- step size 32 loss 0.6298435926437378\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 15 --- step 0 --- step size 32 loss 0.54755038022995\nepoch 15 --- step 1 --- step size 32 loss 0.46295613050460815\nepoch 15 --- step 2 --- step size 32 loss 0.6235133409500122\nepoch 15 --- step 3 --- step size 32 loss 0.5242788791656494\nepoch 15 --- step 4 --- step size 32 loss 0.48005351424217224\nepoch 15 --- step 5 --- step size 32 loss 0.4033750295639038\nepoch 15 --- step 6 --- step size 32 loss 0.4326961934566498\nepoch 15 --- step 7 --- step size 32 loss 0.40298086404800415\nepoch 15 --- step 8 --- step size 32 loss 0.6769436597824097\nepoch 15 --- step 9 --- step size 32 loss 0.4667550027370453\nepoch 15 --- step 10 --- step size 32 loss 0.3437741696834564\nepoch 15 --- step 11 --- step size 32 loss 0.38571876287460327\nepoch 15 --- step 12 --- step size 32 loss 0.42033421993255615\nepoch 15 --- step 13 --- step size 32 loss 0.3394782543182373\nepoch 15 --- step 14 --- step size 32 loss 0.5270661115646362\nepoch 15 --- step 15 --- step size 32 loss 0.5571837425231934\nepoch 15 --- step 16 --- step size 32 loss 0.47988566756248474\nepoch 15 --- step 17 --- step size 32 loss 0.46998095512390137\nepoch 15 --- step 18 --- step size 32 loss 0.5059018135070801\nepoch 15 --- step 19 --- step size 32 loss 0.4904704988002777\nepoch 15 --- step 20 --- step size 32 loss 0.4812348484992981\nepoch 15 --- step 21 --- step size 32 loss 0.4979180693626404\nepoch 15 --- step 22 --- step size 32 loss 0.6095161437988281\nepoch 15 --- step 23 --- step size 32 loss 0.5050292611122131\nepoch 15 --- step 24 --- step size 32 loss 0.39122897386550903\nepoch 15 --- step 25 --- step size 32 loss 0.5870742201805115\nepoch 15 --- step 26 --- step size 32 loss 0.34891411662101746\nepoch 15 --- step 27 --- step size 32 loss 0.38583773374557495\nepoch 15 --- step 28 --- step size 32 loss 0.56788170337677\nepoch 15 --- step 29 --- step size 32 loss 0.49748754501342773\nepoch 15 --- step 30 --- step size 32 loss 0.5152570605278015\nepoch 15 --- step 31 --- step size 32 loss 0.3536030650138855\nepoch 15 --- step 32 --- step size 32 loss 0.4678581655025482\nepoch 15 --- step 33 --- step size 32 loss 0.5877576470375061\nepoch 15 --- step 34 --- step size 32 loss 0.5453907251358032\nepoch 15 --- step 35 --- step size 32 loss 0.6082447171211243\nepoch 15 --- step 36 --- step size 32 loss 0.4909348487854004\nepoch 15 --- step 37 --- step size 32 loss 0.4611627459526062\nepoch 15 --- step 38 --- step size 32 loss 0.48163169622421265\nepoch 15 --- step 39 --- step size 32 loss 0.47693634033203125\nepoch 15 --- step 40 --- step size 32 loss 0.32407402992248535\nepoch 15 --- step 41 --- step size 32 loss 0.48942846059799194\nepoch 15 --- step 42 --- step size 32 loss 0.5382223129272461\nepoch 15 --- step 43 --- step size 32 loss 0.459471195936203\nepoch 15 --- step 44 --- step size 32 loss 0.39584240317344666\nepoch 15 --- step 45 --- step size 32 loss 0.43809565901756287\nepoch 15 --- step 46 --- step size 32 loss 0.463422954082489\nepoch 15 --- step 47 --- step size 32 loss 0.4633063077926636\nepoch 15 --- step 48 --- step size 32 loss 0.539553165435791\nepoch 15 --- step 49 --- step size 32 loss 0.44697141647338867\nepoch 15 --- step 50 --- step size 32 loss 0.3619464635848999\nepoch 15 --- step 51 --- step size 32 loss 0.3703504204750061\nepoch 15 --- step 52 --- step size 32 loss 0.5022274851799011\nepoch 15 --- step 53 --- step size 32 loss 0.5641122460365295\nepoch 15 --- step 54 --- step size 32 loss 0.5483184456825256\nepoch 15 --- step 55 --- step size 32 loss 0.4607616364955902\nepoch 15 --- step 56 --- step size 32 loss 0.48287150263786316\nepoch 15 --- step 57 --- step size 32 loss 0.31011927127838135\nepoch 15 --- step 58 --- step size 32 loss 0.33591771125793457\nepoch 15 --- step 59 --- step size 32 loss 0.42444485425949097\nepoch 15 --- step 60 --- step size 32 loss 0.4372900426387787\nepoch 15 --- step 61 --- step size 32 loss 0.4693238139152527\nepoch 15 --- step 62 --- step size 32 loss 0.5265001654624939\nepoch 15 --- step 63 --- step size 32 loss 0.723212718963623\nepoch 15 --- step 64 --- step size 32 loss 0.5559802055358887\nepoch 15 --- step 65 --- step size 32 loss 0.49209678173065186\nepoch 15 --- step 66 --- step size 32 loss 0.49535471200942993\nepoch 15 --- step 67 --- step size 32 loss 0.6119229793548584\nepoch 15 --- step 68 --- step size 32 loss 0.3911775052547455\nepoch 15 --- step 69 --- step size 32 loss 0.4598176181316376\nepoch 15 --- step 70 --- step size 32 loss 0.39587366580963135\nepoch 15 --- step 71 --- step size 32 loss 0.4166184365749359\nepoch 15 --- step 72 --- step size 32 loss 0.4372088313102722\nepoch 15 --- step 73 --- step size 32 loss 0.464245080947876\nepoch 15 --- step 74 --- step size 32 loss 0.3793880343437195\nepoch 15 --- step 75 --- step size 32 loss 0.3350614905357361\nepoch 15 --- step 76 --- step size 32 loss 0.40183013677597046\nepoch 15 --- step 77 --- step size 32 loss 0.6911147832870483\nepoch 15 --- step 78 --- step size 32 loss 0.44157499074935913\nepoch 15 --- step 79 --- step size 32 loss 0.3649616837501526\nepoch 15 --- step 80 --- step size 32 loss 0.6413453817367554\nepoch 15 --- step 81 --- step size 32 loss 0.3532257080078125\nepoch 15 --- step 82 --- step size 32 loss 0.4329680800437927\nepoch 15 --- step 83 --- step size 32 loss 0.41521310806274414\nepoch 15 --- step 84 --- step size 32 loss 0.5015878081321716\nepoch 15 --- step 85 --- step size 32 loss 0.578669011592865\nepoch 15 --- step 86 --- step size 32 loss 0.5097582340240479\nepoch 15 --- step 87 --- step size 32 loss 0.5317935347557068\nepoch 15 --- step 88 --- step size 32 loss 0.3577239513397217\nepoch 15 --- step 89 --- step size 32 loss 0.37446993589401245\nepoch 15 --- step 90 --- step size 32 loss 0.4485931396484375\nepoch 15 --- step 91 --- step size 32 loss 0.4012375771999359\nepoch 15 --- step 92 --- step size 32 loss 0.5235222578048706\nepoch 15 --- step 93 --- step size 32 loss 0.3908006548881531\nepoch 15 --- step 94 --- step size 32 loss 0.44245919585227966\nepoch 15 --- step 95 --- step size 32 loss 0.4368382692337036\nepoch 15 --- step 96 --- step size 32 loss 0.38950467109680176\nepoch 15 --- step 97 --- step size 32 loss 0.5251485705375671\nepoch 15 --- step 98 --- step size 32 loss 0.41237759590148926\nepoch 15 --- step 99 --- step size 32 loss 0.36124104261398315\nepoch 15 --- step 100 --- step size 32 loss 0.45119810104370117\nepoch 15 --- step 101 --- step size 32 loss 0.5231720209121704\nepoch 15 --- step 102 --- step size 32 loss 0.6965298652648926\nepoch 15 --- step 103 --- step size 32 loss 0.5327490568161011\nepoch 15 --- step 104 --- step size 32 loss 0.42452260851860046\nepoch 15 --- step 105 --- step size 32 loss 0.5477374792098999\nepoch 15 --- step 106 --- step size 32 loss 0.4203187823295593\nepoch 15 --- step 107 --- step size 32 loss 0.4811079204082489\nepoch 15 --- step 108 --- step size 32 loss 0.47830304503440857\nepoch 15 --- step 109 --- step size 32 loss 0.4572688937187195\nepoch 15 --- step 110 --- step size 32 loss 0.36596402525901794\nepoch 15 --- step 111 --- step size 32 loss 0.4539148211479187\nepoch 15 --- step 112 --- step size 32 loss 0.4364708364009857\nepoch 15 --- step 113 --- step size 32 loss 0.4159001111984253\nepoch 15 --- step 114 --- step size 32 loss 0.5403313636779785\nepoch 15 --- step 115 --- step size 32 loss 0.2945985496044159\nepoch 15 --- step 116 --- step size 32 loss 0.4255802035331726\nepoch 15 --- step 117 --- step size 32 loss 0.5034838318824768\nepoch 15 --- step 118 --- step size 32 loss 0.4095458388328552\nepoch 15 --- step 119 --- step size 32 loss 0.4831303060054779\nepoch 15 --- step 120 --- step size 32 loss 0.37608975172042847\nepoch 15 --- step 121 --- step size 32 loss 0.5080479383468628\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 16 --- step 0 --- step size 32 loss 0.40697145462036133\nepoch 16 --- step 1 --- step size 32 loss 0.5074145793914795\nepoch 16 --- step 2 --- step size 32 loss 0.4093131422996521\nepoch 16 --- step 3 --- step size 32 loss 0.3502713143825531\nepoch 16 --- step 4 --- step size 32 loss 0.2593677043914795\nepoch 16 --- step 5 --- step size 32 loss 0.437974750995636\nepoch 16 --- step 6 --- step size 32 loss 0.5780662298202515\nepoch 16 --- step 7 --- step size 32 loss 0.6790172457695007\nepoch 16 --- step 8 --- step size 32 loss 0.4894756078720093\nepoch 16 --- step 9 --- step size 32 loss 0.47651827335357666\nepoch 16 --- step 10 --- step size 32 loss 0.31996968388557434\nepoch 16 --- step 11 --- step size 32 loss 0.5669755935668945\nepoch 16 --- step 12 --- step size 32 loss 0.46930834650993347\nepoch 16 --- step 13 --- step size 32 loss 0.45395809412002563\nepoch 16 --- step 14 --- step size 32 loss 0.5688636302947998\nepoch 16 --- step 15 --- step size 32 loss 0.388799250125885\nepoch 16 --- step 16 --- step size 32 loss 0.6583588123321533\nepoch 16 --- step 17 --- step size 32 loss 0.4029272496700287\nepoch 16 --- step 18 --- step size 32 loss 0.483792781829834\nepoch 16 --- step 19 --- step size 32 loss 0.4653185307979584\nepoch 16 --- step 20 --- step size 32 loss 0.5691893100738525\nepoch 16 --- step 21 --- step size 32 loss 0.3790721297264099\nepoch 16 --- step 22 --- step size 32 loss 0.42089730501174927\nepoch 16 --- step 23 --- step size 32 loss 0.4319835603237152\nepoch 16 --- step 24 --- step size 32 loss 0.2781374156475067\nepoch 16 --- step 25 --- step size 32 loss 0.4636373519897461\nepoch 16 --- step 26 --- step size 32 loss 0.57193922996521\nepoch 16 --- step 27 --- step size 32 loss 0.541441798210144\nepoch 16 --- step 28 --- step size 32 loss 0.3805198669433594\nepoch 16 --- step 29 --- step size 32 loss 0.47685837745666504\nepoch 16 --- step 30 --- step size 32 loss 0.4049071967601776\nepoch 16 --- step 31 --- step size 32 loss 0.4260450005531311\nepoch 16 --- step 32 --- step size 32 loss 0.3794124126434326\nepoch 16 --- step 33 --- step size 32 loss 0.36788779497146606\nepoch 16 --- step 34 --- step size 32 loss 0.4645017385482788\nepoch 16 --- step 35 --- step size 32 loss 0.48754867911338806\nepoch 16 --- step 36 --- step size 32 loss 0.40571337938308716\nepoch 16 --- step 37 --- step size 32 loss 0.3360512852668762\nepoch 16 --- step 38 --- step size 32 loss 0.5366634726524353\nepoch 16 --- step 39 --- step size 32 loss 0.5511009693145752\nepoch 16 --- step 40 --- step size 32 loss 0.5585964322090149\nepoch 16 --- step 41 --- step size 32 loss 0.5031075477600098\nepoch 16 --- step 42 --- step size 32 loss 0.6003856658935547\nepoch 16 --- step 43 --- step size 32 loss 0.3916676640510559\nepoch 16 --- step 44 --- step size 32 loss 0.469801664352417\nepoch 16 --- step 45 --- step size 32 loss 0.4049118459224701\nepoch 16 --- step 46 --- step size 32 loss 0.4044744372367859\nepoch 16 --- step 47 --- step size 32 loss 0.42059922218322754\nepoch 16 --- step 48 --- step size 32 loss 0.5680294036865234\nepoch 16 --- step 49 --- step size 32 loss 0.5922448039054871\nepoch 16 --- step 50 --- step size 32 loss 0.4287463426589966\nepoch 16 --- step 51 --- step size 32 loss 0.4194650650024414\nepoch 16 --- step 52 --- step size 32 loss 0.42557626962661743\nepoch 16 --- step 53 --- step size 32 loss 0.4350559115409851\nepoch 16 --- step 54 --- step size 32 loss 0.4062637686729431\nepoch 16 --- step 55 --- step size 32 loss 0.40871119499206543\nepoch 16 --- step 56 --- step size 32 loss 0.5539469718933105\nepoch 16 --- step 57 --- step size 32 loss 0.4548785090446472\nepoch 16 --- step 58 --- step size 32 loss 0.38068097829818726\nepoch 16 --- step 59 --- step size 32 loss 0.5621641874313354\nepoch 16 --- step 60 --- step size 32 loss 0.4168947637081146\nepoch 16 --- step 61 --- step size 32 loss 0.4536726474761963\nepoch 16 --- step 62 --- step size 32 loss 0.44184011220932007\nepoch 16 --- step 63 --- step size 32 loss 0.5157703757286072\nepoch 16 --- step 64 --- step size 32 loss 0.4059349298477173\nepoch 16 --- step 65 --- step size 32 loss 0.4939597547054291\nepoch 16 --- step 66 --- step size 32 loss 0.4512512683868408\nepoch 16 --- step 67 --- step size 32 loss 0.5157228708267212\nepoch 16 --- step 68 --- step size 32 loss 0.2847236096858978\nepoch 16 --- step 69 --- step size 32 loss 0.38406214118003845\nepoch 16 --- step 70 --- step size 32 loss 0.3822484016418457\nepoch 16 --- step 71 --- step size 32 loss 0.3958922326564789\nepoch 16 --- step 72 --- step size 32 loss 0.5344918370246887\nepoch 16 --- step 73 --- step size 32 loss 0.5230188369750977\nepoch 16 --- step 74 --- step size 32 loss 0.33882781863212585\nepoch 16 --- step 75 --- step size 32 loss 0.5605438947677612\nepoch 16 --- step 76 --- step size 32 loss 0.45354804396629333\nepoch 16 --- step 77 --- step size 32 loss 0.5128046274185181\nepoch 16 --- step 78 --- step size 32 loss 0.4346598982810974\nepoch 16 --- step 79 --- step size 32 loss 0.44528913497924805\nepoch 16 --- step 80 --- step size 32 loss 0.3016413450241089\nepoch 16 --- step 81 --- step size 32 loss 0.4452213644981384\nepoch 16 --- step 82 --- step size 32 loss 0.5262283682823181\nepoch 16 --- step 83 --- step size 32 loss 0.3689408004283905\nepoch 16 --- step 84 --- step size 32 loss 0.41496890783309937\nepoch 16 --- step 85 --- step size 32 loss 0.44472429156303406\nepoch 16 --- step 86 --- step size 32 loss 0.41870951652526855\nepoch 16 --- step 87 --- step size 32 loss 0.5009429454803467\nepoch 16 --- step 88 --- step size 32 loss 0.36125290393829346\nepoch 16 --- step 89 --- step size 32 loss 0.39649462699890137\nepoch 16 --- step 90 --- step size 32 loss 0.3974692225456238\nepoch 16 --- step 91 --- step size 32 loss 0.4253802001476288\nepoch 16 --- step 92 --- step size 32 loss 0.3502216339111328\nepoch 16 --- step 93 --- step size 32 loss 0.4232103228569031\nepoch 16 --- step 94 --- step size 32 loss 0.43876323103904724\nepoch 16 --- step 95 --- step size 32 loss 0.5260186195373535\nepoch 16 --- step 96 --- step size 32 loss 0.4365338683128357\nepoch 16 --- step 97 --- step size 32 loss 0.5706565380096436\nepoch 16 --- step 98 --- step size 32 loss 0.3968404233455658\nepoch 16 --- step 99 --- step size 32 loss 0.3689861297607422\nepoch 16 --- step 100 --- step size 32 loss 0.4214300513267517\nepoch 16 --- step 101 --- step size 32 loss 0.3263821303844452\nepoch 16 --- step 102 --- step size 32 loss 0.4515441358089447\nepoch 16 --- step 103 --- step size 32 loss 0.45674118399620056\nepoch 16 --- step 104 --- step size 32 loss 0.45174700021743774\nepoch 16 --- step 105 --- step size 32 loss 0.3994387090206146\nepoch 16 --- step 106 --- step size 32 loss 0.5059037208557129\nepoch 16 --- step 107 --- step size 32 loss 0.5047891139984131\nepoch 16 --- step 108 --- step size 32 loss 0.35389435291290283\nepoch 16 --- step 109 --- step size 32 loss 0.5074061751365662\nepoch 16 --- step 110 --- step size 32 loss 0.3529542088508606\nepoch 16 --- step 111 --- step size 32 loss 0.517685055732727\nepoch 16 --- step 112 --- step size 32 loss 0.46679744124412537\nepoch 16 --- step 113 --- step size 32 loss 0.4388154447078705\nepoch 16 --- step 114 --- step size 32 loss 0.5061108469963074\nepoch 16 --- step 115 --- step size 32 loss 0.24685820937156677\nepoch 16 --- step 116 --- step size 32 loss 0.4850650727748871\nepoch 16 --- step 117 --- step size 32 loss 0.5065147876739502\nepoch 16 --- step 118 --- step size 32 loss 0.48317885398864746\nepoch 16 --- step 119 --- step size 32 loss 0.5000754594802856\nepoch 16 --- step 120 --- step size 32 loss 0.33822306990623474\nepoch 16 --- step 121 --- step size 32 loss 0.3380674719810486\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 17 --- step 0 --- step size 32 loss 0.40716981887817383\nepoch 17 --- step 1 --- step size 32 loss 0.3894590139389038\nepoch 17 --- step 2 --- step size 32 loss 0.34579038619995117\nepoch 17 --- step 3 --- step size 32 loss 0.5177112221717834\nepoch 17 --- step 4 --- step size 32 loss 0.41602903604507446\nepoch 17 --- step 5 --- step size 32 loss 0.3798186182975769\nepoch 17 --- step 6 --- step size 32 loss 0.4725450873374939\nepoch 17 --- step 7 --- step size 32 loss 0.281680166721344\nepoch 17 --- step 8 --- step size 32 loss 0.4046476483345032\nepoch 17 --- step 9 --- step size 32 loss 0.44176894426345825\nepoch 17 --- step 10 --- step size 32 loss 0.45880016684532166\nepoch 17 --- step 11 --- step size 32 loss 0.4335342347621918\nepoch 17 --- step 12 --- step size 32 loss 0.3637373447418213\nepoch 17 --- step 13 --- step size 32 loss 0.43227753043174744\nepoch 17 --- step 14 --- step size 32 loss 0.42169734835624695\nepoch 17 --- step 15 --- step size 32 loss 0.43780678510665894\nepoch 17 --- step 16 --- step size 32 loss 0.5016487836837769\nepoch 17 --- step 17 --- step size 32 loss 0.38103991746902466\nepoch 17 --- step 18 --- step size 32 loss 0.4754440188407898\nepoch 17 --- step 19 --- step size 32 loss 0.4644123315811157\nepoch 17 --- step 20 --- step size 32 loss 0.42578694224357605\nepoch 17 --- step 21 --- step size 32 loss 0.38177689909935\nepoch 17 --- step 22 --- step size 32 loss 0.38119959831237793\nepoch 17 --- step 23 --- step size 32 loss 0.4212214946746826\nepoch 17 --- step 24 --- step size 32 loss 0.4579540491104126\nepoch 17 --- step 25 --- step size 32 loss 0.611487865447998\nepoch 17 --- step 26 --- step size 32 loss 0.3524363040924072\nepoch 17 --- step 27 --- step size 32 loss 0.44444310665130615\nepoch 17 --- step 28 --- step size 32 loss 0.5486217141151428\nepoch 17 --- step 29 --- step size 32 loss 0.31610745191574097\nepoch 17 --- step 30 --- step size 32 loss 0.5426406860351562\nepoch 17 --- step 31 --- step size 32 loss 0.36235544085502625\nepoch 17 --- step 32 --- step size 32 loss 0.36291950941085815\nepoch 17 --- step 33 --- step size 32 loss 0.35237663984298706\nepoch 17 --- step 34 --- step size 32 loss 0.37251076102256775\nepoch 17 --- step 35 --- step size 32 loss 0.3458544909954071\nepoch 17 --- step 36 --- step size 32 loss 0.5119213461875916\nepoch 17 --- step 37 --- step size 32 loss 0.44374391436576843\nepoch 17 --- step 38 --- step size 32 loss 0.45887506008148193\nepoch 17 --- step 39 --- step size 32 loss 0.5485626459121704\nepoch 17 --- step 40 --- step size 32 loss 0.40939056873321533\nepoch 17 --- step 41 --- step size 32 loss 0.2863823473453522\nepoch 17 --- step 42 --- step size 32 loss 0.44173988699913025\nepoch 17 --- step 43 --- step size 32 loss 0.5126336812973022\nepoch 17 --- step 44 --- step size 32 loss 0.36634692549705505\nepoch 17 --- step 45 --- step size 32 loss 0.36922430992126465\nepoch 17 --- step 46 --- step size 32 loss 0.42783090472221375\nepoch 17 --- step 47 --- step size 32 loss 0.4034935235977173\nepoch 17 --- step 48 --- step size 32 loss 0.4140244126319885\nepoch 17 --- step 49 --- step size 32 loss 0.31612056493759155\nepoch 17 --- step 50 --- step size 32 loss 0.4547867774963379\nepoch 17 --- step 51 --- step size 32 loss 0.4781462550163269\nepoch 17 --- step 52 --- step size 32 loss 0.4696861207485199\nepoch 17 --- step 53 --- step size 32 loss 0.41103872656822205\nepoch 17 --- step 54 --- step size 32 loss 0.5088467001914978\nepoch 17 --- step 55 --- step size 32 loss 0.4096853733062744\nepoch 17 --- step 56 --- step size 32 loss 0.4716600179672241\nepoch 17 --- step 57 --- step size 32 loss 0.4378701150417328\nepoch 17 --- step 58 --- step size 32 loss 0.4567732810974121\nepoch 17 --- step 59 --- step size 32 loss 0.3695133328437805\nepoch 17 --- step 60 --- step size 32 loss 0.5186218023300171\nepoch 17 --- step 61 --- step size 32 loss 0.46305525302886963\nepoch 17 --- step 62 --- step size 32 loss 0.4466855227947235\nepoch 17 --- step 63 --- step size 32 loss 0.23600664734840393\nepoch 17 --- step 64 --- step size 32 loss 0.3708950877189636\nepoch 17 --- step 65 --- step size 32 loss 0.5388584136962891\nepoch 17 --- step 66 --- step size 32 loss 0.3694465160369873\nepoch 17 --- step 67 --- step size 32 loss 0.3497864603996277\nepoch 17 --- step 68 --- step size 32 loss 0.41370075941085815\nepoch 17 --- step 69 --- step size 32 loss 0.4834170937538147\nepoch 17 --- step 70 --- step size 32 loss 0.5028498768806458\nepoch 17 --- step 71 --- step size 32 loss 0.4497990608215332\nepoch 17 --- step 72 --- step size 32 loss 0.4475170969963074\nepoch 17 --- step 73 --- step size 32 loss 0.5315083265304565\nepoch 17 --- step 74 --- step size 32 loss 0.41596803069114685\nepoch 17 --- step 75 --- step size 32 loss 0.5408681035041809\nepoch 17 --- step 76 --- step size 32 loss 0.4135955274105072\nepoch 17 --- step 77 --- step size 32 loss 0.42492544651031494\nepoch 17 --- step 78 --- step size 32 loss 0.427717924118042\nepoch 17 --- step 79 --- step size 32 loss 0.4675353169441223\nepoch 17 --- step 80 --- step size 32 loss 0.45704513788223267\nepoch 17 --- step 81 --- step size 32 loss 0.40159112215042114\nepoch 17 --- step 82 --- step size 32 loss 0.36206144094467163\nepoch 17 --- step 83 --- step size 32 loss 0.4975859522819519\nepoch 17 --- step 84 --- step size 32 loss 0.3554806709289551\nepoch 17 --- step 85 --- step size 32 loss 0.4589969515800476\nepoch 17 --- step 86 --- step size 32 loss 0.436899870634079\nepoch 17 --- step 87 --- step size 32 loss 0.3835882544517517\nepoch 17 --- step 88 --- step size 32 loss 0.3369874358177185\nepoch 17 --- step 89 --- step size 32 loss 0.45300453901290894\nepoch 17 --- step 90 --- step size 32 loss 0.4004076421260834\nepoch 17 --- step 91 --- step size 32 loss 0.22896763682365417\nepoch 17 --- step 92 --- step size 32 loss 0.4610080122947693\nepoch 17 --- step 93 --- step size 32 loss 0.48534053564071655\nepoch 17 --- step 94 --- step size 32 loss 0.24425983428955078\nepoch 17 --- step 95 --- step size 32 loss 0.3192582130432129\nepoch 17 --- step 96 --- step size 32 loss 0.38101696968078613\nepoch 17 --- step 97 --- step size 32 loss 0.32548749446868896\nepoch 17 --- step 98 --- step size 32 loss 0.5171030759811401\nepoch 17 --- step 99 --- step size 32 loss 0.32260480523109436\nepoch 17 --- step 100 --- step size 32 loss 0.3423020839691162\nepoch 17 --- step 101 --- step size 32 loss 0.29246026277542114\nepoch 17 --- step 102 --- step size 32 loss 0.5086129903793335\nepoch 17 --- step 103 --- step size 32 loss 0.35372334718704224\nepoch 17 --- step 104 --- step size 32 loss 0.4675080478191376\nepoch 17 --- step 105 --- step size 32 loss 0.33515605330467224\nepoch 17 --- step 106 --- step size 32 loss 0.4565082788467407\nepoch 17 --- step 107 --- step size 32 loss 0.5422197580337524\nepoch 17 --- step 108 --- step size 32 loss 0.43228909373283386\nepoch 17 --- step 109 --- step size 32 loss 0.41560256481170654\nepoch 17 --- step 110 --- step size 32 loss 0.33250463008880615\nepoch 17 --- step 111 --- step size 32 loss 0.5064648985862732\nepoch 17 --- step 112 --- step size 32 loss 0.34038102626800537\nepoch 17 --- step 113 --- step size 32 loss 0.5203712582588196\nepoch 17 --- step 114 --- step size 32 loss 0.43637627363204956\nepoch 17 --- step 115 --- step size 32 loss 0.41821587085723877\nepoch 17 --- step 116 --- step size 32 loss 0.33381637930870056\nepoch 17 --- step 117 --- step size 32 loss 0.36635303497314453\nepoch 17 --- step 118 --- step size 32 loss 0.43484172224998474\nepoch 17 --- step 119 --- step size 32 loss 0.4597330093383789\nepoch 17 --- step 120 --- step size 32 loss 0.39338263869285583\nepoch 17 --- step 121 --- step size 32 loss 0.40073737502098083\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 18 --- step 0 --- step size 32 loss 0.3607901930809021\nepoch 18 --- step 1 --- step size 32 loss 0.31305575370788574\nepoch 18 --- step 2 --- step size 32 loss 0.3599469065666199\nepoch 18 --- step 3 --- step size 32 loss 0.3265160620212555\nepoch 18 --- step 4 --- step size 32 loss 0.414347380399704\nepoch 18 --- step 5 --- step size 32 loss 0.3421704173088074\nepoch 18 --- step 6 --- step size 32 loss 0.3872523009777069\nepoch 18 --- step 7 --- step size 32 loss 0.4359002709388733\nepoch 18 --- step 8 --- step size 32 loss 0.23913463950157166\nepoch 18 --- step 9 --- step size 32 loss 0.4013979434967041\nepoch 18 --- step 10 --- step size 32 loss 0.3895721435546875\nepoch 18 --- step 11 --- step size 32 loss 0.444241464138031\nepoch 18 --- step 12 --- step size 32 loss 0.3094918429851532\nepoch 18 --- step 13 --- step size 32 loss 0.3495004177093506\nepoch 18 --- step 14 --- step size 32 loss 0.42758092284202576\nepoch 18 --- step 15 --- step size 32 loss 0.3877057433128357\nepoch 18 --- step 16 --- step size 32 loss 0.3154586851596832\nepoch 18 --- step 17 --- step size 32 loss 0.3541606664657593\nepoch 18 --- step 18 --- step size 32 loss 0.4011836647987366\nepoch 18 --- step 19 --- step size 32 loss 0.47850286960601807\nepoch 18 --- step 20 --- step size 32 loss 0.40245264768600464\nepoch 18 --- step 21 --- step size 32 loss 0.5028769969940186\nepoch 18 --- step 22 --- step size 32 loss 0.388208270072937\nepoch 18 --- step 23 --- step size 32 loss 0.5758624076843262\nepoch 18 --- step 24 --- step size 32 loss 0.37948331236839294\nepoch 18 --- step 25 --- step size 32 loss 0.3511214256286621\nepoch 18 --- step 26 --- step size 32 loss 0.4229753017425537\nepoch 18 --- step 27 --- step size 32 loss 0.38190263509750366\nepoch 18 --- step 28 --- step size 32 loss 0.39753004908561707\nepoch 18 --- step 29 --- step size 32 loss 0.26753148436546326\nepoch 18 --- step 30 --- step size 32 loss 0.3439478874206543\nepoch 18 --- step 31 --- step size 32 loss 0.41393399238586426\nepoch 18 --- step 32 --- step size 32 loss 0.3411352336406708\nepoch 18 --- step 33 --- step size 32 loss 0.3653171956539154\nepoch 18 --- step 34 --- step size 32 loss 0.28583937883377075\nepoch 18 --- step 35 --- step size 32 loss 0.49113914370536804\nepoch 18 --- step 36 --- step size 32 loss 0.4260561168193817\nepoch 18 --- step 37 --- step size 32 loss 0.4205784201622009\nepoch 18 --- step 38 --- step size 32 loss 0.3927239775657654\nepoch 18 --- step 39 --- step size 32 loss 0.3900798261165619\nepoch 18 --- step 40 --- step size 32 loss 0.34110602736473083\nepoch 18 --- step 41 --- step size 32 loss 0.4513262212276459\nepoch 18 --- step 42 --- step size 32 loss 0.32530874013900757\nepoch 18 --- step 43 --- step size 32 loss 0.460418701171875\nepoch 18 --- step 44 --- step size 32 loss 0.2777148485183716\nepoch 18 --- step 45 --- step size 32 loss 0.39964836835861206\nepoch 18 --- step 46 --- step size 32 loss 0.46325159072875977\nepoch 18 --- step 47 --- step size 32 loss 0.4658192992210388\nepoch 18 --- step 48 --- step size 32 loss 0.549705982208252\nepoch 18 --- step 49 --- step size 32 loss 0.3618431091308594\nepoch 18 --- step 50 --- step size 32 loss 0.4708712697029114\nepoch 18 --- step 51 --- step size 32 loss 0.3814653754234314\nepoch 18 --- step 52 --- step size 32 loss 0.39936280250549316\nepoch 18 --- step 53 --- step size 32 loss 0.6072350144386292\nepoch 18 --- step 54 --- step size 32 loss 0.46432679891586304\nepoch 18 --- step 55 --- step size 32 loss 0.2728866934776306\nepoch 18 --- step 56 --- step size 32 loss 0.5028099417686462\nepoch 18 --- step 57 --- step size 32 loss 0.3412221670150757\nepoch 18 --- step 58 --- step size 32 loss 0.4284366965293884\nepoch 18 --- step 59 --- step size 32 loss 0.3522323966026306\nepoch 18 --- step 60 --- step size 32 loss 0.363442599773407\nepoch 18 --- step 61 --- step size 32 loss 0.2637932300567627\nepoch 18 --- step 62 --- step size 32 loss 0.5632960796356201\nepoch 18 --- step 63 --- step size 32 loss 0.34186363220214844\nepoch 18 --- step 64 --- step size 32 loss 0.41388368606567383\nepoch 18 --- step 65 --- step size 32 loss 0.3479505777359009\nepoch 18 --- step 66 --- step size 32 loss 0.3389846086502075\nepoch 18 --- step 67 --- step size 32 loss 0.3724006116390228\nepoch 18 --- step 68 --- step size 32 loss 0.3989464044570923\nepoch 18 --- step 69 --- step size 32 loss 0.5244786739349365\nepoch 18 --- step 70 --- step size 32 loss 0.5318002700805664\nepoch 18 --- step 71 --- step size 32 loss 0.33714351058006287\nepoch 18 --- step 72 --- step size 32 loss 0.3219163715839386\nepoch 18 --- step 73 --- step size 32 loss 0.2709500789642334\nepoch 18 --- step 74 --- step size 32 loss 0.3905654847621918\nepoch 18 --- step 75 --- step size 32 loss 0.4632757604122162\nepoch 18 --- step 76 --- step size 32 loss 0.3799007833003998\nepoch 18 --- step 77 --- step size 32 loss 0.37952297925949097\nepoch 18 --- step 78 --- step size 32 loss 0.37896573543548584\nepoch 18 --- step 79 --- step size 32 loss 0.4268309473991394\nepoch 18 --- step 80 --- step size 32 loss 0.3600773811340332\nepoch 18 --- step 81 --- step size 32 loss 0.31069594621658325\nepoch 18 --- step 82 --- step size 32 loss 0.47629207372665405\nepoch 18 --- step 83 --- step size 32 loss 0.38592904806137085\nepoch 18 --- step 84 --- step size 32 loss 0.3900882601737976\nepoch 18 --- step 85 --- step size 32 loss 0.34586843848228455\nepoch 18 --- step 86 --- step size 32 loss 0.3679804503917694\nepoch 18 --- step 87 --- step size 32 loss 0.3908306956291199\nepoch 18 --- step 88 --- step size 32 loss 0.20822231471538544\nepoch 18 --- step 89 --- step size 32 loss 0.3700563907623291\nepoch 18 --- step 90 --- step size 32 loss 0.2954826354980469\nepoch 18 --- step 91 --- step size 32 loss 0.3524872660636902\nepoch 18 --- step 92 --- step size 32 loss 0.2733224034309387\nepoch 18 --- step 93 --- step size 32 loss 0.34701722860336304\nepoch 18 --- step 94 --- step size 32 loss 0.3263413608074188\nepoch 18 --- step 95 --- step size 32 loss 0.4107746481895447\nepoch 18 --- step 96 --- step size 32 loss 0.3551596403121948\nepoch 18 --- step 97 --- step size 32 loss 0.4680057466030121\nepoch 18 --- step 98 --- step size 32 loss 0.35859382152557373\nepoch 18 --- step 99 --- step size 32 loss 0.3698938488960266\nepoch 18 --- step 100 --- step size 32 loss 0.39122629165649414\nepoch 18 --- step 101 --- step size 32 loss 0.2969130277633667\nepoch 18 --- step 102 --- step size 32 loss 0.30458658933639526\nepoch 18 --- step 103 --- step size 32 loss 0.40631669759750366\nepoch 18 --- step 104 --- step size 32 loss 0.42470741271972656\nepoch 18 --- step 105 --- step size 32 loss 0.3922880291938782\nepoch 18 --- step 106 --- step size 32 loss 0.46256256103515625\nepoch 18 --- step 107 --- step size 32 loss 0.3243527114391327\nepoch 18 --- step 108 --- step size 32 loss 0.3867858350276947\nepoch 18 --- step 109 --- step size 32 loss 0.40095198154449463\nepoch 18 --- step 110 --- step size 32 loss 0.4453008770942688\nepoch 18 --- step 111 --- step size 32 loss 0.3768906593322754\nepoch 18 --- step 112 --- step size 32 loss 0.2969205379486084\nepoch 18 --- step 113 --- step size 32 loss 0.38697606325149536\nepoch 18 --- step 114 --- step size 32 loss 0.2887091636657715\nepoch 18 --- step 115 --- step size 32 loss 0.3981570601463318\nepoch 18 --- step 116 --- step size 32 loss 0.4033873975276947\nepoch 18 --- step 117 --- step size 32 loss 0.3587915599346161\nepoch 18 --- step 118 --- step size 32 loss 0.413883775472641\nepoch 18 --- step 119 --- step size 32 loss 0.3489983081817627\nepoch 18 --- step 120 --- step size 32 loss 0.4429217278957367\nepoch 18 --- step 121 --- step size 32 loss 0.480567067861557\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"epoch 19 --- step 0 --- step size 32 loss 0.35611170530319214\nepoch 19 --- step 1 --- step size 32 loss 0.35133689641952515\nepoch 19 --- step 2 --- step size 32 loss 0.3484143018722534\nepoch 19 --- step 3 --- step size 32 loss 0.26366177201271057\nepoch 19 --- step 4 --- step size 32 loss 0.37439775466918945\nepoch 19 --- step 5 --- step size 32 loss 0.3765714764595032\nepoch 19 --- step 6 --- step size 32 loss 0.3203573226928711\nepoch 19 --- step 7 --- step size 32 loss 0.26533132791519165\nepoch 19 --- step 8 --- step size 32 loss 0.3712722659111023\nepoch 19 --- step 9 --- step size 32 loss 0.4896305203437805\nepoch 19 --- step 10 --- step size 32 loss 0.3993581235408783\nepoch 19 --- step 11 --- step size 32 loss 0.5821589231491089\nepoch 19 --- step 12 --- step size 32 loss 0.3498574495315552\nepoch 19 --- step 13 --- step size 32 loss 0.3530066907405853\nepoch 19 --- step 14 --- step size 32 loss 0.393622487783432\nepoch 19 --- step 15 --- step size 32 loss 0.3191549777984619\nepoch 19 --- step 16 --- step size 32 loss 0.28473028540611267\nepoch 19 --- step 17 --- step size 32 loss 0.20648837089538574\nepoch 19 --- step 18 --- step size 32 loss 0.3910624384880066\nepoch 19 --- step 19 --- step size 32 loss 0.371705025434494\nepoch 19 --- step 20 --- step size 32 loss 0.29822224378585815\nepoch 19 --- step 21 --- step size 32 loss 0.4512763023376465\nepoch 19 --- step 22 --- step size 32 loss 0.31684544682502747\nepoch 19 --- step 23 --- step size 32 loss 0.4238227605819702\nepoch 19 --- step 24 --- step size 32 loss 0.3592352271080017\nepoch 19 --- step 25 --- step size 32 loss 0.28205710649490356\nepoch 19 --- step 26 --- step size 32 loss 0.37894317507743835\nepoch 19 --- step 27 --- step size 32 loss 0.44447001814842224\nepoch 19 --- step 28 --- step size 32 loss 0.4009208679199219\nepoch 19 --- step 29 --- step size 32 loss 0.381044864654541\nepoch 19 --- step 30 --- step size 32 loss 0.4470897614955902\nepoch 19 --- step 31 --- step size 32 loss 0.2295508086681366\nepoch 19 --- step 32 --- step size 32 loss 0.4208623766899109\nepoch 19 --- step 33 --- step size 32 loss 0.23221883177757263\nepoch 19 --- step 34 --- step size 32 loss 0.3029303550720215\nepoch 19 --- step 35 --- step size 32 loss 0.49713143706321716\nepoch 19 --- step 36 --- step size 32 loss 0.46066007018089294\nepoch 19 --- step 37 --- step size 32 loss 0.37330231070518494\nepoch 19 --- step 38 --- step size 32 loss 0.34119313955307007\nepoch 19 --- step 39 --- step size 32 loss 0.3591383397579193\nepoch 19 --- step 40 --- step size 32 loss 0.4070906639099121\nepoch 19 --- step 41 --- step size 32 loss 0.31197768449783325\nepoch 19 --- step 42 --- step size 32 loss 0.3219148516654968\nepoch 19 --- step 43 --- step size 32 loss 0.4335728883743286\nepoch 19 --- step 44 --- step size 32 loss 0.459133118391037\nepoch 19 --- step 45 --- step size 32 loss 0.3628730773925781\nepoch 19 --- step 46 --- step size 32 loss 0.29692360758781433\nepoch 19 --- step 47 --- step size 32 loss 0.35007011890411377\nepoch 19 --- step 48 --- step size 32 loss 0.2649141848087311\nepoch 19 --- step 49 --- step size 32 loss 0.5388559103012085\nepoch 19 --- step 50 --- step size 32 loss 0.32036131620407104\nepoch 19 --- step 51 --- step size 32 loss 0.4158070981502533\nepoch 19 --- step 52 --- step size 32 loss 0.33678993582725525\nepoch 19 --- step 53 --- step size 32 loss 0.24981534481048584\nepoch 19 --- step 54 --- step size 32 loss 0.2858886420726776\nepoch 19 --- step 55 --- step size 32 loss 0.3532847762107849\nepoch 19 --- step 56 --- step size 32 loss 0.5251578688621521\nepoch 19 --- step 57 --- step size 32 loss 0.4098464846611023\nepoch 19 --- step 58 --- step size 32 loss 0.41666823625564575\nepoch 19 --- step 59 --- step size 32 loss 0.37203192710876465\nepoch 19 --- step 60 --- step size 32 loss 0.29679274559020996\nepoch 19 --- step 61 --- step size 32 loss 0.337249219417572\nepoch 19 --- step 62 --- step size 32 loss 0.301271915435791\nepoch 19 --- step 63 --- step size 32 loss 0.4860011339187622\nepoch 19 --- step 64 --- step size 32 loss 0.3385683000087738\nepoch 19 --- step 65 --- step size 32 loss 0.3695691227912903\nepoch 19 --- step 66 --- step size 32 loss 0.3830987811088562\nepoch 19 --- step 67 --- step size 32 loss 0.3204318583011627\nepoch 19 --- step 68 --- step size 32 loss 0.34295639395713806\nepoch 19 --- step 69 --- step size 32 loss 0.23843877017498016\nepoch 19 --- step 70 --- step size 32 loss 0.37582993507385254\nepoch 19 --- step 71 --- step size 32 loss 0.38387975096702576\nepoch 19 --- step 72 --- step size 32 loss 0.2623223662376404\nepoch 19 --- step 73 --- step size 32 loss 0.3675668239593506\nepoch 19 --- step 74 --- step size 32 loss 0.2924322485923767\nepoch 19 --- step 75 --- step size 32 loss 0.3523756265640259\nepoch 19 --- step 76 --- step size 32 loss 0.34244343638420105\nepoch 19 --- step 77 --- step size 32 loss 0.4861123561859131\nepoch 19 --- step 78 --- step size 32 loss 0.36590123176574707\nepoch 19 --- step 79 --- step size 32 loss 0.4193032681941986\nepoch 19 --- step 80 --- step size 32 loss 0.2844899296760559\nepoch 19 --- step 81 --- step size 32 loss 0.2644488513469696\nepoch 19 --- step 82 --- step size 32 loss 0.2617248296737671\nepoch 19 --- step 83 --- step size 32 loss 0.37340840697288513\nepoch 19 --- step 84 --- step size 32 loss 0.40841493010520935\nepoch 19 --- step 85 --- step size 32 loss 0.44262978434562683\nepoch 19 --- step 86 --- step size 32 loss 0.393998384475708\nepoch 19 --- step 87 --- step size 32 loss 0.37208980321884155\nepoch 19 --- step 88 --- step size 32 loss 0.3011384904384613\nepoch 19 --- step 89 --- step size 32 loss 0.4008597135543823\nepoch 19 --- step 90 --- step size 32 loss 0.36057013273239136\nepoch 19 --- step 91 --- step size 32 loss 0.3317948579788208\nepoch 19 --- step 92 --- step size 32 loss 0.36860692501068115\nepoch 19 --- step 93 --- step size 32 loss 0.4467812478542328\nepoch 19 --- step 94 --- step size 32 loss 0.36545294523239136\nepoch 19 --- step 95 --- step size 32 loss 0.32698363065719604\nepoch 19 --- step 96 --- step size 32 loss 0.27349913120269775\nepoch 19 --- step 97 --- step size 32 loss 0.2665294408798218\nepoch 19 --- step 98 --- step size 32 loss 0.35734328627586365\nepoch 19 --- step 99 --- step size 32 loss 0.3843601644039154\nepoch 19 --- step 100 --- step size 32 loss 0.3693237900733948\nepoch 19 --- step 101 --- step size 32 loss 0.36522388458251953\nepoch 19 --- step 102 --- step size 32 loss 0.28040480613708496\nepoch 19 --- step 103 --- step size 32 loss 0.2736726999282837\nepoch 19 --- step 104 --- step size 32 loss 0.29546573758125305\nepoch 19 --- step 105 --- step size 32 loss 0.38280928134918213\nepoch 19 --- step 106 --- step size 32 loss 0.270656943321228\nepoch 19 --- step 107 --- step size 32 loss 0.3594452738761902\nepoch 19 --- step 108 --- step size 32 loss 0.4037706255912781\nepoch 19 --- step 109 --- step size 32 loss 0.36878976225852966\nepoch 19 --- step 110 --- step size 32 loss 0.4348914325237274\nepoch 19 --- step 111 --- step size 32 loss 0.3819171190261841\nepoch 19 --- step 112 --- step size 32 loss 0.4059452712535858\nepoch 19 --- step 113 --- step size 32 loss 0.42009854316711426\nepoch 19 --- step 114 --- step size 32 loss 0.31826648116111755\nepoch 19 --- step 115 --- step size 32 loss 0.2965129613876343\nepoch 19 --- step 116 --- step size 32 loss 0.29054784774780273\nepoch 19 --- step 117 --- step size 32 loss 0.43153485655784607\nepoch 19 --- step 118 --- step size 32 loss 0.5066729784011841\nepoch 19 --- step 119 --- step size 32 loss 0.37949857115745544\nepoch 19 --- step 120 --- step size 32 loss 0.32739099860191345\nepoch 19 --- step 121 --- step size 32 loss 0.3804762065410614\n","output_type":"stream"}]},{"cell_type":"code","source":"\npredictions = clf.predict(data=TEST)\nsubmission = pd.merge(TEST, predictions, left_index=True,right_index=True)\nsubmission = submission.drop('full_text',axis=1)\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:32:28.703952Z","iopub.execute_input":"2022-10-29T04:32:28.704372Z","iopub.status.idle":"2022-10-29T04:32:33.271968Z","shell.execute_reply.started":"2022-10-29T04:32:28.704328Z","shell.execute_reply":"2022-10-29T04:32:33.270316Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at /kaggle/input/dberta-base-model/ were not used when initializing DebertaForMaskedLM: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n- This IS expected if you are initializing DebertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaForMaskedLM were not initialized from the model checkpoint at /kaggle/input/dberta-base-model/ and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 3/3 [00:00<00:00,  4.18it/s]\n100%|██████████| 3/3 [00:00<00:00,  4.80it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-10-29T04:32:33.274076Z","iopub.execute_input":"2022-10-29T04:32:33.274480Z","iopub.status.idle":"2022-10-29T04:32:33.293907Z","shell.execute_reply.started":"2022-10-29T04:32:33.274438Z","shell.execute_reply":"2022-10-29T04:32:33.292750Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.745597  2.571096    2.752243     2.604530  2.616288   \n1  000BAD50D026  2.479983  2.218592    2.382292     2.226262  2.300099   \n2  00367BB2546B  2.659872  2.457405    2.632870     2.482533  2.514230   \n\n   conventions  \n0     2.687251  \n1     2.428221  \n2     2.603660  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.745597</td>\n      <td>2.571096</td>\n      <td>2.752243</td>\n      <td>2.604530</td>\n      <td>2.616288</td>\n      <td>2.687251</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.479983</td>\n      <td>2.218592</td>\n      <td>2.382292</td>\n      <td>2.226262</td>\n      <td>2.300099</td>\n      <td>2.428221</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>2.659872</td>\n      <td>2.457405</td>\n      <td>2.632870</td>\n      <td>2.482533</td>\n      <td>2.514230</td>\n      <td>2.603660</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}