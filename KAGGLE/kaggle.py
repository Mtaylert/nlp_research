# -*- coding: utf-8 -*-
"""Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yIwteaccUGDY-vTSoKWV_yj6h51AtMOp
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np 
import os
import pandas as pd
!pip install transformers 
from transformers import AutoModel, AutoTokenizer, AutoConfig
import torch
from torch.utils.data import DataLoader, Dataset
from torch import nn
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from tqdm import tqdm
os.system('pip install iterative-stratification==0.1.7')
from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
!pip install sentencepiece
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

class config:
    TRAIN = "/content/drive/MyDrive/EnglishGrading/train.csv"
    TEST = "/content/drive/MyDrive/EnglishGrading/test.csv"
    LR = 9e-6
    TRAIN_BATCH_SIZE = 8
    TEST_BATCH_SIZE = 4
    EPOCHS = 3
    N_FOLDS = 4
    TARGETS = ['cohesion', 'syntax', 'vocabulary',
       'phraseology', 'grammar', 'conventions']
    PADDING = False
    MAX_LEN = 512
    NUM_WORKERS = 4
    OUTPUT_PATH = '/content/drive/MyDrive/EnglishGrading/output/'
    MODEL = 'microsoft/deberta-v3-base'
    ENSEMBLE_PATH = "/content/drive/MyDrive/EnglishGrading/output/deberta/"
    CONFIG_PATH = "/content/drive/MyDrive/EnglishGrading/output/model_config/config.pth"
    ENCODER_LR=2e-5
    DECODER_LR=2e-5
    POOLING = 'attention'
    MIN_LR=1e-6
    EPS=1e-6
    BETAS=(0.9, 0.999)
    WEIGHT_DECAY=0.01
    GRAD_ACCUM_STEP=1
    MAX_GRAD_NORM=1000
    GRADIENT_CHECKPOINT = True
    NUM_CYCLES = 0.5
    #Layer-Wise Learning Rate Decay
    LLRD = True
    LAYERWISE_LR = 5e-5
    LAYERWISE_LR_DECAY = 0.9
    LAYERWISE_WEIGHT_DECAY = 0.01
    LAYERWISE_ADAM_EPS = 1e-6
    LAYERWISE_BERTADAM = False

tokenizer = AutoTokenizer.from_pretrained(config.MODEL)
tokenizer.save_pretrained(config.OUTPUT_PATH+'tokenizer/')

class TrainDataSetup(Dataset):
    def __init__(self, text, targets):
        self.data = text
        self.labels = targets
        self.tokenizer = AutoTokenizer.from_pretrained(config.OUTPUT_PATH+'tokenizer')
    def __len__(self):
        return len(self.data)

    def __getitem__(self, item):
        text = self.data.iloc[item]
        target = self.labels.iloc[item]
        inputs = self.tokenizer.encode_plus(text,
                                                return_tensors=None,
                                                add_special_tokens=True,
                                                max_length = config.MAX_LEN,
                                                pad_to_max_length=True,
                                                truncation=True,
                                            return_attention_mask=True,
                                            return_token_type_ids=True,
                                            )

        ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

        return {
            "input_ids": torch.tensor(ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(target,dtype=torch.long),
        }
    
    
class TestDataSetup(Dataset):
    def __init__(self, text):
        self.data = text
        self.tokenizer = AutoTokenizer.from_pretrained(config.OUTPUT_PATH+'tokenizer')
    def __len__(self):
        return len(self.data)

    def __getitem__(self, item):
        text = self.data[item]
        inputs = self.tokenizer.encode_plus(text,
                                                return_tensors=None,
                                                add_special_tokens=True,
                                                max_length = config.MAX_LEN,
                                                pad_to_max_length=True,
                                                truncation=True,
                                            return_attention_mask=True,
                                            return_token_type_ids=True,
                                            )
        ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

        return {
            "input_ids": torch.tensor(ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
        }

class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()

    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        mean_embeddings = sum_embeddings/sum_mask
        return mean_embeddings
    
    
class AttentionPooling(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.attention = nn.Sequential(
        nn.Linear(in_dim, in_dim),
        nn.LayerNorm(in_dim),
        nn.GELU(),
        nn.Linear(in_dim, 1),
        )

    def forward(self, last_hidden_state, attention_mask):
        w = self.attention(last_hidden_state).float()
        w[attention_mask==0]=float('-inf')
        w = torch.softmax(w,1)
        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)
        return attention_embeddings


class EnglishMultiLabelModel(nn.Module):
    def __init__(self, pretrained, config_path):
        super(EnglishMultiLabelModel, self).__init__()
        if config_path is None:
          self.config = AutoConfig.from_pretrained(config.MODEL)
          self.config.hidden_dropout = 0.
          self.config.hidden_dropout_prob = 0.
          self.config.attention_dropout = 0.
          self.config.attention_probs_dropout_prob = 0.
        else:
          self.config = torch.load(config_path)
        if pretrained:
             self.model = AutoModel.from_pretrained(config.MODEL,config=self.config)
        else:
            self.model = AutoModel.from_config(self.config)

        if config.GRADIENT_CHECKPOINT:
            self.model.gradient_checkpointing_enable()
        if config.POOLING == 'attention':
            self.pool = AttentionPooling(self.config.hidden_size)       
        elif config.POOLING == 'mean':
            self.pool = MeanPooling()
        self.fc = nn.Linear(self.config.hidden_size, 6)
        self._init_weights(self.fc)
        
        self.model.embeddings.requires_grad_(False)
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, inputs_ids, attention_mask):
        outputs= self.model(inputs_ids, attention_mask)
        last_hidden_states = outputs[0]
        feature = self.pool(last_hidden_states, attention_mask)
        outout = self.fc(feature)
        return outout
    
class RMSELoss(nn.Module):
    def __init__(self, reduction='mean', eps=1e-9):
        super().__init__()
        self.mse = nn.MSELoss(reduction='none')
        self.reduction = reduction
        self.eps = eps

    def forward(self, y_pred, y_true):
        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)
        if self.reduction == 'none':
            loss = loss
        elif self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'mean':
            loss = loss.mean()
        return loss

def collate(inputs):
    mask_len = int(inputs["attention_mask"].sum(axis=1).max())
    for k, v in inputs.items():
        inputs[k] = inputs[k][:,:mask_len]
    return inputs

class EnglishGrader:
    def __init__(self, epochs, learning_rate, scheduler, config_path=None, pretrained=False):
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.scheduler = scheduler
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = EnglishMultiLabelModel(pretrained=pretrained, config_path=config_path)
        if pretrained:
          torch.save(self.model.config, config.OUTPUT_PATH+'model_config/config.pth')

    def __get_scheduler(self, optimizer, num_warmup_steps, num_train_steps):
        if self.scheduler == 'linear':
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps
            )
        elif self.scheduler == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps,
                num_cycles=0.5
            )
        return scheduler
        
    def get_optimizer_grouped_parameters(self, model, 
                                         layerwise_lr,
                                         layerwise_weight_decay,
                                         layerwise_lr_decay):
        
        no_decay = ["bias", "LayerNorm.weight"]
        # initialize lr for task specific layer
        optimizer_grouped_parameters = [{"params": [p for n, p in model.named_parameters() if "model" not in n],
                                         "weight_decay": 0.0,
                                         "lr": layerwise_lr,
                                        },]
        # initialize lrs for every layer
        layers = [model.model.embeddings] + list(model.model.encoder.layer)
        layers.reverse()
        lr = layerwise_lr
        for layer in layers:
            optimizer_grouped_parameters += [{"params": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],
                                              "weight_decay": layerwise_weight_decay,
                                              "lr": lr,
                                             },
                                             {"params": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],
                                              "weight_decay": 0.0,
                                              "lr": lr,
                                             },]
            lr *= layerwise_lr_decay
        return optimizer_grouped_parameters
    
    
    def _get_optimizer_params(self, model, encoder_lr, decoder_lr, weight_decay=0.0):
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': weight_decay},
            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': 0.0},
            {'params': [p for n, p in model.named_parameters() if "model" not in n],
             'lr': decoder_lr, 'weight_decay': 0.0}
        ]
        return optimizer_parameters

    def fit(self, data, model_name):
        train_dataset = TrainDataSetup(text=data['full_text'], targets=data[config.TARGETS])
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=config.TRAIN_BATCH_SIZE,
            shuffle=True,
            num_workers=config.NUM_WORKERS,
            pin_memory=True,
            drop_last=True,
        )
        
        self.model.to(self.device)
        self.model.train()
        param_optimizer = list(self.model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {
                "params": [
                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.001,
            },
            {
                "params": [
                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]
        if config.LLRD:
          from transformers import AdamW
          optimizer_grouped = self.get_optimizer_grouped_parameters(model=self.model,
                                                                  layerwise_lr=config.LAYERWISE_LR,
                                                                  layerwise_weight_decay=config.LAYERWISE_WEIGHT_DECAY,
                                                                  layerwise_lr_decay=config.LAYERWISE_LR_DECAY)
          optimizer = AdamW(optimizer_grouped,
                          lr = config.LAYERWISE_LR,
                          eps = config.LAYERWISE_ADAM_EPS,
                          correct_bias = not config.LAYERWISE_BERTADAM)
        else:
          from torch.optim import AdamW
          optimizer_parameters = self._get_optimizer_params(model=self.model,
                                                  encoder_lr=config.ENCODER_LR, 
                                                  decoder_lr=config.DECODER_LR,
                                                  weight_decay=config.WEIGHT_DECAY)

          optimizer = AdamW(params=optimizer_parameters, lr=config.LR,eps=config.EPS, betas=config.BETAS)


        num_train_steps = int(len(data) / config.TRAIN_BATCH_SIZE * self.epochs)
        scheduler = self.__get_scheduler(optimizer=optimizer, num_warmup_steps=0, num_train_steps=num_train_steps)
        criterion = nn.SmoothL1Loss(reduction='mean')  # RMSELoss(reduction="mean")

        for epoch in range(self.epochs):
            self.model.train()
            for step, batch in enumerate(train_dataloader):
                self.model.zero_grad()
                batch = collate(batch)
                b_input_ids = batch["input_ids"].to(self.device)
                b_attn_mask = batch["attention_mask"].to(self.device)
                b_labels = batch["labels"].to(self.device)
                prediction_probas = self.model(b_input_ids, b_attn_mask)
                loss = criterion(prediction_probas, b_labels)

                # start backwards prop
                loss.backward()
                # Clip the norm of the gradients to 1.0 to prevent "exploding gradients"
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

                # Update model parameters:
                # fine tune BERT params and train additional dense layers
                optimizer.step()
                # update learning rate
                scheduler.step()
                print(f"Epoch: {epoch+1} ---- Loss: {loss.item()} ---- LR: {scheduler.get_lr()[0]}")
            torch.save(self.model.state_dict(), f"{config.OUTPUT_PATH+model_name}_model.bin")
            
            
    def validate(self, X):
        test_dataset = TestDataSetup(text=X)
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.TEST_BATCH_SIZE,
            shuffle=False,
            num_workers=config.NUM_WORKERS,
            pin_memory=True,
            drop_last=False,
        )
        self.model.eval()
        predictions = []
        for step, batch in enumerate(tqdm(test_loader)):
            batch = collate(batch)
            b_input_ids = batch["input_ids"].to(self.device)
            b_attn_mask = batch["attention_mask"].to(self.device)
            with torch.no_grad():
                pred_probas = self.model(b_input_ids, b_attn_mask)
            predictions.append(pred_probas.to("cpu").numpy())
        predictions = pd.DataFrame(np.concatenate(predictions))
        predictions.columns = config.TARGETS
        return predictions

            
    def predict(self, X, model_path):
        test_dataset = TestDataSetup(text=X)
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.TEST_BATCH_SIZE,
            shuffle=False,
            num_workers=config.NUM_WORKERS,
            pin_memory=True,
            drop_last=False,
        )

        self.model.load_state_dict(
            torch.load(model_path)
        )
        self.model.to(self.device)
        self.model.eval()
        predictions = []
        for step, batch in enumerate(tqdm(test_loader)):
            b_input_ids = batch["input_ids"].to(self.device)
            b_attn_mask = batch["attention_mask"].to(self.device)
            with torch.no_grad():
                pred_probas = self.model(b_input_ids, b_attn_mask)
            predictions.append(pred_probas.to("cpu").numpy())
        predictions = pd.DataFrame(np.concatenate(predictions))
        predictions.columns = config.TARGETS
        return predictions

train = pd.read_csv(config.TRAIN)
X_train,X_val,y_train,y_val = train_test_split(train['full_text'], train[config.TARGETS])
X_val=X_val.reset_index(drop=True)
y_val=y_val.reset_index(drop=True)
X_train=X_train.reset_index(drop=True)
y_train=y_train.reset_index(drop=True)
y_train['full_text'] = X_train
test =  pd.read_csv(config.TEST)

start_train = 1
start_val = 0
predictions_dict = {}
scores = {}
if start_train == 1:
    Fold = MultilabelStratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=1234)
    for n, (train_index, val_index) in enumerate(Fold.split(y_train, y_train[config.TARGETS])):
        y_train.loc[val_index, 'fold'] = int(n)
    y_train['fold'] = y_train['fold'].astype(int)
    for fold in range(config.N_FOLDS):
        train_data = y_train[y_train['fold']==fold].reset_index(drop=True)
        clf = EnglishGrader(epochs=config.EPOCHS, learning_rate=config.LR, scheduler='cosine', config_path=None, pretrained=True)
        clf.fit(data=train_data, model_name=f"model_{fold}")
        predictions = clf.validate(X=X_val)
        predictions_dict[fold] = predictions
        all_scores = []
        for col in config.TARGETS:
            score = mean_squared_error(predictions[col], y_val[col])
            print(f'score ------ {score}')
            all_scores.append(score)
        print(f'mean score -------- {np.mean(all_scores)}')
        scores[fold] = np.mean(all_scores)
                                  
    
else:
    
    clf = EnglishGrader(epochs=config.EPOCHS, learning_rate=config.LR, scheduler='cosine', config_path=config.CONFIG_PATH, pretrained=False)
    all_predictions = None
    for idx, model_name in enumerate(os.listdir(config.ENSEMBLE_PATH)):
        predictions = clf.predict(X=test['full_text'], model_path=config.ENSEMBLE_PATH+model_name)
        if idx == 0:
            all_predictions = predictions
        else:
            all_predictions += predictions
    all_predictions_avg = all_predictions/4
    submission = pd.merge(test, all_predictions_avg, left_index=True,right_index=True)
    submission = submission.drop('full_text',axis=1)