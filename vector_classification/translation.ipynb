{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0b1f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from utils import (cosine_similarity, get_dict,\n",
    "                   process_tweet)\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02859d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_embeddings = KeyedVectors.load_word2vec_format('word_vecs/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('word_vecs/wiki.r.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db81c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "\n",
    "english_set = set(en_embeddings.key_to_index)\n",
    "french_set = set(fr_embeddings.key_to_index)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "french_words = set(en_fr_train.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9f6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88350dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for en_word in en_fr_test.keys():\n",
    "    fr_word = en_fr_test[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09f3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation_embeddings(en_fr,en_embed,fr_embed):\n",
    "    X = []\n",
    "    Y = []\n",
    "    english_set = en_embed.keys()\n",
    "    french_set = fr_embed.keys()\n",
    "    \n",
    "    for eng,fre in en_fr.items():\n",
    "        if eng in english_set and fre in french_set:\n",
    "            X.append(en_embed[eng])\n",
    "            Y.append(fr_embed[fre])\n",
    "            \n",
    "            \n",
    "    X = np.vstack(X)\n",
    "    Y = np.vstack(Y)\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "# getting the training set:\n",
    "X_train, Y_train = generate_translation_embeddings(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f109b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_loss(X,Y,R):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    diff = np.dot(X,R)- Y\n",
    "    diff_squared = diff**2\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    loss = sum_diff_squared/m\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(X,Y,iters=10000, alpha = 0.1):\n",
    "    m = X.shape[0]\n",
    "    np.random.seed(123)\n",
    "    #initialize R\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    \n",
    "    for i in tqdm(range(iters)):\n",
    "        \n",
    "        gradient =  np.dot(X.transpose(),np.dot(X,R)-Y) * (2/m)\n",
    "        \n",
    "        R -= alpha * gradient\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40136628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 289.64it/s]\n"
     ]
    }
   ],
   "source": [
    "R_train = train(X_train,Y_train,iters=500,alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f84bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(v, candidates, k = 1):\n",
    "    \n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    ### END CODE HERE ###\n",
    "    return k_idx\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1157404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(X,Y,R):\n",
    "    \n",
    "    prediction = np.dot(X,R)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in tqdm(range(len(prediction))):\n",
    "        pred_idx = KNN(prediction[i],Y)\n",
    "        if pred_idx == i:\n",
    "            num_correct +=1\n",
    "            \n",
    "            \n",
    "    accuracy = num_correct/len(prediction)\n",
    "            \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b93d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 265.88it/s]\n",
      "100%|██████████| 1438/1438 [00:19<00:00, 72.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006954102920723226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R_train = train(X_train,Y_train,iters=100,alpha=.8)\n",
    "X_val,Y_val = generate_translation_embeddings(en_fr_test, en_embeddings_subset,fr_embeddings_subset)\n",
    "accuracy = test_accuracy(X_val,Y_val,R_train)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d592b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
